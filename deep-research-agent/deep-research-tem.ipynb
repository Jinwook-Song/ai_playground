{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaabcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from tools import web_search_tool, save_report_to_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe3b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-5-nano-2025-08-07\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80cd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_planner = AssistantAgent(\n",
    "    \"research_planner\",\n",
    "    description=\"A strategic research coordinator that breaks down complex questions into research subtasks\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are a research planning specialist. Your job is to create a focused research plan.\n",
    "\n",
    "For each research question, create a FOCUSED research plan with:\n",
    "\n",
    "1. **Core Topics**: 2-3 main areas to investigate\n",
    "2. **Search Queries**: Create 3-5 specific search queries covering:\n",
    "   - Latest developments and news\n",
    "   - Key statistics or data\n",
    "   - Expert analysis or studies\n",
    "   - Future outlook\n",
    "\n",
    "Keep the plan focused and achievable. Quality over quantity.\"\"\",\n",
    ")\n",
    "\n",
    "research_agent = AssistantAgent(\n",
    "    \"research_agent\",\n",
    "    description=\"A web research specialist that searches and extracts content\",\n",
    "    tools=[web_search_tool],\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are a web research specialist. Your job is to conduct focused searches based on the research plan.\n",
    "\n",
    "RESEARCH STRATEGY:\n",
    "1. **Execute 3-5 searches** from the research plan\n",
    "2. **Extract key information** from the results:\n",
    "   - Main facts and statistics\n",
    "   - Recent developments\n",
    "   - Expert opinions\n",
    "   - Important context\n",
    "\n",
    "3. **Quality focus**:\n",
    "   - Prioritize authoritative sources\n",
    "   - Look for recent information (within 2 years)\n",
    "   - Note diverse perspectives\n",
    "\n",
    "After completing the searches from the plan, summarize what you found. Your goal is to gather 5-10 quality sources.\"\"\",\n",
    ")\n",
    "\n",
    "research_analyst = AssistantAgent(\n",
    "    \"research_analyst\",\n",
    "    description=\"An expert analyst that creates research reports\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are a research analyst. Create a comprehensive report from the gathered research.\n",
    "\n",
    "CREATE A RESEARCH REPORT with:\n",
    "\n",
    "## Executive Summary\n",
    "- Key findings and conclusions\n",
    "- Main insights\n",
    "\n",
    "## Background & Current State\n",
    "- Current landscape\n",
    "- Recent developments\n",
    "- Key statistics and data\n",
    "\n",
    "## Analysis & Insights\n",
    "- Main trends\n",
    "- Different perspectives\n",
    "- Expert opinions\n",
    "\n",
    "## Future Outlook\n",
    "- Emerging trends\n",
    "- Predictions\n",
    "- Implications\n",
    "\n",
    "## Sources\n",
    "- List all sources used\n",
    "\n",
    "Write a clear, well-structured report based on the research gathered. End with \"REPORT_COMPLETE\" when finished.\"\"\",\n",
    ")\n",
    "\n",
    "quality_reviewer = AssistantAgent(\n",
    "    \"quality_reviewer\",\n",
    "    description=\"A quality assurance specialist that evaluates research completeness and accuracy\",\n",
    "    tools=[save_report_to_md],\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are a quality reviewer. Your job is to check if the research analyst has produced a complete research report.\n",
    "\n",
    "Look for:\n",
    "- A comprehensive research report from the research analyst that ends with \"REPORT_COMPLETE\"\n",
    "- The research question is fully answered\n",
    "- Sources are cited and reliable\n",
    "- The report includes summary, key information, analysis, and sources\n",
    "\n",
    "When you see a complete research report that ends with \"REPORT_COMPLETE\":\n",
    "1. First, use the save_report_to_md tool to save the report to report.md\n",
    "2. Then say: \"The research is complete. The report has been saved to report.md. Please review the report and let me know if you approve it or need additional research.\"\n",
    "\n",
    "If the research analyst has NOT yet created a complete report, tell them to create one now.\"\"\",\n",
    ")\n",
    "\n",
    "research_enhancer = AssistantAgent(\n",
    "    \"research_enhancer\",\n",
    "    description=\"A specialist that identifies critical gaps only\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are a research enhancement specialist. Your job is to identify ONLY CRITICAL gaps.\n",
    "\n",
    "Review the research and ONLY suggest additional searches if there are MAJOR gaps like:\n",
    "- Completely missing recent developments (last 6 months)\n",
    "- No statistics or data at all\n",
    "- Missing a crucial perspective that was specifically asked for\n",
    "\n",
    "If the research covers the basics reasonably well, say: \"The research is sufficient to proceed with the report.\"\n",
    "\n",
    "Only suggest 1-2 additional searches if absolutely necessary. We prioritize getting a good report done rather than perfect coverage.\"\"\",\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    description=\"Human reviewer who can request additional research or approve final results\",\n",
    "    input_func=input,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df17670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = \"\"\"\n",
    "Choose the best agent for the current task based on the conversation history:\n",
    "\n",
    "{roles}\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Available agents:\n",
    "- research_planner: Plan the research approach (ONLY at the start)\n",
    "- research_agent: Search for and extract content from web sources (after planning)\n",
    "- research_enhancer: Identify CRITICAL gaps only (use sparingly)\n",
    "- research_analyst: Write the final research report\n",
    "- quality_reviewer: Check if a complete report exists\n",
    "- user_proxy: Ask the human for feedback\n",
    "\n",
    "WORKFLOW:\n",
    "1. If no planning done yet → select research_planner\n",
    "2. If planning done but no research → select research_agent  \n",
    "3. After research_agent completes initial searches → select research_enhancer ONCE\n",
    "4. If enhancer says \"sufficient to proceed\" → select research_analyst\n",
    "5. If enhancer suggests critical searches → select research_agent ONCE more then research_analyst\n",
    "6. If research_analyst said \"REPORT_COMPLETE\" → select quality_reviewer\n",
    "7. If quality_reviewer asked for user feedback → select user_proxy\n",
    "\n",
    "IMPORTANT: After research_agent has searched 2 times maximum, proceed to research_analyst regardless.\n",
    "\n",
    "Pick the agent that should work next based on this workflow.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc8df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_termination = TextMentionTermination(\"APPROVED\")\n",
    "max_message_termination = MaxMessageTermination(max_messages=50)\n",
    "termination_condition = text_termination | max_message_termination\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    participants=[\n",
    "        research_agent,\n",
    "        research_analyst,\n",
    "        research_enhancer,\n",
    "        research_planner,\n",
    "        quality_reviewer,\n",
    "        user_proxy,\n",
    "    ],\n",
    "    selector_prompt=selector_prompt,\n",
    "    model_client=model_client,\n",
    "    allow_repeated_speaker=True,\n",
    "    termination_condition=termination_condition,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ac19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "바이브 코딩과, 소프트웨어 개발자의 미래에 대해서 조사해\n",
      "---------- TextMessage (research_planner) ----------\n",
      "좋습니다. 다만 “바이브 코딩”이 어떤 개념을 가리키는지 정확히 파악해야 계획을 정확하게 맞출 수 있습니다.\n",
      "\n",
      "확인 질문\n",
      "1) 바이브 코딩이 특정 기술/도구/프레임워크의 이름인가요, 아니면 일반적인 개념으로서의 분위기(바이브) 기반 코딩이나 감정/환경 인식과 연계된 코딩 방식인가요?\n",
      "2) 연구의 주된 초점은 학술 연구(논문 중심), 업계 트렌드(실무 영향), 또는 교육/직무 전망 중 어느 쪽에 맞추길 원하나요?\n",
      "3) 산출물 형식은 요약 보고서, 비교 분석 표, 권고안 등의 어떤 형식을 선호하시나요?\n",
      "4) 최근 2–3년의 최신 내용까지 포함하는 것이 좋나요, 아니면 더 넓은 시계열을 다루어도 되나요?\n",
      "\n",
      "확인되면, 위의 방향에 맞춰 바로 FOCUSED 연구 계획을 드리겠습니다. 아래는 한 가지 가정 하에 빠르게 예시 구조를 보여드리는 형태이며, 실제 해석이 확정되면 그에 맞춰 세부를 조정하겠습니다.\n",
      "\n",
      "예시(가정: 바이브 코딩을 분위기/감정 기반 코딩이나 인터랙티브 코딩이라는 일반 개념으로 해석)\n",
      "- Core Topics (2–3개)\n",
      "  1) 바이브 코딩의 정의와 현황: 용어 정의, 현재 사용 사례, 도구/플랫폼 현황\n",
      "  2) 실용성 및 도입 사례: 산업별 적용 사례, 장점·한계, 도입 장애 요인\n",
      "  3) 소프트웨어 개발자의 미래: AI 보조 coding의 진화, 스킬 체인 변화, 고용 시장과 교육의 적응\n",
      "\n",
      "- Search Queries (각 코어 토픽당 3–5개 예시)\n",
      "  1) 최신 개발 동향/뉴스\n",
      "     - \"바이브 코딩 최신 동향\" / \"vibe coding latest developments\"\n",
      "     - \"감정 기반 코딩 도구 신제품\" / \"emotion-based coding tools 2024-2025\"\n",
      "  2) 주요 통계나 데이터\n",
      "     - \"바이브 코딩 도입 현황\" / \"adoption rate of mood-based coding techniques\"\n",
      "     - \"AI 보조 개발 도구 사용률 통계\" / \"AI-assisted coding market size\"\n",
      "  3) 전문가 분석/연구\n",
      "     - \"전문가 분석 vibe coding\" / \"studies on emotion-aware programming\"\n",
      "     - \"사례 연구 mood-based development\" / \"case studies of ambient/interactive coding\"\n",
      "  4) 미래 전망\n",
      "     - \"바이브 코딩의 미래 전망\" / \"future of mood-based programming\"\n",
      "     - \"AI와 개발자의 역할 변화 예측\" / \"impact of AI on software developer skills\"\n",
      "\n",
      "참고로, 위 예시는 “바이브 코딩”을 일반적인 분위기/감정 기반 코딩으로 해석한 가정 하의 설계안입니다. 실제 용어 정의에 맞춰 코어 토픽과 질의를 정확히 재구성해 드리겠습니다. 원하시는 해석을 알려주시면, 그에 맞춰 2–3개의 핵심 주제와 3–5개의 검색 쿼리로 바로 맞춤형 FOCUSED 계획을 작성해 드리겠습니다.\n",
      "---------- TextMessage (research_planner) ----------\n",
      "확인된 가정: 본 계획은 “바이브 코딩”을 분위기/감정 기반의 코딩 방식 혹은 인터랙티브한 분위기 인식과 연계된 개발 도구들로 해석한 것입니다. 다른 정의가 있다면 알려주시면 그에 맞춰 재구성하겠습니다.\n",
      "\n",
      "FOCUSED 연구 계획: 바이브 코딩과 소프트웨어 개발자의 미래\n",
      "\n",
      "1) Core Topics (2–3개)\n",
      "- 바이브 코딩의 정의와 현황\n",
      "  - 용어 정의, 현재 사용 사례, 도구/플랫폼의 현황 및 비교\n",
      "- 실용성/도입 사례와 한계\n",
      "  - 산업별 적용 사례, 생산성/품질에 주는 영향, 도입 장애 요인\n",
      "- 소프트웨어 개발자의 미래 변화\n",
      "  - AI 보조 코딩의 진화, 개발자 스킬 체인 변화, 교육 및 고용시장 영향\n",
      "\n",
      "2) Search Queries (4개; 각 항목은 최신 동향, 통계, 전문가 분석, 미래 전망을 포괄)\n",
      "- 최신 개발 동향 및 뉴스\n",
      "  - \"emotion-based coding tools 2024 2025 latest developments\" (감정 기반 코딩 도구 2024–2025 최신 동향)\n",
      "  - \"바이브 코딩 최신 동향\" (한국어 자료 포함)\n",
      "- 주요 통계나 데이터\n",
      "  - \"adoption rate of emotion-based coding tools\" / \"감정 기반 코딩 도구 채택 현황\"\n",
      "  - \"AI-assisted coding usage statistics 2023-2025\" / \"AI 보조 코딩 사용 통계 2023–2025\"\n",
      "- 전문가 분석이나 연구\n",
      "  - \"studies on emotion-aware programming\" / \"감정 인식 프로그래밍 연구\"\n",
      "  - \"case studies mood-based development\" / \"기분 기반 개발 사례 연구\"\n",
      "- 미래 전망\n",
      "  - \"future of mood-based programming\" / \"바이브 코딩의 미래 전망\"\n",
      "  - \"impact of AI on software developer skills\" / \"AI가 소프트웨어 개발자 스킬에 미치는 영향\"\n",
      "\n",
      "추가 안내\n",
      "- 위 계획은 빠르게 시작할 수 있도록 4개 질의로 구성했습니다. 필요 시 1–2개 주제를 더 세분화하거나, 특정 산업(예: 금융, 헬스케어)에 맞춘 사례 연구를 추가로 넣어 맞춤화해 드리겠습니다.\n",
      "- 용어 정의가 확정되면, 핵심 주제의 세부 하위 주제 목록과 각 주제별 구체적 자료 목록(저널, 컨퍼런스, 보고서)을 함께 제시해 드리겠습니다.\n",
      "---------- ToolCallRequestEvent (research_agent) ----------\n",
      "[FunctionCall(id='call_MDdhX9RZh3sRp9e9V2HGdIZp', arguments='{\"query\": \"emotion-based coding tools 2024 2025 latest developments\"}', name='web_search_tool'), FunctionCall(id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', arguments='{\"query\": \"바이브 코딩 최신 동향\"}', name='web_search_tool'), FunctionCall(id='call_4PAMAyFwVRDo2ioyrSmVFaI2', arguments='{\"query\": \"emotion-aware programming studies\"}', name='web_search_tool'), FunctionCall(id='call_MZOCIL7HSdei6STfRI8fQssA', arguments='{\"query\": \"AI-assisted coding usage statistics 2023-2025\"}', name='web_search_tool')]\n",
      "---------- ToolCallExecutionEvent (research_agent) ----------\n",
      "[FunctionExecutionResult(content='Unexpected error during search: Status code 502. Server returned non-JSON response: \\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>502 Server Error</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Server Error</h1>\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\n<h2></h2>\\n</body></html>\\n - Full response status: 502', name='web_search_tool', call_id='call_MDdhX9RZh3sRp9e9V2HGdIZp', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'2025년 바이브 코딩 AI 개발 동향 보고서\\', \\'url\\': \\'https://seo.goover.ai/report/202506/go-public-report-ko-e086645e-079a-44f8-ae63-13f0358d262d-0-0.html\\', \\'markdown\\': \"![]( 2025년 바이브 코딩 AI 개발 동향 보고서일반 리포트2025년 06월 16일!## 목차1. 요약2. 서론3. 데이터브릭스의 통합 AI 플랫폼 전략4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황5. 최고의 AI 코드 생성 도구 비교 분석6. 미래 전망 및 시사점7. 결론## 1. 요약- 2025년 바이브 코딩 AI 개발 동향 리포트에서는 인공지능 기술의 발전에 따라 변화하고 있는 소프트웨어 개발 환경을 다룹니다. 특히, \\'바이브 코딩\\'이라는 새로운 개념을 중심으로 데이터브릭스의 통합 플랫폼 전략과 스타트업 경쟁 구도를 분석합니다. 이 보고서의 핵심 발견은, 데이터브릭스가 제시하는 레이크베이스 플랫폼과 AI 자동화 도구가 기업들이 데이터와 AI를 효과적으로 통합하고 활용할 수 있는 환경을 제공한다는 것입니다. 바이브 코딩 스타트업인 Cursor, Windsurf, Codeium 등의 사례와 90억 달러 이상의 기업 가치를 통해 AI 코드 생성 시장의 동향이 더욱 뚜렷하게 드러나며, 개발자 역할의 변화와 더불어 AI 도구의 중요성이 지속적으로 증가하고 있음을 보여줍니다.## 2. 서론- AI 기술이 급속히 발전함에 따라 소프트웨어 개발의 패러다임이 전환되고 있습니다. 이러한 변화의 중심에 자리한 \\'바이브 코딩\\'은 개발 지식이 부족한 사용자들도 손쉽게 코드를 작성할 수 있는 기회를 제공합니다. 비록 이 기술이 새로운 가능성을 창출하지만, 동시에 기존의 개발자 일자리에 위협이 되고 있습니다. 본 리포트는 바이브 코딩을 통해 신생 기업과 대규모 플랫폼 간의 경쟁 구도, 그리고 AI 코드 생성 도구의 진화에 대해 심층적으로 분석하고자 합니다. 현재 시장에서 주목받고 있는 데이터브릭스의 레이크베이스 플랫폼 전략을 시작으로, 여러 스타트업 사례와 AI 코드 생성 도구의 특징을 살펴보며, 미래의 개발 환경이 어떤 방향으로 나아갈지를 예측합니다.## 3. 데이터브릭스의 통합 AI 플랫폼 전략- 인공지능의 세계가 진화하는 과정에서 데이터브릭스는 두 가지 측면에서 주목할 만한 변화를 이끌고 있습니다. 첫째, 인공지능의 사용 범위를 확장하는 \\'바이브 코딩(vibe coding)\\' 개념의 도입과 둘째, 이를 바탕으로 한 새로운 플랫폼인 레이크베이스(Lakebase)의 출시입니다. 데이터브릭스는 근본적으로 기업의 데이터 생태계를 통합하고, AI 중심의 혁신을 통해 기업들이 데이터를 더 효과적으로 활용할 수 있도록 새로운 방향성을 제시하고 있습니다.- ### 3-1. ‘바이브 코딩’ 개념과 Databricks의 레이크베이스(Lakebase) 출시- ‘바이브 코딩’은 데이터브릭스가 최근 발표한 혁신적인 개념으로, 자연어 기반의 AI 코드 생성과 업무 자동화를 통합한 것을 의미합니다. 사용자가 원하는 작업을 자연어로 설명하면, 시스템은 내장된 데이터와 AI 모델을 활용하여 적절한 코드를 자동으로 작성합니다. 이 접근 방식은 개발자들이 직접 코드를 입력하는 대신 시스템과 대화하며 작업을 수행할 수 있도록 돕습니다. 이는 기존의 코드 중심 개발 방식을 넘어서는 중요한 이정표로서, AI의 역할을 더 효율적이고 인간 친화적으로 만들어 줍니다.- 데이터브릭스는 데이터+AI 서밋 2025에서 새로운 플랫폼인 레이크베이스를 공식 출시했습니다. 레이크베이스는 서버리스 OLTP(온라인 트랜잭션 처리) 데이터베이스로, PostgreSQL과 호환되며 오브젝트 스토리지와의 통합 기능을 갖추고 있습니다. 이 플랫폼은 데이터 처리의 효율성과 확장성을 모두 만족시키며, 기업들이 데이터 분석, 트랜잭션 처리, AI 개발을 하나의 플랫폼 내에서 유기적으로 연결될 수 있는 환경을 제공합니다. 이는 기업의 데이터 생태계를 통합하는 데 있어 중대한 시점입니다.- ### 3-2. Agent Bricks와 AI 판사 기능 설명- 에이전트 브릭스(Agent Bricks)는 데이터브릭스의 또 다른 혁신으로, 자연어 명령을 통해 정보 추출, 지식 지원, 멀티에이전트 감시 등의 역할을 수행하는 AI 에이전트를 자동으로 생성할 수 있는 도구입니다. 이 시스템은 사용자 오류를 최소화하고, 정보의 질을 높이는 방향으로 설계되었습니다.- 특히 주목할 만한 것은 AI 판사(judges) 기능입니다. 이 기능은 AI 에이전트의 성능을 모니터링하고 평가하는 데 사용되며, 특정 기준에 따라 생성된 결과의 질을 점검하고 개선하는 역할을 합니다. 이를 통해 에이전트들은 학습하고 발전할 수 있으며, 최적의 결과를 얻기 위해 코드 수정 과정을 연구하게 됩니다. 데이터브릭스는 이러한 기능들을 통해 정보 활용의 효율성을 극대화하고, AI의 잠재력을 더욱 확장하고자 합니다.- ### 3-3. 플랫폼 통합으로 달성하는 데이터·AI 워크플로우 혁신- 데이터브릭스의 통합 AI 플랫폼 전략은 단순한 기술 통합을 넘어 데이터와 AI가 밀접하게 연결된 생태계를 형성하는 데 기여하고 있습니다. BI(비즈니스 인텔리전스) 툴과 머신러닝 모델을 통합함으로써 기업들은 데이터 기반 의사결정을 신속히 내릴 수 있는 능력을 갖추게 됩니다. 이는 데이터 흐름을 자동화하고, 실시간으로 분석을 가능하게 하여 비즈니스의 민첩성을 높이는 중요한 단계입니다.- 임무별 에이전트와 AI 분석 툴 간의 협업은 기업 내에서 심층적인 데이터 분석을 수행할 수 있도록 하며, 사용자들은 실시간으로 인사이트를 확보하게 됩니다. 이러한 혁신은 기존의 데이터 분석, 데이터 관리 및 AI 모델에 대한 접근 방식을 바꾸고 있으며, 기업들이 더욱 빠르게 변화하는 시장에서 경쟁력을 유지할 수 있도록 돕습니다. 데이터브릭스는 AI 자동화와 데이터 관리의 통합을 통해 이 전환을 가속화하고 있으며, 이는 향후 기업의 데이터와 AI 활용 방식에 지대한 영향을 미칠 것입니다.## 4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황- AI 기술의 급속한 발전은 소프트웨어 개발 생태계에 혁신적인 변화를 불러오고 있습니다. 특히, 바이브 코딩(vibe coding)이라는 개념이 등장하면서 일반 사용자도 손쉽게 코드 작성이 가능해졌습니다. 이는 프로그래밍 지식을 가진 개발자들에게만 국한되지 않고, 폭넓은 사용자에게 소프트웨어 개발의 기회를 제공합니다. 이러한 변화는 단순히 기술적인 것에 그치지 않고 투자 시장에서도 큰 물결을 일으키고 있습니다. 이번 섹션에서는 바이브 코딩 스타트업의 경쟁 구도와 최근 투자 현황에 대해 분석하겠습니다.- 2025년 현재, 바이브 코딩 시장에서 주목받고 있는 스타트업은 Cursor, Windsurf, Codeium 등입니다. 특히, Cursor는 최근 90억 달러의 기업 가치에 도달하며 10억 달러 규모의 투자를 유치했습니다. 이들은 \\'AI 코드 생성\\'을 통해 프로그래밍 언어에 대한 깊은 이해 없이도 소프트웨어를 개발하는 것을 가능하게 하여, 이를 통해 많은 사용자가 개발에 참여할 수 있도록 하고 있습니다. Windsurf와 Codeium 역시 30억 달러 가치를 인정받으며, 오픈AI의 인수 협상 대상으로 떠오르는 등 바이브 코딩 시장에서 활발한 활동을 보이고 있습니다.- ### 4-1. Cursor·Windsurf·Codeium 등 주요 스타트업 사례- Cursor는 인공지능을 활용한 코드 생성 플랫폼을 제공하며, 사용자가 쉽게 코드 스니펫을 생성하고 통합할 수 있도록 돕습니다. 또한, 이 플랫폼은 커뮤니티와의 협업 기능을 강화하여 다양한 사용자가 서로의 코드 작업을 공유할 수 있게 하며, 이는 소프트웨어 개발이 전통적인 개발 흐름을 넘어선 새로운 차원으로 나아가게 합니다. Windsurf는 Codeium 개발을 통해 비개발자들도 코드 작성에 참여할 수 있는 환경을 조성하고 있으며, 이를 통해 소프트웨어 개발의 인력 부족 문제를 해결하고 있습니다.- 또한, 이들 스타트업들은 단순한 코드 작성 도구를 넘어, 사용자 경험을 최적화하고 개발자 커뮤니티와의 연결성을 증대시키기 위한 방향으로 발전하고 있습니다. 이들은 AI 코딩 도구의 수요와 더불어, 사용자 친화적인 인터페이스를 제공함으로써 모바일 기기 및 다양한 플랫폼에서의 접근성을 높이고 있습니다.- ### 4-2. AI 코드 생성 스타트업의 밸류에이션 상승 배경- AI 코드 생성 스타트업들의 밸류에이션 상승은 몇 가지 주요 요인에 기인합니다. 첫째, 기업들이 AI 도입을 통해 업무 생산성을 극대화하고 싶어하는 경향이 있습니다. 소프트웨어 개발 분야에서 AI의 활용은 코딩을 자동화하고, 반복적인 업무를 감소시켜 기업들이 원하는 시간 안에 프로젝트를 완수하도록 돕고 있습니다. 이에 따라 AI 코드 생성 스타트업은 높은 수익 성장 잠재력을 지닌 것으로 평가받고 있습니다.- 둘째, AI 기술에 대한 투자자들의 관심이 급증하면서 스타트업들이 쉽게 자금을 조달할 수 있는 환경이 조성되었습니다. 특정 투자자들은 AI가 가져올 수 있는 혁신성과 시장의 미래 가능성을 높게 평가하고 있으며, 이는 스타트업 가치의 상승으로 이어지고 있습니다.- ### 4-3. 엔트리 레벨 개발자 일자리 변화 및 빅테크 진출 현황- AI의 도입은 엔트리 레벨 개발자 일자리에 큰 변화를 일으키고 있습니다. 최근 통계에 따르면, AI가 반복적이고 기본적인 코딩 작업을 대체하면서 1년 미만 경력의 개발자 신규 채용이 24% 감소한 것으로 나타났습니다. 이는 기술 발전이 개발자들의 일자리 환경에 어떠한 영향을 끼치고 있는지를 잘 보여줍니다.- 더불어, 구글, 아마존, 마이크로소프트(MS)와 같은 빅테크 기업들은 자체적인 AI 코드 생성 제품을 출시하며 이 시장에 진입하고 있습니다. 이러한 방식으로 이들 기업은 인건비를 절감하고 효율성을 극대화하고 있습니다. 예를 들어, MS는 올해 6000명 감원을 통해 개발자 역할의 변화가 불가피함을 강조하며, 자동화 기술이 고용에 미치는 영향을 조명하고 있습니다.## 5. 최고의 AI 코드 생성 도구 비교 분석- AI 코드 생성 도구는 현대 소프트웨어 개발의 필수 요소로 자리 잡았습니다. 코드 생성의 혁신이 이루어짐에 따라, 개발자들은 반복적인 작업에서 벗어나 더 창의적이고 전략적인 업무에 집중할 시간을 확보할 수 있게 되었습니다. 이러한 도구들은 단순히 코드를 생성하는 것을 넘어서, 개발자의 작업 흐름을 혁신하고, 팀 협업을 강화하며, 코드 품질을 향상시키는 역할을 합니다. 따라서 다양한 AI 코드 생성 도구들의 기능, 장점과 단점을 분석하는 것은 이 시대의 개발 환경을 이해하는 데 필요합니다.- AI 코드 생성기는 이제 단순한 도우미가 아니라, 실제로 개발과정의 핵심적인 파트너로 성장하고 있습니다. 특정 도구의 선택은 프로젝트의 성공 여부를 크게 좌우할 수 있으며, 여러 도구 중 어떤 것이 가장 적합한지는 사용자의 요구와 팀의 구조에 따라 달라질 수 있습니다. 따라서 이러한 도구들을 비교하고 분석하는 것은 매우 중요한 작업입니다.- ### 5-1. 10가지 주요 AI 코드 생성기 기능·장단점 요약- 다양한 AI 코드 생성기 중에서 10개의 주요 도구를 선정하여 그 기능과 장단점을 요약하였습니다. 각 도구는 특정 기능에 특화되어 있으며, 사용 용도에 따라 효과적인 선택이 이루어져야 합니다. 예를 들어, GitHub Copilot은 강력한 코드 자동 완성 기능과 통합된 학습 기능을 통해 개인 개발자와 팀 모두에게 유용하나, 보안상의 이유로 민감한 프로젝트에는 적합하지 않을 수 있습니다.- 반면에, Replit은 사용자 친화적인 인터페이스와 팀 협업 기능을 제공하여 초보자와 경력 개발자 모두에게 적합합니다. 그러나 상대적으로 많은 기능이 통합되어 인프라 관리에 추가적인 학습이 필요할 수 있습니다. 각 도구의 인상적인 기능은 다음과 같습니다. 1) GitHub Copilot: 예측 코드 생성, 다국어 기능, 지속적인 학습. 2) Replit: 실시간 협업, 객체 저장소, 에이전트 기능. 3) Cursor: 스마트 자동 완성 및 상황 인식. 4) Lovable.dev: 풀스택 개발 환경, 연속적 호스팅, GitHub 통합.- 각 도구의 장단점을 제대로 이해하고 활용한다면, 기업의 코드 작성 프로세스를 획기적으로 개선할 수 있습니다.- ### 5-2. Replit 등 대표 플랫폼의 에이전트·배포·협업 기능- Replit은 클라우드 기반 환경에서 에이전트를 활용하여 애플리케이션 개발을 지원하는 대표적인 플랫폼입니다. 에이전트 v2는 사용자의 대화 내용을 바탕으로 자연어로 명령을 수행하고, 코드를 작성하는 과정에서 기계 학습을 통해 더 나은 결과를 생성합니다. 사용자는 오히려 더 많은 시간을 절약하고 창의적인 작업에 집중할 수 있습니다.- 또한, Replit의 협업 기능은 팀 프로그래밍에 최적화되어 있습니다. 팀원들은 실시간으로 함께 코드 작성, 디버깅 및 배포 작업을 수행할 수 있어 작업 효율성을 높입니다. 이러한 협업 기능은 특히 소규모 팀과 스타트업에서 효과적으로 나타납니다. 하지만 클라우드 기반 서비스이기 때문에 인터넷 연결이 필수적이며, 민감한 데이터를 처리하는 데 있어 일부 사용자에게 불안감을 줄 수 있습니다.- 편리함과 안정성만을 찾는 사용자를 위해 Windsurf와 같은 로컬 IDE 기반의 코드 편집기 또한 주목받고 있습니다. 이는 클라우드와의 연결이 필요 없으며, 오프라인 상태에서도 높은 성능을 발휘합니다. 각 플랫폼의 특성을 고려하여 조합하면 더 효과적인 개발 환경을 구축할 수 있습니다.- ### 5-3. 도구별 활용 시나리오 및 성능 비교- AI 코드 생성 도구들은 각각의 기능과 특성에 따라 다양한 활용 시나리오가 존재합니다. 예를 들어, GitHub Copilot은 대규모 오픈소스 프로젝트에서 일반적으로 사용되며, 누구에게나 접근이 용이한 코드 제안을 통해 신속한 개발을 도와줍니다. 와 같은 대규모 솔루션에서는 보안이 필수적이므로 지극히 주의가 필요합니다.- 반면 Replit은 시작 단계의 스타트업이나 부트캠프 등에서 인기를 끌고 있으며, 초보자들이 스크립트를 작성하고 간단한 애플리케이션을 만들기에 적합합니다. 이러한 교육적인 측면도 고려해야 합니다. Windsurf는 보다 고급 코드 편집 및 오프라인 환경이 필요한 사용자에게 적합하여 프리랜서 개발자들이나 개인 프로젝트를 추진하는 이들에게 이상적입니다.- 기본적으로 각 도구의 성능은 사용자의 요구와 프로젝트의 특성에 따라 매우 다르게 나타납니다. 예를 들어, 고급 사용자는 지연 시간과 성능을 중시할 수 있지만, 초보자는 사용자 경험과 접근성에 중점을 두는 경향이 있습니다. 그러므로 이러한 도구를 선택할 때에는 반드시 사용자의 요구와 환경을 충분히 고려해야 합니다.## 6. 미래 전망 및 시사점- AI의 발전은 단순한 기술 혁신을 넘어, 인류의 생활 방식과 일하는 방식을 근본적으로 변화시키고 있습니다. 특히 바이브 코딩의 확산은 이러한 변화의 중심에서 새로운 가능성을 열어주고 있습니다. 이 과정을 통해 개발자, 기업, 그리고 사회 전체가 어떻게 적응할 것인지에 대한 논의는 앞으로의 방향성을 잡는 데 중요합니다. 따라서 AI의 자동화가 가져올 개발자 역할의 변화와 플랫폼과 스타트업 간의 연계, 기술의 상호 발전 방향에 대한 통찰이 필요합니다.- ### 6-1. 주요 기술의 상호 보완적 발전 방향- AI 코드 생성기와 자연어 프로세싱(NLP) 기술의 발전은 서로를 보완하며 지속적으로 진화하고 있습니다. 예를 들어, Replit과 같은 플랫폼은 비 프로그래머도 소프트웨어 개발에 참여할 수 있도록 도와주며, 이는 결국 개발 생태계의 다변화를 가져옵니다. 이처럼 다양한 도구가 상호 연결됨으로써, 사람들이 코드를 작성하는 방식을 극 대화하는 기회를 제공합니다. 또한, AI 기반 플랫폼은 사용자가 입력한 자연어 명령을 바탕으로 즉각적인 피드백과 결과를 제공하여, 개발 속도를 높이고 품질을 개선합니다. 이는 코드 작성에서의 인간의 역량을 한 단계 끌어올리는 데 기여하고 있으며, 향후 이러한 기술의 발전 방향은 더욱 협업적이고 직관적인 개발 환경으로 이어질 것입니다. 따라서 이러한 혁신이 어떻게 산업 전체에 파급효과를 미칠 것인가에 대한 연구가 필요합니다.## 7. 결론- 이번 보고서를 통해 바이브 코딩 그리고 AI 기술이 소프트웨어 개발의 혁신을 이끌고 있음을 확인할 수 있었습니다. 데이터브릭스의 통합 플랫폼 전략이 기업들이 데이터 활용을 극대화하는 데 기여하는 동시에, 스타트업들이 생겨나는 환경은 기회이자 도전과제로 작용합니다. 특히, AI 코드 생성 도구들은 단순한 코드 생성기를 넘어, 개발자들의 업무 방식을 혁신하며 이들의 지속적인 활용이 요구됩니다. 앞으로의 연구는 이러한 기술들이 기업과 개인의 개발 환경에 미칠 장기적인 영향을 심화하여 분석하고, 변화하는 시장 환경에 대한 비즈니스 전략을 재정립하는 데 초점을 맞춰야 하겠습니다. AI 기술의 발전은 결국 인간의 일하는 방식을 변화시키고 있으며, 이에 따라 새로운 접근과 대처가 필요합니다.## 용어집- 바이브 코딩: 개발 지식이 부족한 사용자도 손쉽게 코드를 작성할 수 있도록 돕는 인공지능 기반의 코드 생성 시스템을 의미.- 데이터브릭스: 기업의 데이터 생태계를 통합하고 AI 중심의 혁신을 통해 데이터를 효과적으로 활용할 수 있도록 지원하는 플랫폼.- 레이크베이스(Lakebase): 데이터브릭스에서 출시한 서버리스 OLTP 데이터베이스로, PostgreSQL과 호환되며 데이터 처리의 효율성과 확장성을 제공.- AI 자동화: 인공지능 기술을 활용하여 반복적이고 기본적인 작업을 자동으로 수행하도록 하는 프로세스.- 에이전트 브릭스(Agent Bricks): 데이터브릭스의 도구로, 자연어 명령을 통해 AI 에이전트를 자동으로 생성 및 관리하는 기능을 가지고 있음.- AI 판사(judges): AI 에이전트의 생성 결과를 평가하고 개선하는 기능으로, 품질을 모니터링하는 역할을 함.- BI(비즈니스 인텔리전스): 기업이 데이터를 분석하고 제공하여 비즈니스 의사결정을 지원하는 기술과 시스템을 통칭.- 스타트업: 신생 기업으로, 주로 혁신적인 제품이나 서비스를 바탕으로 빠른 성장을 목표로 하는 비즈니스 모델.- 코드 생성기: 인공지능 기술을 활용하여 소프트웨어 개발자가 아닌 사용자가 코드 작성을 가능하게 도움.- AI 코드 생성: 인공지능이 자동으로 소프트웨어 코드를 작성하는 기술.- 머신러닝: 인공지능의 한 분야로, 컴퓨터가 데이터를 학습하고 예측할 수 있도록 하는 알고리즘 및 기술.- 서버리스: 서버 관리 없이 클라우드에서 애플리케이션을 실행할 수 있는 아키텍처 모델.- 클라우드 기반: 인터넷을 통해 클라우드 서비스 제공업체가 제공하는 리소스를 활용하는 방식.출처 문서- **데이터브릭스, \\'바이브 코딩\\'으로 기업 AI 생태계 통합 선언 - TokenPost** - **AI \\'바이브 코딩\\' 스타트업, 몸값 고공행진…수익성은 글쎄 : 네이트 뉴스** - **바이브 코딩을 위한 최고의 AI 코드 생성기 10가지 (2025년 XNUMX월) - Unite.AI** \"}, {\\'title\\': \\'개발자는 결국 대체될까? 바이브코딩 트렌드 리포트\\', \\'url\\': \\'https://eopla.net/magazines/32364\\', \\'markdown\\': \\'![]( 결국 대체될까? 바이브코딩 트렌드 리포트2025. 07. 23![]( 이 글은  에서 발행되었습니다.>> 가장 전문적인 바이브코딩 인사이트를 매주 받아보세요.>> [[구독하러 가기]]( 이미지]( 최근 가장 핫하고 변화도 빠른 분야입니다.매일 새로운 서비스가 등장하고 이를 소개하는 컨텐츠도 많은데요,그중\\\\xa0**팀차원에서, 실무수준으로 운영가능한 코드** 를 작성하는 관점의 컨텐츠는 거의 없습니다.이번 레터에서는 바이브코딩 컨설팅업체를 운영하며 느낀 바이브코딩의 현 위치와,\\\\xa0**제대로 사용하기위한 준비사항** 들을 소개해보겠습니다.## **바이브코딩이란?**바이브코딩은 쉽게 말해\\\\xa0AI가 대신 코딩하는 방법론입니다.어려운 개발언어 공부할 필요 없이 \"계산기 만들어줘\"처럼 요청하면 되는거죠.아티클을 더 읽고 싶다면?이오플래닛에 가입해주세요.로그인 후 모든 아티클을 무료로 볼 수 있어요.]( 7]( 뉴스레터]( 어썸데브 · CEO 댓글 5]( studio약 2달 전]( 뉴스레터 님의 아티클이 EO 뉴스레터에 실렸습니다. 이번 주 이오레터를 확인하세요!>>> ]( 0]( l 가능성에 반응하는 사람들약 2달 전]( 쉽게 이해할 수 있어 잘 읽었습니다. 한 두개 프로잭트에 활용해 봐야 겠어요. 고맙습미다.]( 0](\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0개발자약 2달 전]( 실력에서 나온 통찰력 있는 내용이라 배우고 갑니다.다음글도 기대가 되네요~]( 0]( 코리아\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Manager약 2달 전]( 맥을 주사 바늘처럼 정교하게 찔러주셔서많은 영감을 얻고 갑니다. 향후 글 기대 됩니다.]( 1]( 뉴스레터어썸데브\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0CEO약 2달 전]( 정말 감사합니다! 앞으로도 좋은 컨텐츠 남기겠습니다. :)]( 0추천 아티클최바울  페오펫]( 약 3년 전]( 약 3년 전디오니소스  디오니소스\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Owner]( 1년 이상 전]( Product Owner  1년 이상 전[# 사업전략# 마인드셋# 트렌드# 기타‘챗GPT의 아버지’ 샘 알트만에 대해 알아보자 [아티클 한 눈에 보기]0.샘 알트만이라는 독특한 캐릭터1.\"이해 받지 못하는 것이 강점이다\"2.CEO는 어떻게 시간을 써야 하는가3.실리콘밸리 요다의 3가지 깨달음4.핵융합, 기본소득, 규제, 정치적 올바름\\\\xa0\\\\xa02022년 여름부터 연일 헤드라인을 장식하는 사람이 ...](  eo\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0에디터]( 2년 이상 전]( 에디터  2년 이상 전]( 뉴스레터]( 어썸데브 · CEO ]( 7![](\\'}]', name='web_search_tool', call_id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'Real time emotion aware applications: A case study employing ...\\', \\'url\\': \\'https://www.sciencedirect.com/science/article/abs/pii/S0169260712000831\\', \\'markdown\\': \\' - - - - Other access optionsSearch ScienceDirect## Article preview- - - - - ]( \"Go to Computer Methods and Programs in Biomedicine on ScienceDirect\")## , July 2012, Pages 16-27]( Real time emotion aware applications: A case study employing emotion evocative pictures and neuro-physiological sensing enhanced by Graphic Processor UnitsAuthor links open overlay panel , , , Show moreAdd to MendeleyCite ## AbstractIn this paper the feasibility of adopting  Units towards real-time emotion aware computing is investigated for boosting the time  employed in such applications. The proposed methodology was employed in analysis of encephalographic and electrodermal data gathered when participants passively viewed emotional evocative stimuli. The  effectiveness when processing electroencephalographic and electrodermal recordings is demonstrated by comparing the  of chaos/complexity analysis through  (multi-channel correlation dimension/D2) and  (computation of  level/SCL) into various popular programming environments. Apart from the beneficial role of parallel programming, the adoption of special design techniques regarding  may further enhance the time minimization which approximates a factor of 30 in comparison with ANSI C language (single-core sequential execution). Therefore, the use of  parallel capabilities offers a reliable and  for real-time sensing the user\\\\\\'s affective state.## IntroductionEmotion aware computing was for a large period a neglected topic in the scientific community [1]. However, recent neuroscience findings have highlighted the critical role of emotions in a variety of cognitive functions like decision making [2], memory [3] and perception [4]. These arguments demonstrated the significance of emotional intelligence [5] not only when interacting with other people but also between human and machines [6]. Therefore, motivated research efforts investigate how to provide computers with abilities to recognize the user\\\\\\'s emotional state and to naturally adapt to it [7]. The importance of emotion aware computing is desirable only in cases where the user should interact with the machine in order to achieve high performance during the task procedure that should be accomplished [8]. So, providing the machine with the capability to robustly sense the users’ negative feelings [9] (frustration, anger, stress, anxiety, disappointment, etc.) the appropriate feedback may be given to neutralize their mood [10] and to encourage them to improve their performance in several applications like tests controlled through computer [11], virtual gaming [12] or remote monitoring of elderly or disabled people [13], [14]. Initial research attempts have demonstrated that the core element of a successful affective computing system is its ability to emulate the ways that are employed in the communication between human beings [15]. The pioneering work of MIT group led to the introduction of the “Affective Computing” term and to the establishment of a framework that could be adopted for a successful human–computer interaction (HCI) system [16], while also dealing with the challenges that have to be faced and the expectations created by potential applications [8].Previous research attempts have adopted communicative ways like facial expressions [17] and posture recognition [18]. However, several limitations occur since these modalities are highly dependent from the users’ personality [19] and their culture, resulting thus in enhanced inter-subject variability. Robust emotion recognition assumes the utilization of exaggerated expressions that are unlikely to be elicited in real-life situations [20]. Moreover, the use of cameras produces huge amount of data, while also communicates irrelevant information (e.g. subject\\\\\\'s identity) which the user may be unwilling to reveal [7]. Since the aforementioned methodologies are based on the recognition of externally expressed emotions, some innermost may not be easily recognized [8]. Such feelings are not easily communicated even among human beings and may be better recognized by neuro-physiological sensing [7]. Data fusion [21] from both the central and the autonomic nervous system may create discrete emotional patterns for a wide range of emotions [22], which are poorly distinguishable otherwise. However, special care should be given to the experimental methodology used for emotion elicitation.So, a key issue towards the achievement of a robust emotion aware computerized system is the establishment of a framework that is in close connection with the modern emotional theory assuring thus the reliable emotion elicitation. Recent trends regard emotions as behavioral attitudes related with evolutionary processes aiming to assure the human\\\\\\'s survival and perpetuation [23], [24], [25]. Therefore, each situation may be judged as either a pleasant or an unpleasant one. Its importance modulates the activation level needed in order to confront the stimulus appeared. Erotic or life-threatening situations require higher activation degree than melancholic or relaxing occasions. Adopting this notion, a bi-directional model was proposed. According to this approach, emotional processing is governed by two motivational systems which are the appetitive approach dealing with the pleasant situations and the defensive one activated in case of life-threatening occasions. The activation of the aforementioned systems is described through the valence dimension, while the activation degree is represented by the arousal dimension. So, these affective variables form a 2D emotional space.The International Affective Picture System (IAPS) collection adopts the aforementioned emotional model and provides a variety of affective visual stimuli as well as their normative ratings for both the arousal and valence dimension [26]. The use of this picture collection with simultaneous neurophysiological recordings demonstrated the facilitated encoding of emotional stimuli [27]. The combination of central nervous (event-related potentials/ERPs) and autonomic (electrodermal) activity revealed a significant correlation between skin conductance responses (SCRs) and the arousal ratings of the IAPS stimuli [23]. Moreover, late ERPs were more positive for emotional pictures [28], while their time course was influenced by the valence dimension [29]. A recent study investigated whether emotional processing is affected by the subject\\\\\\'s gender. Early (N100) and mid (N200) ERPs were significantly greater for female participants during passive viewing of unpleasant pictures [30].The bi-directional emotion model and the aforementioned neuroscience findings have not been widely adopted until now in the field of emotion aware computing. Relying on these notions, a Mahalanobis distance-based classification scheme was proposed for discriminating emotional instances selected from the IAPS collection. The output of the recognition sub-system was then used by an avatar which emulated the user\\\\\\'s affective state by adapting its face and voice characteristics [14]. However, there was need for further improvement of the classification accuracy by applying data mining (decision trees) and pattern recognition (Support Vector Machines) techniques [31]. Towards the achievement of a reliable emotion-aware application, extended feature fusion from different neuro-physiological modalities was proposed as well as a close connection with the theoretical emotional framework and the independency of the two emotional variables. Moreover, gender specific classifiers were proposed according to [32] in order to further enhance the method\\\\\\'s robustness which reached 81.3% for 4 emotional categories.Despite the adequate classification accuracy that was demonstrated by these research efforts, there are several open issues that should be further investigated prior to the introduction of real world emotion aware applications. The proposed discrimination framework was developed for research purposes. It is oriented towards the achievement of the optimal result employing time-consuming computations that reduce its applicability. Moreover, it has been developed as an isolated application under controlled lab environments which may differ from generic real-life applications. So, an integrative approach should be adopted for linking the emotion methodology with the acquisition subsystem as well as with the avatar behavior-generation routines. Then, the proposed system would be able to gather short segments of neuro-physiological data which are processed within fractions of seconds. The user\\\\\\'s affective state is recognized and serves as an input to the avatar which adapts its behavior either to mirror or to neutralize the user\\\\\\'s affective state.The current study investigates the feasibility of the Graphics Processing Unit (GPU) for the fast processing of neuro-physiological data. Short segments from both the central (ERPs) and the autonomic (SCRs) nervous system serve as an input to the system. These data are parallel processed during the feature extraction stage by algorithmic procedures that were re-designed in order to provide the optimal solution regarding the memory management. So, the aim of this paper is to demonstrate that the adoption of parallel processing may be greatly beneficial for the development of real-time emotion aware applications. Therefore, it is not focused to the extensive description of the parallelization techniques adopted. Moreover, it highlights some significant issues like time consumption on data transfer between host and device that should be taken into consideration during the system design in order to further minimize the execution time. So, the work\\\\\\'s contribution lays on the introduction of a framework for the adoption of parallel programming for real time emotion-aware applications.So, the remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture as well as with special programming techniques adopted for the proper parallelization of an algorithm. Then, a brief description of the parallelized algorithms is performed. Within Section 3 results of the algorithms’ implementation and the execution time are presented in Section 4. Finally, the discussion of this paper appears in Section 4.## Section snippets## The NVIDIA GPU architecture – CUDAThe voracious market demand for realtime and high definition 3D graphics led to the introduction of highly parallel, multithreaded, manycore processor Graphic Processor Unit (GPU). Characterized by high memory bandwidth and astounding computational horsepower, the GPU (Fig. 1) serves the demanding requirements of the modern designs and implementations. Its main difference with CPU is that it facilitates compute-intensive and parallel computation. Stemming from the graphics rendering demands, it## ResultsThe features (D2 complexity and SCL values) obtained from the parallel processing of electroencephalographic and autonomic data were analyzed in order to highlight differences among the various emotional states. Each emotional state is characterized by two independent variables (valence and arousal degree).Regarding the multi-channel D2 correlation dimension algorithm, the analysis was performed for each participant and for each one of the four emotional categories. As depicted in Fig. 9 (left## DiscussionThe current work aims to highlight the significant acceleration that may be achieved to emotion aware computing in case of adopting parallel programming on GPU. So, the detailed description of the parallelization techniques are beyond the paper\\\\\\'s scope and may be found in [33], [36]. These recent code execution techniques are exploited in boosting complex and time-consuming computations, such as nonlinear dynamic analysis or processing of dense data arrays. Selected results are included in## ConclusionA novel parallel-programming approach based on the CUDA architecture was proposed in order to accelerate the processing of neurophysiological recordings requiring complex computations. It aims to facilitate the already proposed emotion discrimination methodologies with the computing solution needed in order to perform real-time classification. To this end, the importance of this work towards an integrative approach of providing the machines with the capabilities to adapt their behavior## Conflict of interestThe authors do not report any conflict of interest.Special issue articlesRecommended articles- K. Sergerie _et al._### ### Neuroscience and Biobehavioral Reviews(2008)P. Vuilleumier _et al._### ### Nature Neuroscience(2003)- R.W. Picard _et al._### ### IEEE Transactions on Pattern Analysis and Machine Intelligence(2001)- J. Klein _et al._### ### Interacting with Computers(2002)- F. Nasoz _et al._### - C.A. Frantzidis _et al._### - P.J. Lang _et al._### (1997)- C. Amrhein _et al._### ### International Journal of Psychophysiology(2004)- C. Lithari _et al._### ### Brain Topography(2010)- G. Noaje _et al._- K.C. Berridge _et al._### Affective neuroscience of pleasure: reward in humans and animals### Psychopharmacology(2008)A. Bechara _et al._### Emotion, decision making and the orbitofrontal cortex### Cerebral Cortex(2000)M. Pantic _et al._### Toward an affect-sensitive multimodal human–computer interaction### Proceedings of the IEEE(2003)E. Hudlicka### To feel or not to feel: the role of affect in human–computer interaction### International Journal of Human–Computer Studies(2003)R.W. Picard### Affective computing: challenges### International Journal of Human–Computer Studies(2003)B. Kort _et al._### An affective module for an intelligent tutoring systemE. Hudlicka### Affective computing for game designC.A. Frantzidis _et al._### Description and future trends of ICT solutions offered towards independent living: the case of LLM projectP.D. Bamidis _et al._### An integrated approach to emotion recognition for advanced emotional intelligenceB. Reeves _et al._### The Media Equation(1996)R.W. Picard### Affective Computing(1997)View more references- ### 2019, Information FusionCitation Excerpt :Mehmood et al. utilized the late positive potential (LPP)-based feature extraction method and used two classifiers (SVM an KNN) for emotion detection from EEG [44]. Konstantinidis et al. utilized K-nearest neighbors (KNN) classifiers to detect six basic emotions from a three-channel forehead EEG combined with GSR [45]. Recently, Alarcao et al. [10] presented an analysis which shows that 59% of works in the literature used SVM classifiers to detect emotions, 8% of works used different variations of SVM like adaptive SVM etc.; 14% of the works used the kNN, 6.3% of the works used Linear Discriminant Analysis (LDA) and 3.17% of the works used the Naive Bayes (NB) classifier.Show abstractRecently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.- ### 2017, Computer Methods and Programs in BiomedicineShow abstractUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.- ### 2017, Frontiers in NeuroroboticsShow abstractUsing machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features. However, the conventional subject-specific classifiers may induce additional burdens to each subject for preparing multiple-session EEG data as training sets. To this end, we developed a new EEG feature selection approach, transfer recursive feature elimination (T-RFE), to determine a set of the most robust EEG indicators with stable geometrical distribution across a group of training subjects and a specific testing subject. A validating set is introduced to independently determine the optimal hyper-parameter and the feature ranking of the T-RFE model aiming at controlling the overfitting. The effectiveness of the T-RFE algorithm for such cross-subject emotion classification paradigm has been validated by DEAP database. With a linear least square support vector machine classifier implemented, the performance of the T-RFE is compared against several conventional feature selection schemes and the statistical significant improvement has been found. The classification rate and _F_-score achieve 0.7867, 0.7526, 0.7875, and 0.8077 for arousal and valence dimensions, respectively, and outperform several recent reported works on the same database. In the end, the T-RFE based classifier is compared against two subject-generic classifiers in the literature. The investigation of the computational time for all classifiers indicates the accuracy improvement of the T-RFE is at the cost of the longer training time.- ### 2014, Plos OneShow abstractInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method\\\\\\'s practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.- ### 2014, IEEE Transactions on Affective Computing- ### 2013, Computational and Mathematical Methods in MedicineCopyright © 2012 Elsevier Ireland Ltd. All rights reserved.The Identity Selector: Persistence Service\\'}, {\\'title\\': \\'Towards Emotion-awareness in Programming Education with ...\\', \\'url\\': \\'https://ieeexplore.ieee.org/document/9842520/\\', \\'markdown\\': \"Loading [MathJax]/extensions/TeX/ieee_stixext.jsTowards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation | IEEE Conference Publication | IEEE XploreSkip to Main Content# Towards Emotion-awareness in Programming Education with Behavior-based Emotion EstimationPublisher: IEEECite ThisPDF; ; ; ; ; All AuthorsSign In or Purchase1Cites inPaper239FullText Views- AlertsAdd to Citation Alerts* * *- Download PDF- Download References- Request Permissions- Save to- Alerts## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learnin...Show More## Metadata## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learning to program, rich emotions are experienced which can provide valuable feedback to their teachers. To accurately model stu-dents\\' emotions, this paper adopts the well-recognized model of emotions during complex learning that involves four states: engaged, confused, frustrated, and bored. To perform continuous estimation of students\\' emotions in a non-intrusive manner, this paper proposes to track students\\' programming behavior and estimate their corresponding emotional states. Compare to the existing approaches on acquiring the students\\' emotional states with self-reports or bio-sensors, the proposed approach is more feasible in conducting real-world, and large-scale studies for not requiring extensive human interventions or additional devices. Evaluated using data collected from a real-world course project, the proposed approach is showed to be promising for achieving an estimation accuracy of 72.06 % for the above four emotional states. As an enabling technology, the proposed is potentially useful in supporting many applications and improve the quality of programming education in computer science.**Published in:** **Date of Conference:** 27 June 2022 - 01 July 2022**Date Added to IEEE _Xplore_:** 10 August 2022**ISBN Information:****Print on Demand(PoD) ISSN:** 0730-3157**DOI:** Publisher: IEEE**Conference Location:** Los Alamitos, CA, USA## Funding Agency:![]( Contents* * *### I. IntroductionBoth learning and computer programming are high cog-nitively demanding tasks which involve the experience of complex emotions during the process [1]–[3]. Studies from the areas of education [3], psychology [2], and software engineering [1], [4], [5] have more than once shown the significance of one\\'s emotions in affecting the performance and productivity of the learners and developers, respectively. Bringing the above researches together, this paper studies the problem of understanding the students\\' emotions during the task of learning to program.Sign in to Continue Reading## Authors## Figures## References## Citations## Keywords## Metrics## Footnotes]( Like This2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)Published: 20162024 International Conference on Inventive Computation Technologies (ICICT)Published: 2024Show More**References is not available for this document.**### IEEE Account- - ### Purchase Details- - - ### Profile Information- - - ### Need Help?- **US & Canada:** +1 800 678 4333- **Worldwide:** +1 732 981 0060- - - - - - - - - A not-for-profit organization, IEEE is the world\\'s largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.The Identity Selector: Persistence Service\"}]', name='web_search_tool', call_id='call_4PAMAyFwVRDo2ioyrSmVFaI2', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'AI | 2025 Stack Overflow Developer Survey\\', \\'url\\': \\'https://survey.stackoverflow.co/2025/ai\\', \\'markdown\\': \\'### 84% of respondents are using or planning to use AI tools in their development process, an increase over last year (76%). This year we can see 51% of professional developers use AI tools daily.Do you currently use AI tools in your development process?     All RespondentsYes, I use AI tools daily47.1%Yes, I use AI tools weekly17.7%Yes, I use AI tools monthly or infrequently13.7%No, but I plan to soon5.3%No, and I don\\\\\\'t plan to16.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,662(68.7%)Professional DevelopersYes, I use AI tools daily50.6%Yes, I use AI tools weekly17.4%Yes, I use AI tools monthly or infrequently12.8%No, but I plan to soon4.6%No, and I don\\\\\\'t plan to14.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,004(53%)Learning to CodeYes, I use AI tools daily39.5%Yes, I use AI tools weekly18.7%Yes, I use AI tools monthly or infrequently15.1%No, but I plan to soon7.2%No, and I don\\\\\\'t plan to19.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,843(5.8%)Early Career DevsYes, I use AI tools daily55.5%Yes, I use AI tools weekly18.1%Yes, I use AI tools monthly or infrequently11.5%No, but I plan to soon2.5%No, and I don\\\\\\'t plan to12.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,360(13%)Early career defined as 1 - 5 years work experienceMid Career DevsYes, I use AI tools daily52.8%Yes, I use AI tools weekly16.8%Yes, I use AI tools monthly or infrequently13.5%No, but I plan to soon3.7%No, and I don\\\\\\'t plan to13.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,997(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsYes, I use AI tools daily47.3%Yes, I use AI tools weekly17.2%Yes, I use AI tools monthly or infrequently13%No, but I plan to soon6%No, and I don\\\\\\'t plan to16.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 13,001(26.5%)Experienced dev defined as 10+ years work experience### Conversely to usage, positive sentiment for AI tools has decreased in 2025: 70%+ in 2023 and 2024 to just 60% this year. Professionals show a higher overall favorable sentiment (61%) than those learning to code (53%).How favorable is your stance on using AI tools as part of your development workflow?     All RespondentsVery favorable22.9%Favorable36.8%Indifferent17.6%Unsure2.3%Unfavorable10.8%Very unfavorable9.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,412(68.2%)Professional DevelopersVery favorable23.5%Favorable37.7%Indifferent17.4%Unsure1.8%Unfavorable10.6%Very unfavorable9.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,814(52.7%)Learning to CodeVery favorable19.3%Favorable33.5%Indifferent16.6%Unsure4.3%Unfavorable13.6%Very unfavorable12.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,812(5.7%)Early Career DevsVery favorable22.8%Favorable40.3%Indifferent17%Unsure1.3%Unfavorable10.3%Very unfavorable8.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,293(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery favorable23.8%Favorable38.9%Indifferent16.2%Unsure1.5%Unfavorable11%Very unfavorable8.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,957(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery favorable23.9%Favorable36%Indifferent18.1%Unsure2.1%Unfavorable10.3%Very unfavorable9.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,941(26.4%)Experienced devs defined as 10+ years work experience### More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report \"highly trusting\" the output. Experienced developers are the most cautious, with the lowest \"highly trust\" rate (2.6%) and the highest \"highly distrust\" rate (20%), indicating a widespread need for human verification for those in roles with accountability.How much do you trust the accuracy of the output from AI tools as part of your development workflow?     All RespondentsHighly trust3.1%Somewhat trust29.6%Somewhat distrust26.1%Highly distrust19.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,244(67.8%)Professional DevelopersHighly trust2.7%Somewhat trust29.6%Somewhat distrust26.3%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,701(52.4%)Learning to CodeHighly trust6.1%Somewhat trust31.3%Somewhat distrust24.2%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,781(5.7%)Early Career DevsHighly trust3%Somewhat trust31.1%Somewhat distrust25.7%Highly distrust17.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,254(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsHighly trust2.8%Somewhat trust30.3%Somewhat distrust26.1%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,931(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsHighly trust2.5%Somewhat trust28.6%Somewhat distrust26.7%Highly distrust20.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,908(26.3%)Experienced devs defined as 10+ years work experience### In 2024, 35% of professional developers already believed that AI tools struggled with complex tasks. This year, that number has dropped to 29% among professional developers and is consistent amongst experience levels. Complex tasks carry too much risk to spend extra time proving out the efficacy of AI tools.How well do the AI tools you use in your development workflow handle complex tasks?     All RespondentsVery well at handling complex tasks4.4%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.1%Bad at handling complex tasks22%Very poor at handling complex tasks17.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know16.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,230(67.8%)Professional DevelopersVery well at handling complex tasks3.9%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.2%Bad at handling complex tasks22.8%Very poor at handling complex tasks18.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know15.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,695(52.4%)Learning to CodeVery well at handling complex tasks7.9%Good, but not great at handling complex tasks25.8%Neither good or bad at handling complex tasks12.4%Bad at handling complex tasks19%Very poor at handling complex tasks16.3%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,779(5.7%)Early Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks28.1%Neither good or bad at handling complex tasks13.4%Bad at handling complex tasks23.6%Very poor at handling complex tasks19.2%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know11.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,258(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks25.4%Neither good or bad at handling complex tasks13.8%Bad at handling complex tasks23.9%Very poor at handling complex tasks19.5%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know13.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,922(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery well at handling complex tasks3.6%Good, but not great at handling complex tasks23.5%Neither good or bad at handling complex tasks14.9%Bad at handling complex tasks22.1%Very poor at handling complex tasks17.9%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,901(26.3%)Experienced dev career defined as 10+ years work experience### Developers show the most resistance to using AI for high-responsibility, systemic tasks like Deployment and monitoring (76% don\\\\\\'t plan to) and Project planning (69% don\\\\\\'t plan to).Which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently Mostly AISearch for answers54.1%Generating content or synthetic data35.8%Learning new concepts or technologies33.1%Documenting code30.8%Creating or maintaining documentation24.8%Learning about a codebase20.8%Debugging or fixing code20.7%Testing code17.9%Writing code16.9%Predictive analytics11%Project planning10.8%Committing and reviewing code10.2%Deployment and monitoring6.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,202(22.9%)Currently Partially AISearch for answers55.8%Generating content or synthetic data28.6%Learning new concepts or technologies47.4%Documenting code30.3%Creating or maintaining documentation27.3%Learning about a codebase32.7%Debugging or fixing code47.1%Testing code27.5%Writing code59%Predictive analytics12.7%Project planning17.1%Committing and reviewing code22.6%Deployment and monitoring10.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,991(42.8%)Plan to Partially Use AISearch for answers24%Generating content or synthetic data28%Learning new concepts or technologies27.9%Documenting code30.5%Creating or maintaining documentation32.5%Learning about a codebase34.9%Debugging or fixing code30.9%Testing code34.7%Writing code32.4%Predictive analytics25%Project planning24.8%Committing and reviewing code31.4%Deployment and monitoring25%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,518(45.9%)Plan to Mostly Use AISearch for answers17.2%Generating content or synthetic data28.9%Learning new concepts or technologies15.7%Documenting code28.6%Creating or maintaining documentation31.8%Learning about a codebase23.1%Debugging or fixing code14.8%Testing code25.8%Writing code12.4%Predictive analytics23%Project planning14.3%Committing and reviewing code16.3%Deployment and monitoring15.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,790(26.1%)Don\\\\\\'t Plan to Use AI for This TaskSearch for answers19.6%Generating content or synthetic data38.2%Learning new concepts or technologies32.3%Documenting code38.5%Creating or maintaining documentation39.6%Learning about a codebase39.4%Debugging or fixing code36.4%Testing code44.1%Writing code28.9%Predictive analytics65.6%Project planning69.2%Committing and reviewing code58.7%Deployment and monitoring75.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,349(51.7%)### Respondents who said they are currently using mostly AI tools to complete tasks in the development workflow are highly satisfied with and frequently using AI to search for answers or learn new concepts; respondents plan to mostly use AI in the future for documentation and testing tasks and are slightly less satisfied with the tools they are using now.How favorable is your stance on using AI tools as part of your development workflow and which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently mostly AINumber of responses6,053685Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents5.255.35.355.45.455.55.555.65.65%5%10%15%20%25%30%35%40%45%50%55Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,184(22.8%)Currently partially AINumber of responses12,3822,194Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.74.754.84.854.94.9555.055.15.155.25.25%10%15%20%25%30%35%40%45%50%55%60Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,980(42.8%)Plan to partially use AINumber of responses7,8585,400Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents3.73.83.944.14.24.34.44.54.64.74.84.9%24%25%26%27%28%29%30%31%32%33%34%35Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,500(45.9%)Plan to mostly use AINumber of responses4,0561,588Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.64.654.74.754.84.854.94.9555.055.15.155.2%12%14%16%18%20%22%24%26%28%30%32Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,777(26.1%)Don\\\\\\'t plan to use AI for this taskNumber of responses19,2114,953Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents2.42.62.833.23.43.63.84%20%25%30%35%40%45%50%55%60%65%70%75%80Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,332(51.7%)### The biggest single frustration, cited by 66% of developers, is dealing with \"AI solutions that are almost right, but not quite,\" which often leads to the second-biggest frustration: \"Debugging AI-generated code is more time-consuming\" (45%)When using AI tools, which of the following problems or frustrations have you encountered? Select all that apply.All RespondentsAI solutions that are almost right, but not quite66%Debugging AI-generated code is more time-consuming45.2%I don’t use AI tools regularly23.5%I’ve become less confident in my own problem-solving20%It’s hard to understand how or why the code works16.3%Other (write in):11.6%I haven’t encountered any problems4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,476(64.2%)### In a future with advanced AI, the #1 reason developers would still ask a person for help is \"When I don’t trust AI’s answers\" (75%). This positions human developers as the ultimate arbiters of quality and correctness.In the future, if AI can do most coding tasks, in which situations would you still want to ask another person for help? Select all that apply.All RespondentsWhen I don’t trust AI’s answers75.3%When I have ethical or security concerns about code61.7%When I want to fully understand something61.3%When I want to learn best practices58.1%When I’m stuck and can’t explain the problem54.6%When I need help fixing complex or unfamiliar code49.8%When I want to compare different solutions44.1%When I need quick help troubleshooting27.5%Other6.1%I don’t think I’ll need help from people anymore4.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 29,163(59.5%)### Most respondents are not vibe coding (72%), and an additional 5% are emphatic it not being part of their development workflow.In your own words, is \"vibe coding\" part of your professional development work? For this question, we define vibe coding according to the , the process of generating software from LLM prompts.     All RespondentsYes, emphatically0.4%Yes11.9%Yes, somewhat2.8%I have tried it2.1%Not sure1.2%No72.2%No, emphatically5.3%Uncategorized4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,564(54.2%)18-24 years oldYes, emphatically0.3%Yes11.6%Yes, somewhat3.2%I have tried it2.4%Not sure1.2%No72.8%No, emphatically5.1%Uncategorized3.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 4,212(8.6%)25-34 years oldYes, emphatically0.4%Yes11.8%Yes, somewhat3.2%I have tried it1.6%Not sure1.3%No72.3%No, emphatically5.7%Uncategorized3.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,526(17.4%)35-44 years oldYes, emphatically0.5%Yes12%Yes, somewhat2.8%I have tried it2.2%Not sure1.1%No72%No, emphatically5.4%Uncategorized4.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 7,607(15.5%)45-54 years oldYes, emphatically0.5%Yes12.7%Yes, somewhat2.5%I have tried it1.9%Not sure1.3%No71.3%No, emphatically5.2%Uncategorized4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,838(7.8%)55-64 years oldYes, emphatically0.8%Yes11.4%Yes, somewhat2%I have tried it3.1%Not sure1.5%No71.3%No, emphatically4.6%Uncategorized5.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 1,657(3.4%)### AI agents are not yet mainstream. A majority of developers (52%) either don\\\\\\'t use agents or stick to simpler AI tools, and a significant portion (38%) have no plans to adopt them.Are you using AI agents in your work (development or otherwise)? AI agents refer to autonomous software entities that can operate with minimal to no direct human intervention using artificial intelligence techniques.    All RespondentsYes, I use AI agents at work daily14.1%Yes, I use AI agents at work weekly9%Yes, I use AI agents at work monthly or infrequently7.8%No, but I plan to17.4%No, I use AI exclusively in copilot/autocomplete mode13.8%No, and I don\\\\\\'t plan to37.9%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,877(65%)Professional DevelopersYes, I use AI agents at work daily14.9%Yes, I use AI agents at work weekly9.2%Yes, I use AI agents at work monthly or infrequently7.7%No, but I plan to17.2%No, I use AI exclusively in copilot/autocomplete mode14.2%No, and I don\\\\\\'t plan to36.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 24,752(50.5%)Learning to CodeYes, I use AI agents at work daily13.2%Yes, I use AI agents at work weekly7.8%Yes, I use AI agents at work monthly or infrequently7.4%No, but I plan to15.6%No, I use AI exclusively in copilot/autocomplete mode12.1%No, and I don\\\\\\'t plan to44.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,610(5.3%)Professional AI UsersYes, I use AI agents at work daily17.5%Yes, I use AI agents at work weekly10.8%Yes, I use AI agents at work monthly or infrequently8.9%No, but I plan to18.6%No, I use AI exclusively in copilot/autocomplete mode16.3%No, and I don\\\\\\'t plan to27.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,892(42.6%)Learning AI UsersYes, I use AI agents at work daily16.5%Yes, I use AI agents at work weekly9.6%Yes, I use AI agents at work monthly or infrequently8.7%No, but I plan to16.9%No, I use AI exclusively in copilot/autocomplete mode14.7%No, and I don\\\\\\'t plan to33.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,019(4.1%)### 52% of developers agree that AI tools and/or AI agents have had a positive effect on their productivity.Have AI tools or AI agents changed how you complete development work in the past year?All RespondentsYes, to a great extent16.3%Yes, somewhat35.3%Not at all or minimally41.4%No, but my development work has significantly changed due to non-AI factors2.6%No, but my development work has changed somewhat due to non-AI factors4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,636(64.5%)### If you happen to be using AI agents at work and you are a software developer, chances are high that you are using agents for software development (84%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.Industry PurposeSoftware engineering83.5%Data and analytics24.9%IT operations18%Business process automation17.6%Decision intelligence11.3%Customer service support11.2%Marketing8.6%Cybersecurity7.4%Robotics3.9%Other2.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,301(25.1%)### TL;DR: Agents used outside of work are mostly used for language processing tasks (49%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.General PurposeLanguage processing49%Integration with external agents and APIs38.3%MCP servers34.4%Agent/multi-agent orchestration28.1%Vector databases for AI applications24.1%Multi-platform search enablement19.4%Personalized agent creation18.3%Other3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,797(11.8%)### The most recognized impacts are personal efficiency gains, and not team-wide impact. Approximately 70% of agent users agree that agents have reduced the time spent on specific development tasks, and 69% agree they have increased productivity. Only 17% of users agree that agents have improved collaboration within their team, making it the lowest-rated impact by a wide margin.To what extent do you agree with the following statements regarding the impact of AI agents on your work as a developer?All Respondents27.3%35.9%21.3%8.2%7.3%AI agents have accelerated my learning about new technologies or codebases.29.3%34.9%22.4%7%6.4%AI agents have helped me automate repetitive tasks.17.1%31.9%25.3%14.2%11.5%AI agents have helped me solve complex problems more effectively.6.6%10.7%40.5%20%22.2%AI agents have improved collaboration within my team.12.2%25.3%32.4%17.1%13.1%AI agents have improved the quality of my code.27.7%41%20.4%6%4.9%AI agents have increased my productivity.29.3%40.8%17.8%6.9%5.1%AI agents have reduced the time spent on specific development tasks.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,823(26.2%)### Is it a learning curve, or is the tech not there yet? 87% of all respondents agree they are concerned about the accuracy, and 81% agree they have concerns about the security and privacy of data.To what extent do you agree with the following statements regarding AI agents?All Respondents57.1%29.8%9.7%2.3%1.1%I am concerned about the accuracy of the information provided by AI agents.56.1%25.3%11.7%4.7%2.2%I have concerns about the security and privacy of data when using AI agents.16.5%29.7%37.3%12.6%3.9%Integrating AI agents with my existing tools and workflows can be difficult.15.5%27.9%31.8%17.8%6.9%It takes significant time and effort to learn how to use AI agents effectively.13.8%14.4%30.6%15%26.2%My company\\\\\\'s IT and/or InfoSec teams have strict rules that do not allow me to use AI agent tools or platforms25.4%27.9%31.8%10.3%4.6%The cost of using certain AI agent platforms is a barrier.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 28,930(59%)### When it comes to data management for agents, traditional, developer-friendly tools like Redis (43%) are being repurposed for AI, alongside emerging vector-native databases like ChromaDB (20%) and pgvector (18%).You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent memory or data management in the past year?All RespondentsRedis42.9%GitHub MCP Server42.8%supabase20.9%ChromaDB19.7%pgvector17.9%Neo4j12.3%Pinecone11.2%Qdrant8.2%Milvus5.2%Fireproof5%LangMem4.8%Weaviate4.5%LanceDB4.4%mem04%Zep2.8%Letta2.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,398(6.9%)### The agent orchestration space is currently led by open-source tools. Among developers building agents, Ollama (51%) and LangChain (33%) are the most-used frameworks.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent orchestration or agent frameworks in the past year?All RespondentsOllama51.1%LangChain32.9%LangGraph16.2%Vertex AI15.1%Amazon Bedrock Agents14.5%OpenRouter13.4%Llama Index13.3%AutoGen (Microsoft)12%Zapier11.8%CrewAI7.5%Semantic Kernel6%IBM watsonx.ai5.7%Haystack4.4%Smolagents3.7%Agno3.4%phidata2.1%Smol-AGI1.9%Martian1.7%lyzr1.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,758(7.7%)### Developers are primarily adapting their existing, traditional monitoring tools for this new task, rather than adopting new, AI-native solutions. The most used tools for AI agent observability are staples of the DevOps and application monitoring world: Grafana + Prometheus are used by 43% of agent developers, and Sentry is used by 32%.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent observability, monitoring or security in the past year?All RespondentsGrafana + Prometheus43%Sentry31.8%Snyk18.2%New Relic13%LangSmith12.5%Honeycomb8.8%Langfuse8.8%Wiz6.9%Galileo6.2%Adversarial Robustness Toolbox (ART)5.5%Protect AI5%Vectra AI4.4%arize3.7%helicone3.2%Metero2.7%opik2.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,689(5.5%)### ChatGPT (82%) and GitHub Copilot (68%) are the clear market leaders, serving as the primary entry point for most developers using out-of-the-box AI assistance.You indicated you use or develop AI agents as part of your development work. Have you used any of the following out-of-the-box agents, copilots or assistants?All RespondentsChatGPT81.7%GitHub Copilot67.9%Google Gemini47.4%Claude Code40.8%Microsoft Copilot31.3%Perplexity16.2%v0.dev9.1%Bolt.new6.5%Lovable.dev5.7%AgentGPT5%Tabnine5%Replit5%Auto-GPT4.7%Amazon Codewhisperer3.9%Blackbox AI3.5%Roo code (Roo-Cline)3.4%Cody3%Devin AI2.7%Glean (Enterprise Agents)1.3%OpenHands (formerly OpenDevin)1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,323(17%) \\'}, {\\'title\\': \\'AI-Generated Code Stats 2025: How Much Is Written by AI?\\', \\'url\\': \\'https://www.elitebrains.com/blog/aI-generated-code-statistics-2025\\', \\'markdown\\': \"26 min read08/27/2025# AI-Generated Code Stats 2025: How Much Is Written by AI?#### AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development.Hey there, fellow tech enthusiasts! One thing is crystal clear as we look to the future: artificial intelligence (AI) is rapidly changing how we write code. From building entire functions to completing code snippets in milliseconds, AI is helping developers work faster and smarter. But the big question on everyone’s mind is: **How much of that code is AI-generated?** And more importantly, should developers or  worry about their jobs as AI evolves?As the founder of EliteBrains, where we focus on helping developers stay sharp with coding tests and skills development, I want to break down the AI-generated code stats for 2025, explore the tools that are making this happen, and give you a better understanding of what the future holds for software developers in this new world of AI-assisted coding.As AI takes over more of the repetitive coding tasks, the real question becomes how we measure human skill alongside machine-generated output. That’s exactly where **EliteBrains helps.**Put AI to the test and see which of your developers are truly qualified.\\\\xa0Mix and match AI-powered coding tests from 120+ programming languages, choose the right seniority level, and instantly spot who rises above the rest.Create your\\\\xa0**FREE AI coding tests today** **( only\\\\xa03 free slots available)**## **What Is AI-Generated Code?**AI-generated code is programming written or completed by artificial intelligence tools from natural language prompts, existing context or prior code patterns. In 2025, it typically means an AI assistant drafted the solution or produced most of the final lines that a developer then reviews and refines.## **What Percentage of \\\\xa0Code Is AI-Generated in 2025?**AI has been in the software development game for a while now, but its role in coding is reaching new heights in 2025. It’s not just about suggesting code anymore. AI tools are writing **massive** amounts of code, optimizing development, and helping developers be more efficient than ever before.Here’s the shocking truth: . That number alone shows just how integrated AI tools like **GitHub Copilot** have become in developers’ workflows. Whether it’s generating entire functions or suggesting small snippets of code, AI is now deeply embedded into the way we code.## **How Much Code Has AI Written So Far?**But just how much code is AI generating? Let’s break it down.![]( **How Does AI Code Generation Affect Developer Jobs and the Economy?**With AI generating such a massive amount of code, one big question looms: What does this mean for job security and the economy?![]( **How Many Lines of Code Have Been Generated by AI?**Buckle up, because here’s where it gets mind-blowing. As of 2024, **256 billion lines of code have already been generated by AI**. Think about that for a second—**256 billion lines**. That’s an insane amount of code, and we’re just getting started. From simple scripts to full-fledged software systems, AI is helping to write an overwhelming amount of code across all industries.This number is so massive that it\\'s hard to grasp. Consider this: the **total lines of code** written by humans in the last decade likely pales in comparison to this explosion of AI-generated code. And we’re still in the early stages of integrating AI tools into the development process. The future will see even more code churned out by AI as the tools become smarter, faster, and more efficient.## **How Is Google Using AI to Write Code in 2025?**Let’s look at how AI is playing a huge role in tech giants like Google. Google, known for its cutting-edge AI innovations, has been using AI to power some of its most important software. According to Sundar Pichai, Google’s CEO, **over a quarter of all new code generated by Google is written by AI**. This includes everything from Google Cloud services to Android apps. AI is also playing a significant role in developing machine learning models that power a large chunk of Google’s products.Pichai’s announcement isn’t just a marketing stunt—it’s a reflection of how deeply integrated AI has become in Google’s development process. Whether it\\'s AI-driven infrastructure management or software development for consumer-facing products, Google is leading the way in showing just how powerful AI can be for developers.Let’s dive deeper into why Google has taken this approach. With over 20,000 engineers working on various products, relying on AI to handle repetitive, time-consuming tasks such as code generation allows the company to save countless hours. This, in turn, frees up developers to focus on more complex tasks that require problem-solving skills, creativity, and innovation.## **Will AI Replace Software Developers in 2025?**As AI continues to make its mark on software development, many developers are wondering if their jobs are at risk. Should they be worried? The short answer is **no**but let’s unpack that.AI isn’t here to replace developers. Far from it. AI tools are designed to assist developers, not eliminate them. These tools automate repetitive and mundane tasks, such as writing boilerplate code or generating simple functions. But when it comes to the creative, high-level work of designing software, making complex decisions, and debugging, developers are still very much in the driver’s seat.For example, imagine a scenario where a developer is working on building a web application. AI can generate the basic structure of the code such as setting up routing, creating a simple login form, or connecting to a database but it cannot make decisions about the best UI layout for user experience or develop the logic that handles complex business rules. That’s where the developer’s creativity and problem-solving skills come into play.AI is an assistant, not a replacement. It helps developers write code faster, but it doesn’t have the creativity, problem-solving skills, or domain knowledge that human developers bring to the table. In fact, AI is actually helping developers become more productive, allowing them to focus on more important aspects of the development process.The key takeaway here is that AI is making software development more efficient, but it still needs a human touch for the final product to succeed.### **How Much Code Is AI-Generated on GitHub in 2025?**GitHub is one of the largest code repositories in the world, and it has seen a massive increase in AI-generated code in recent years. That’s not a small chunk—it’s a significant portion of the code that powers everything from web applications to mobile apps.This doesn’t mean that developers are being replaced. Instead, it shows that developers are using AI to streamline their workflows and focus on more complex tasks. **GitHub Copilot** is perhaps the most popular AI tool used for code generation on the platform. It suggests entire lines of code and can even write entire functions based on developer input. It’s designed to help developers write code faster, with fewer bugs, and more efficiently.By integrating Copilot into their workflow, developers save time and avoid getting bogged down by menial tasks, ultimately improving both speed and quality.## **Which Programming Languages and Frameworks Does AI Write Best?**One of the most exciting things about AI in software development is its ability to generate code across a wide range of programming languages and frameworks. Here’s a breakdown of the most popular ones:#### **Python:**Python has become one of the most popular programming languages, especially in fields like data science, machine learning, and web development. AI tools like GitHub Copilot are particularly skilled at generating Python code for these use cases, whether you’re building machine learning models or working with web frameworks like Django and Flask.#### **JavaScript:**JavaScript is the backbone of web development, and AI tools are excellent at generating code for JavaScript frameworks like React, Angular, and Vue.js. AI can generate entire components, hooks, and functions in JavaScript, saving developers a ton of time and effort.#### **Java:**Java remains one of the most widely used programming languages in enterprise software development. AI can help developers generate Java code for popular frameworks like Spring and Hibernate. AI can also generate code for building microservices and APIs, speeding up development time.#### **C and C++:**AI tools are also capable of generating code in low-level programming languages like C and C++, which are crucial for systems programming and embedded systems. AI is particularly helpful in optimizing low-level code for performance and efficiency.#### **Ruby, Swift, PHP, and Go:**AI is also being used to generate code in languages like Ruby, Swift (for iOS development), PHP, and Go. These languages are frequently used for web development, mobile apps, and cloud applications, and AI is helping developers write faster and more efficient code across these platforms.## **What Are the Best AI Code Generation Tools in 2025?**Now, let’s take a look at the tools that are making this all happen. These tools are transforming the way developers work, making coding faster and more efficient:#### **GitHub Copilot:**GitHub Copilot is arguably the most well-known AI tool for developers. Powered by OpenAI’s Codex, Copilot offers suggestions for entire lines or blocks of code as developers work. It supports a wide range of languages and frameworks, including Python, JavaScript, Java, and more. Copilot is designed to help developers write code faster and with fewer errors.Source: #### **Amazon CodeWhisperer:**Amazon’s CodeWhisperer is another powerful AI tool designed to assist developers in generating code for cloud-based applications. It integrates seamlessly with Amazon Web Services (AWS), allowing developers to write code for serverless applications, APIs, and more. CodeWhisperer is especially useful for developers working in the AWS ecosystem.#### **Tabnine:**Tabnine is an AI-powered code completion tool that integrates with popular code editors like VS Code, JetBrains, and more. It offers context-aware code suggestions to help developers write code more quickly and efficiently. Tabnine works with over 40 programming languages, making it a versatile tool for developers.#### **Replit Ghostwriter:**Replit’s Ghostwriter is another AI tool designed for real-time collaborative coding. It provides developers with AI-generated code suggestions and helps speed up the development process. Whether you’re working on a solo project or collaborating with others, Ghostwriter helps you write code faster and more efficiently.**Should Developers Be Worried About AI in Coding?**So, let’s get back to the big question: should developers worry about their jobs in an AI-driven world? The answer is no, but there’s a catch.While AI is indeed transforming the way we write code, it’s not replacing developers. Instead, it’s empowering them to be more productive and efficient. Developers are still needed for creative problem-solving, high-level system design, and debugging. AI tools can generate code, but they can’t design systems, make architectural decisions, or think creatively about solutions. That’s where developers come in.The same idea holds true across industries. AI can build websites or generate campaigns, but real growth still comes from people who understand the field. Veterinary practices, for example, often lean on expert guidance like this , just as tech companies depend on developers for the creativity and insight that AI cannot replace.What’s really changing is the developer’s role. Instead of getting bogged down with repetitive tasks like boilerplate code, they now have the chance to focus on more interesting, high-value work. Those who embrace AI as a tool rather than view it as a threat will find their jobs not just safe but more important than ever.### **How Can Developers Stay Ahead of AI in Coding with EliteBrains?**At , we understand the importance of staying ahead of the curve. As AI continues to transform the software development industry, we’re here to help developers stay competitive. With our **coding tests**, developers can hone their skills, stay updated on new trends, and ensure they’re prepared for the future of software development.Our platform allows developers to:- **Practice with AI-assisted tools** and sharpen their skills- **Test their knowledge** in real-world coding environments- Stay up to date with **emerging trends in AI and software development**- **Learn how to integrate AI** into their development workflow to increase efficiencyAt EliteBrains, we’re committed to helping developers succeed in an AI-powered world, ensuring they remain competitive and continue to thrive.### **Conclusion: AI and Developers—A Winning Partnership**The rise of AI in software development is reshaping the way we write code, but it’s not here to replace developers. Instead, it’s here to make their jobs easier and more efficient. By embracing AI tools like GitHub Copilot, Amazon CodeWhisperer, and Tabnine, developers can work smarter, not harder.AI is an assistant, not a replacement. The key for developers is to adapt and use these tools to their advantage. The future of software development is collaborative—AI working alongside developers to create better, faster, and more innovative software.With **EliteBrains**, developers can stay ahead of the curve, test their skills, and learn how to leverage AI to boost their productivity. The future of coding is bright, and it’s full of exciting opportunities for those who are willing to embrace the change.01. 02. 03. 04. 05. 06. 07. 08. 09. 10. #### Our clients say!With senior resources from EliteBrains, we are able to drive our digital harmonization agenda: Migration of different reporting platforms to QlikSense analytics and Harmonization of different digital detailing systems into Veeva CLM.!Jiri Matousek, Head of CRM at Hexal AG!EliteBrains provided us with a top PHP developer who has been working for us for more than 4 years. Jan became indispensable part of our team within few weeks.!Dave Etchells, CEO at Imaging-resource.com!Each time we are in a need of new software developer, we just contact the guys from EliteBrains. Their network is huge so they provide us with demanded sources very quickly.!Martin Svach, CEO at Starkys club#### Useful articles]( Do Resumes Still Matter for Backend Developers in 2025? Here’s How to Build One That Gets You Hired Faster 02. 09. 2025 . 5 min read]( AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development. 27. 08. 2025 . 11 min readSign up now to see more tipsSubscribecancel anytime#### Hiring?We picked the best developers for you#### Looking for a job?## This website uses cookiesWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Accept allManage preferencesChat Widget\"}]', name='web_search_tool', call_id='call_MZOCIL7HSdei6STfRI8fQssA', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (research_agent) ----------\n",
      "Unexpected error during search: Status code 502. Server returned non-JSON response: \n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      " - Full response status: 502\n",
      "[{'title': '2025년 바이브 코딩 AI 개발 동향 보고서', 'url': 'https://seo.goover.ai/report/202506/go-public-report-ko-e086645e-079a-44f8-ae63-13f0358d262d-0-0.html', 'markdown': \"![]( 2025년 바이브 코딩 AI 개발 동향 보고서일반 리포트2025년 06월 16일!## 목차1. 요약2. 서론3. 데이터브릭스의 통합 AI 플랫폼 전략4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황5. 최고의 AI 코드 생성 도구 비교 분석6. 미래 전망 및 시사점7. 결론## 1. 요약- 2025년 바이브 코딩 AI 개발 동향 리포트에서는 인공지능 기술의 발전에 따라 변화하고 있는 소프트웨어 개발 환경을 다룹니다. 특히, '바이브 코딩'이라는 새로운 개념을 중심으로 데이터브릭스의 통합 플랫폼 전략과 스타트업 경쟁 구도를 분석합니다. 이 보고서의 핵심 발견은, 데이터브릭스가 제시하는 레이크베이스 플랫폼과 AI 자동화 도구가 기업들이 데이터와 AI를 효과적으로 통합하고 활용할 수 있는 환경을 제공한다는 것입니다. 바이브 코딩 스타트업인 Cursor, Windsurf, Codeium 등의 사례와 90억 달러 이상의 기업 가치를 통해 AI 코드 생성 시장의 동향이 더욱 뚜렷하게 드러나며, 개발자 역할의 변화와 더불어 AI 도구의 중요성이 지속적으로 증가하고 있음을 보여줍니다.## 2. 서론- AI 기술이 급속히 발전함에 따라 소프트웨어 개발의 패러다임이 전환되고 있습니다. 이러한 변화의 중심에 자리한 '바이브 코딩'은 개발 지식이 부족한 사용자들도 손쉽게 코드를 작성할 수 있는 기회를 제공합니다. 비록 이 기술이 새로운 가능성을 창출하지만, 동시에 기존의 개발자 일자리에 위협이 되고 있습니다. 본 리포트는 바이브 코딩을 통해 신생 기업과 대규모 플랫폼 간의 경쟁 구도, 그리고 AI 코드 생성 도구의 진화에 대해 심층적으로 분석하고자 합니다. 현재 시장에서 주목받고 있는 데이터브릭스의 레이크베이스 플랫폼 전략을 시작으로, 여러 스타트업 사례와 AI 코드 생성 도구의 특징을 살펴보며, 미래의 개발 환경이 어떤 방향으로 나아갈지를 예측합니다.## 3. 데이터브릭스의 통합 AI 플랫폼 전략- 인공지능의 세계가 진화하는 과정에서 데이터브릭스는 두 가지 측면에서 주목할 만한 변화를 이끌고 있습니다. 첫째, 인공지능의 사용 범위를 확장하는 '바이브 코딩(vibe coding)' 개념의 도입과 둘째, 이를 바탕으로 한 새로운 플랫폼인 레이크베이스(Lakebase)의 출시입니다. 데이터브릭스는 근본적으로 기업의 데이터 생태계를 통합하고, AI 중심의 혁신을 통해 기업들이 데이터를 더 효과적으로 활용할 수 있도록 새로운 방향성을 제시하고 있습니다.- ### 3-1. ‘바이브 코딩’ 개념과 Databricks의 레이크베이스(Lakebase) 출시- ‘바이브 코딩’은 데이터브릭스가 최근 발표한 혁신적인 개념으로, 자연어 기반의 AI 코드 생성과 업무 자동화를 통합한 것을 의미합니다. 사용자가 원하는 작업을 자연어로 설명하면, 시스템은 내장된 데이터와 AI 모델을 활용하여 적절한 코드를 자동으로 작성합니다. 이 접근 방식은 개발자들이 직접 코드를 입력하는 대신 시스템과 대화하며 작업을 수행할 수 있도록 돕습니다. 이는 기존의 코드 중심 개발 방식을 넘어서는 중요한 이정표로서, AI의 역할을 더 효율적이고 인간 친화적으로 만들어 줍니다.- 데이터브릭스는 데이터+AI 서밋 2025에서 새로운 플랫폼인 레이크베이스를 공식 출시했습니다. 레이크베이스는 서버리스 OLTP(온라인 트랜잭션 처리) 데이터베이스로, PostgreSQL과 호환되며 오브젝트 스토리지와의 통합 기능을 갖추고 있습니다. 이 플랫폼은 데이터 처리의 효율성과 확장성을 모두 만족시키며, 기업들이 데이터 분석, 트랜잭션 처리, AI 개발을 하나의 플랫폼 내에서 유기적으로 연결될 수 있는 환경을 제공합니다. 이는 기업의 데이터 생태계를 통합하는 데 있어 중대한 시점입니다.- ### 3-2. Agent Bricks와 AI 판사 기능 설명- 에이전트 브릭스(Agent Bricks)는 데이터브릭스의 또 다른 혁신으로, 자연어 명령을 통해 정보 추출, 지식 지원, 멀티에이전트 감시 등의 역할을 수행하는 AI 에이전트를 자동으로 생성할 수 있는 도구입니다. 이 시스템은 사용자 오류를 최소화하고, 정보의 질을 높이는 방향으로 설계되었습니다.- 특히 주목할 만한 것은 AI 판사(judges) 기능입니다. 이 기능은 AI 에이전트의 성능을 모니터링하고 평가하는 데 사용되며, 특정 기준에 따라 생성된 결과의 질을 점검하고 개선하는 역할을 합니다. 이를 통해 에이전트들은 학습하고 발전할 수 있으며, 최적의 결과를 얻기 위해 코드 수정 과정을 연구하게 됩니다. 데이터브릭스는 이러한 기능들을 통해 정보 활용의 효율성을 극대화하고, AI의 잠재력을 더욱 확장하고자 합니다.- ### 3-3. 플랫폼 통합으로 달성하는 데이터·AI 워크플로우 혁신- 데이터브릭스의 통합 AI 플랫폼 전략은 단순한 기술 통합을 넘어 데이터와 AI가 밀접하게 연결된 생태계를 형성하는 데 기여하고 있습니다. BI(비즈니스 인텔리전스) 툴과 머신러닝 모델을 통합함으로써 기업들은 데이터 기반 의사결정을 신속히 내릴 수 있는 능력을 갖추게 됩니다. 이는 데이터 흐름을 자동화하고, 실시간으로 분석을 가능하게 하여 비즈니스의 민첩성을 높이는 중요한 단계입니다.- 임무별 에이전트와 AI 분석 툴 간의 협업은 기업 내에서 심층적인 데이터 분석을 수행할 수 있도록 하며, 사용자들은 실시간으로 인사이트를 확보하게 됩니다. 이러한 혁신은 기존의 데이터 분석, 데이터 관리 및 AI 모델에 대한 접근 방식을 바꾸고 있으며, 기업들이 더욱 빠르게 변화하는 시장에서 경쟁력을 유지할 수 있도록 돕습니다. 데이터브릭스는 AI 자동화와 데이터 관리의 통합을 통해 이 전환을 가속화하고 있으며, 이는 향후 기업의 데이터와 AI 활용 방식에 지대한 영향을 미칠 것입니다.## 4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황- AI 기술의 급속한 발전은 소프트웨어 개발 생태계에 혁신적인 변화를 불러오고 있습니다. 특히, 바이브 코딩(vibe coding)이라는 개념이 등장하면서 일반 사용자도 손쉽게 코드 작성이 가능해졌습니다. 이는 프로그래밍 지식을 가진 개발자들에게만 국한되지 않고, 폭넓은 사용자에게 소프트웨어 개발의 기회를 제공합니다. 이러한 변화는 단순히 기술적인 것에 그치지 않고 투자 시장에서도 큰 물결을 일으키고 있습니다. 이번 섹션에서는 바이브 코딩 스타트업의 경쟁 구도와 최근 투자 현황에 대해 분석하겠습니다.- 2025년 현재, 바이브 코딩 시장에서 주목받고 있는 스타트업은 Cursor, Windsurf, Codeium 등입니다. 특히, Cursor는 최근 90억 달러의 기업 가치에 도달하며 10억 달러 규모의 투자를 유치했습니다. 이들은 'AI 코드 생성'을 통해 프로그래밍 언어에 대한 깊은 이해 없이도 소프트웨어를 개발하는 것을 가능하게 하여, 이를 통해 많은 사용자가 개발에 참여할 수 있도록 하고 있습니다. Windsurf와 Codeium 역시 30억 달러 가치를 인정받으며, 오픈AI의 인수 협상 대상으로 떠오르는 등 바이브 코딩 시장에서 활발한 활동을 보이고 있습니다.- ### 4-1. Cursor·Windsurf·Codeium 등 주요 스타트업 사례- Cursor는 인공지능을 활용한 코드 생성 플랫폼을 제공하며, 사용자가 쉽게 코드 스니펫을 생성하고 통합할 수 있도록 돕습니다. 또한, 이 플랫폼은 커뮤니티와의 협업 기능을 강화하여 다양한 사용자가 서로의 코드 작업을 공유할 수 있게 하며, 이는 소프트웨어 개발이 전통적인 개발 흐름을 넘어선 새로운 차원으로 나아가게 합니다. Windsurf는 Codeium 개발을 통해 비개발자들도 코드 작성에 참여할 수 있는 환경을 조성하고 있으며, 이를 통해 소프트웨어 개발의 인력 부족 문제를 해결하고 있습니다.- 또한, 이들 스타트업들은 단순한 코드 작성 도구를 넘어, 사용자 경험을 최적화하고 개발자 커뮤니티와의 연결성을 증대시키기 위한 방향으로 발전하고 있습니다. 이들은 AI 코딩 도구의 수요와 더불어, 사용자 친화적인 인터페이스를 제공함으로써 모바일 기기 및 다양한 플랫폼에서의 접근성을 높이고 있습니다.- ### 4-2. AI 코드 생성 스타트업의 밸류에이션 상승 배경- AI 코드 생성 스타트업들의 밸류에이션 상승은 몇 가지 주요 요인에 기인합니다. 첫째, 기업들이 AI 도입을 통해 업무 생산성을 극대화하고 싶어하는 경향이 있습니다. 소프트웨어 개발 분야에서 AI의 활용은 코딩을 자동화하고, 반복적인 업무를 감소시켜 기업들이 원하는 시간 안에 프로젝트를 완수하도록 돕고 있습니다. 이에 따라 AI 코드 생성 스타트업은 높은 수익 성장 잠재력을 지닌 것으로 평가받고 있습니다.- 둘째, AI 기술에 대한 투자자들의 관심이 급증하면서 스타트업들이 쉽게 자금을 조달할 수 있는 환경이 조성되었습니다. 특정 투자자들은 AI가 가져올 수 있는 혁신성과 시장의 미래 가능성을 높게 평가하고 있으며, 이는 스타트업 가치의 상승으로 이어지고 있습니다.- ### 4-3. 엔트리 레벨 개발자 일자리 변화 및 빅테크 진출 현황- AI의 도입은 엔트리 레벨 개발자 일자리에 큰 변화를 일으키고 있습니다. 최근 통계에 따르면, AI가 반복적이고 기본적인 코딩 작업을 대체하면서 1년 미만 경력의 개발자 신규 채용이 24% 감소한 것으로 나타났습니다. 이는 기술 발전이 개발자들의 일자리 환경에 어떠한 영향을 끼치고 있는지를 잘 보여줍니다.- 더불어, 구글, 아마존, 마이크로소프트(MS)와 같은 빅테크 기업들은 자체적인 AI 코드 생성 제품을 출시하며 이 시장에 진입하고 있습니다. 이러한 방식으로 이들 기업은 인건비를 절감하고 효율성을 극대화하고 있습니다. 예를 들어, MS는 올해 6000명 감원을 통해 개발자 역할의 변화가 불가피함을 강조하며, 자동화 기술이 고용에 미치는 영향을 조명하고 있습니다.## 5. 최고의 AI 코드 생성 도구 비교 분석- AI 코드 생성 도구는 현대 소프트웨어 개발의 필수 요소로 자리 잡았습니다. 코드 생성의 혁신이 이루어짐에 따라, 개발자들은 반복적인 작업에서 벗어나 더 창의적이고 전략적인 업무에 집중할 시간을 확보할 수 있게 되었습니다. 이러한 도구들은 단순히 코드를 생성하는 것을 넘어서, 개발자의 작업 흐름을 혁신하고, 팀 협업을 강화하며, 코드 품질을 향상시키는 역할을 합니다. 따라서 다양한 AI 코드 생성 도구들의 기능, 장점과 단점을 분석하는 것은 이 시대의 개발 환경을 이해하는 데 필요합니다.- AI 코드 생성기는 이제 단순한 도우미가 아니라, 실제로 개발과정의 핵심적인 파트너로 성장하고 있습니다. 특정 도구의 선택은 프로젝트의 성공 여부를 크게 좌우할 수 있으며, 여러 도구 중 어떤 것이 가장 적합한지는 사용자의 요구와 팀의 구조에 따라 달라질 수 있습니다. 따라서 이러한 도구들을 비교하고 분석하는 것은 매우 중요한 작업입니다.- ### 5-1. 10가지 주요 AI 코드 생성기 기능·장단점 요약- 다양한 AI 코드 생성기 중에서 10개의 주요 도구를 선정하여 그 기능과 장단점을 요약하였습니다. 각 도구는 특정 기능에 특화되어 있으며, 사용 용도에 따라 효과적인 선택이 이루어져야 합니다. 예를 들어, GitHub Copilot은 강력한 코드 자동 완성 기능과 통합된 학습 기능을 통해 개인 개발자와 팀 모두에게 유용하나, 보안상의 이유로 민감한 프로젝트에는 적합하지 않을 수 있습니다.- 반면에, Replit은 사용자 친화적인 인터페이스와 팀 협업 기능을 제공하여 초보자와 경력 개발자 모두에게 적합합니다. 그러나 상대적으로 많은 기능이 통합되어 인프라 관리에 추가적인 학습이 필요할 수 있습니다. 각 도구의 인상적인 기능은 다음과 같습니다. 1) GitHub Copilot: 예측 코드 생성, 다국어 기능, 지속적인 학습. 2) Replit: 실시간 협업, 객체 저장소, 에이전트 기능. 3) Cursor: 스마트 자동 완성 및 상황 인식. 4) Lovable.dev: 풀스택 개발 환경, 연속적 호스팅, GitHub 통합.- 각 도구의 장단점을 제대로 이해하고 활용한다면, 기업의 코드 작성 프로세스를 획기적으로 개선할 수 있습니다.- ### 5-2. Replit 등 대표 플랫폼의 에이전트·배포·협업 기능- Replit은 클라우드 기반 환경에서 에이전트를 활용하여 애플리케이션 개발을 지원하는 대표적인 플랫폼입니다. 에이전트 v2는 사용자의 대화 내용을 바탕으로 자연어로 명령을 수행하고, 코드를 작성하는 과정에서 기계 학습을 통해 더 나은 결과를 생성합니다. 사용자는 오히려 더 많은 시간을 절약하고 창의적인 작업에 집중할 수 있습니다.- 또한, Replit의 협업 기능은 팀 프로그래밍에 최적화되어 있습니다. 팀원들은 실시간으로 함께 코드 작성, 디버깅 및 배포 작업을 수행할 수 있어 작업 효율성을 높입니다. 이러한 협업 기능은 특히 소규모 팀과 스타트업에서 효과적으로 나타납니다. 하지만 클라우드 기반 서비스이기 때문에 인터넷 연결이 필수적이며, 민감한 데이터를 처리하는 데 있어 일부 사용자에게 불안감을 줄 수 있습니다.- 편리함과 안정성만을 찾는 사용자를 위해 Windsurf와 같은 로컬 IDE 기반의 코드 편집기 또한 주목받고 있습니다. 이는 클라우드와의 연결이 필요 없으며, 오프라인 상태에서도 높은 성능을 발휘합니다. 각 플랫폼의 특성을 고려하여 조합하면 더 효과적인 개발 환경을 구축할 수 있습니다.- ### 5-3. 도구별 활용 시나리오 및 성능 비교- AI 코드 생성 도구들은 각각의 기능과 특성에 따라 다양한 활용 시나리오가 존재합니다. 예를 들어, GitHub Copilot은 대규모 오픈소스 프로젝트에서 일반적으로 사용되며, 누구에게나 접근이 용이한 코드 제안을 통해 신속한 개발을 도와줍니다. 와 같은 대규모 솔루션에서는 보안이 필수적이므로 지극히 주의가 필요합니다.- 반면 Replit은 시작 단계의 스타트업이나 부트캠프 등에서 인기를 끌고 있으며, 초보자들이 스크립트를 작성하고 간단한 애플리케이션을 만들기에 적합합니다. 이러한 교육적인 측면도 고려해야 합니다. Windsurf는 보다 고급 코드 편집 및 오프라인 환경이 필요한 사용자에게 적합하여 프리랜서 개발자들이나 개인 프로젝트를 추진하는 이들에게 이상적입니다.- 기본적으로 각 도구의 성능은 사용자의 요구와 프로젝트의 특성에 따라 매우 다르게 나타납니다. 예를 들어, 고급 사용자는 지연 시간과 성능을 중시할 수 있지만, 초보자는 사용자 경험과 접근성에 중점을 두는 경향이 있습니다. 그러므로 이러한 도구를 선택할 때에는 반드시 사용자의 요구와 환경을 충분히 고려해야 합니다.## 6. 미래 전망 및 시사점- AI의 발전은 단순한 기술 혁신을 넘어, 인류의 생활 방식과 일하는 방식을 근본적으로 변화시키고 있습니다. 특히 바이브 코딩의 확산은 이러한 변화의 중심에서 새로운 가능성을 열어주고 있습니다. 이 과정을 통해 개발자, 기업, 그리고 사회 전체가 어떻게 적응할 것인지에 대한 논의는 앞으로의 방향성을 잡는 데 중요합니다. 따라서 AI의 자동화가 가져올 개발자 역할의 변화와 플랫폼과 스타트업 간의 연계, 기술의 상호 발전 방향에 대한 통찰이 필요합니다.- ### 6-1. 주요 기술의 상호 보완적 발전 방향- AI 코드 생성기와 자연어 프로세싱(NLP) 기술의 발전은 서로를 보완하며 지속적으로 진화하고 있습니다. 예를 들어, Replit과 같은 플랫폼은 비 프로그래머도 소프트웨어 개발에 참여할 수 있도록 도와주며, 이는 결국 개발 생태계의 다변화를 가져옵니다. 이처럼 다양한 도구가 상호 연결됨으로써, 사람들이 코드를 작성하는 방식을 극 대화하는 기회를 제공합니다. 또한, AI 기반 플랫폼은 사용자가 입력한 자연어 명령을 바탕으로 즉각적인 피드백과 결과를 제공하여, 개발 속도를 높이고 품질을 개선합니다. 이는 코드 작성에서의 인간의 역량을 한 단계 끌어올리는 데 기여하고 있으며, 향후 이러한 기술의 발전 방향은 더욱 협업적이고 직관적인 개발 환경으로 이어질 것입니다. 따라서 이러한 혁신이 어떻게 산업 전체에 파급효과를 미칠 것인가에 대한 연구가 필요합니다.## 7. 결론- 이번 보고서를 통해 바이브 코딩 그리고 AI 기술이 소프트웨어 개발의 혁신을 이끌고 있음을 확인할 수 있었습니다. 데이터브릭스의 통합 플랫폼 전략이 기업들이 데이터 활용을 극대화하는 데 기여하는 동시에, 스타트업들이 생겨나는 환경은 기회이자 도전과제로 작용합니다. 특히, AI 코드 생성 도구들은 단순한 코드 생성기를 넘어, 개발자들의 업무 방식을 혁신하며 이들의 지속적인 활용이 요구됩니다. 앞으로의 연구는 이러한 기술들이 기업과 개인의 개발 환경에 미칠 장기적인 영향을 심화하여 분석하고, 변화하는 시장 환경에 대한 비즈니스 전략을 재정립하는 데 초점을 맞춰야 하겠습니다. AI 기술의 발전은 결국 인간의 일하는 방식을 변화시키고 있으며, 이에 따라 새로운 접근과 대처가 필요합니다.## 용어집- 바이브 코딩: 개발 지식이 부족한 사용자도 손쉽게 코드를 작성할 수 있도록 돕는 인공지능 기반의 코드 생성 시스템을 의미.- 데이터브릭스: 기업의 데이터 생태계를 통합하고 AI 중심의 혁신을 통해 데이터를 효과적으로 활용할 수 있도록 지원하는 플랫폼.- 레이크베이스(Lakebase): 데이터브릭스에서 출시한 서버리스 OLTP 데이터베이스로, PostgreSQL과 호환되며 데이터 처리의 효율성과 확장성을 제공.- AI 자동화: 인공지능 기술을 활용하여 반복적이고 기본적인 작업을 자동으로 수행하도록 하는 프로세스.- 에이전트 브릭스(Agent Bricks): 데이터브릭스의 도구로, 자연어 명령을 통해 AI 에이전트를 자동으로 생성 및 관리하는 기능을 가지고 있음.- AI 판사(judges): AI 에이전트의 생성 결과를 평가하고 개선하는 기능으로, 품질을 모니터링하는 역할을 함.- BI(비즈니스 인텔리전스): 기업이 데이터를 분석하고 제공하여 비즈니스 의사결정을 지원하는 기술과 시스템을 통칭.- 스타트업: 신생 기업으로, 주로 혁신적인 제품이나 서비스를 바탕으로 빠른 성장을 목표로 하는 비즈니스 모델.- 코드 생성기: 인공지능 기술을 활용하여 소프트웨어 개발자가 아닌 사용자가 코드 작성을 가능하게 도움.- AI 코드 생성: 인공지능이 자동으로 소프트웨어 코드를 작성하는 기술.- 머신러닝: 인공지능의 한 분야로, 컴퓨터가 데이터를 학습하고 예측할 수 있도록 하는 알고리즘 및 기술.- 서버리스: 서버 관리 없이 클라우드에서 애플리케이션을 실행할 수 있는 아키텍처 모델.- 클라우드 기반: 인터넷을 통해 클라우드 서비스 제공업체가 제공하는 리소스를 활용하는 방식.출처 문서- **데이터브릭스, '바이브 코딩'으로 기업 AI 생태계 통합 선언 - TokenPost** - **AI '바이브 코딩' 스타트업, 몸값 고공행진…수익성은 글쎄 : 네이트 뉴스** - **바이브 코딩을 위한 최고의 AI 코드 생성기 10가지 (2025년 XNUMX월) - Unite.AI** \"}, {'title': '개발자는 결국 대체될까? 바이브코딩 트렌드 리포트', 'url': 'https://eopla.net/magazines/32364', 'markdown': '![]( 결국 대체될까? 바이브코딩 트렌드 리포트2025. 07. 23![]( 이 글은  에서 발행되었습니다.>> 가장 전문적인 바이브코딩 인사이트를 매주 받아보세요.>> [[구독하러 가기]]( 이미지]( 최근 가장 핫하고 변화도 빠른 분야입니다.매일 새로운 서비스가 등장하고 이를 소개하는 컨텐츠도 많은데요,그중\\xa0**팀차원에서, 실무수준으로 운영가능한 코드** 를 작성하는 관점의 컨텐츠는 거의 없습니다.이번 레터에서는 바이브코딩 컨설팅업체를 운영하며 느낀 바이브코딩의 현 위치와,\\xa0**제대로 사용하기위한 준비사항** 들을 소개해보겠습니다.## **바이브코딩이란?**바이브코딩은 쉽게 말해\\xa0AI가 대신 코딩하는 방법론입니다.어려운 개발언어 공부할 필요 없이 \"계산기 만들어줘\"처럼 요청하면 되는거죠.아티클을 더 읽고 싶다면?이오플래닛에 가입해주세요.로그인 후 모든 아티클을 무료로 볼 수 있어요.]( 7]( 뉴스레터]( 어썸데브 · CEO 댓글 5]( studio약 2달 전]( 뉴스레터 님의 아티클이 EO 뉴스레터에 실렸습니다. 이번 주 이오레터를 확인하세요!>>> ]( 0]( l 가능성에 반응하는 사람들약 2달 전]( 쉽게 이해할 수 있어 잘 읽었습니다. 한 두개 프로잭트에 활용해 봐야 겠어요. 고맙습미다.]( 0](\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0개발자약 2달 전]( 실력에서 나온 통찰력 있는 내용이라 배우고 갑니다.다음글도 기대가 되네요~]( 0]( 코리아\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0Product Manager약 2달 전]( 맥을 주사 바늘처럼 정교하게 찔러주셔서많은 영감을 얻고 갑니다. 향후 글 기대 됩니다.]( 1]( 뉴스레터어썸데브\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0CEO약 2달 전]( 정말 감사합니다! 앞으로도 좋은 컨텐츠 남기겠습니다. :)]( 0추천 아티클최바울  페오펫]( 약 3년 전]( 약 3년 전디오니소스  디오니소스\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0Product Owner]( 1년 이상 전]( Product Owner  1년 이상 전[# 사업전략# 마인드셋# 트렌드# 기타‘챗GPT의 아버지’ 샘 알트만에 대해 알아보자 [아티클 한 눈에 보기]0.샘 알트만이라는 독특한 캐릭터1.\"이해 받지 못하는 것이 강점이다\"2.CEO는 어떻게 시간을 써야 하는가3.실리콘밸리 요다의 3가지 깨달음4.핵융합, 기본소득, 규제, 정치적 올바름\\xa0\\xa02022년 여름부터 연일 헤드라인을 장식하는 사람이 ...](  eo\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0에디터]( 2년 이상 전]( 에디터  2년 이상 전]( 뉴스레터]( 어썸데브 · CEO ]( 7![]('}]\n",
      "[{'title': 'Real time emotion aware applications: A case study employing ...', 'url': 'https://www.sciencedirect.com/science/article/abs/pii/S0169260712000831', 'markdown': ' - - - - Other access optionsSearch ScienceDirect## Article preview- - - - - ]( \"Go to Computer Methods and Programs in Biomedicine on ScienceDirect\")## , July 2012, Pages 16-27]( Real time emotion aware applications: A case study employing emotion evocative pictures and neuro-physiological sensing enhanced by Graphic Processor UnitsAuthor links open overlay panel , , , Show moreAdd to MendeleyCite ## AbstractIn this paper the feasibility of adopting  Units towards real-time emotion aware computing is investigated for boosting the time  employed in such applications. The proposed methodology was employed in analysis of encephalographic and electrodermal data gathered when participants passively viewed emotional evocative stimuli. The  effectiveness when processing electroencephalographic and electrodermal recordings is demonstrated by comparing the  of chaos/complexity analysis through  (multi-channel correlation dimension/D2) and  (computation of  level/SCL) into various popular programming environments. Apart from the beneficial role of parallel programming, the adoption of special design techniques regarding  may further enhance the time minimization which approximates a factor of 30 in comparison with ANSI C language (single-core sequential execution). Therefore, the use of  parallel capabilities offers a reliable and  for real-time sensing the user\\'s affective state.## IntroductionEmotion aware computing was for a large period a neglected topic in the scientific community [1]. However, recent neuroscience findings have highlighted the critical role of emotions in a variety of cognitive functions like decision making [2], memory [3] and perception [4]. These arguments demonstrated the significance of emotional intelligence [5] not only when interacting with other people but also between human and machines [6]. Therefore, motivated research efforts investigate how to provide computers with abilities to recognize the user\\'s emotional state and to naturally adapt to it [7]. The importance of emotion aware computing is desirable only in cases where the user should interact with the machine in order to achieve high performance during the task procedure that should be accomplished [8]. So, providing the machine with the capability to robustly sense the users’ negative feelings [9] (frustration, anger, stress, anxiety, disappointment, etc.) the appropriate feedback may be given to neutralize their mood [10] and to encourage them to improve their performance in several applications like tests controlled through computer [11], virtual gaming [12] or remote monitoring of elderly or disabled people [13], [14]. Initial research attempts have demonstrated that the core element of a successful affective computing system is its ability to emulate the ways that are employed in the communication between human beings [15]. The pioneering work of MIT group led to the introduction of the “Affective Computing” term and to the establishment of a framework that could be adopted for a successful human–computer interaction (HCI) system [16], while also dealing with the challenges that have to be faced and the expectations created by potential applications [8].Previous research attempts have adopted communicative ways like facial expressions [17] and posture recognition [18]. However, several limitations occur since these modalities are highly dependent from the users’ personality [19] and their culture, resulting thus in enhanced inter-subject variability. Robust emotion recognition assumes the utilization of exaggerated expressions that are unlikely to be elicited in real-life situations [20]. Moreover, the use of cameras produces huge amount of data, while also communicates irrelevant information (e.g. subject\\'s identity) which the user may be unwilling to reveal [7]. Since the aforementioned methodologies are based on the recognition of externally expressed emotions, some innermost may not be easily recognized [8]. Such feelings are not easily communicated even among human beings and may be better recognized by neuro-physiological sensing [7]. Data fusion [21] from both the central and the autonomic nervous system may create discrete emotional patterns for a wide range of emotions [22], which are poorly distinguishable otherwise. However, special care should be given to the experimental methodology used for emotion elicitation.So, a key issue towards the achievement of a robust emotion aware computerized system is the establishment of a framework that is in close connection with the modern emotional theory assuring thus the reliable emotion elicitation. Recent trends regard emotions as behavioral attitudes related with evolutionary processes aiming to assure the human\\'s survival and perpetuation [23], [24], [25]. Therefore, each situation may be judged as either a pleasant or an unpleasant one. Its importance modulates the activation level needed in order to confront the stimulus appeared. Erotic or life-threatening situations require higher activation degree than melancholic or relaxing occasions. Adopting this notion, a bi-directional model was proposed. According to this approach, emotional processing is governed by two motivational systems which are the appetitive approach dealing with the pleasant situations and the defensive one activated in case of life-threatening occasions. The activation of the aforementioned systems is described through the valence dimension, while the activation degree is represented by the arousal dimension. So, these affective variables form a 2D emotional space.The International Affective Picture System (IAPS) collection adopts the aforementioned emotional model and provides a variety of affective visual stimuli as well as their normative ratings for both the arousal and valence dimension [26]. The use of this picture collection with simultaneous neurophysiological recordings demonstrated the facilitated encoding of emotional stimuli [27]. The combination of central nervous (event-related potentials/ERPs) and autonomic (electrodermal) activity revealed a significant correlation between skin conductance responses (SCRs) and the arousal ratings of the IAPS stimuli [23]. Moreover, late ERPs were more positive for emotional pictures [28], while their time course was influenced by the valence dimension [29]. A recent study investigated whether emotional processing is affected by the subject\\'s gender. Early (N100) and mid (N200) ERPs were significantly greater for female participants during passive viewing of unpleasant pictures [30].The bi-directional emotion model and the aforementioned neuroscience findings have not been widely adopted until now in the field of emotion aware computing. Relying on these notions, a Mahalanobis distance-based classification scheme was proposed for discriminating emotional instances selected from the IAPS collection. The output of the recognition sub-system was then used by an avatar which emulated the user\\'s affective state by adapting its face and voice characteristics [14]. However, there was need for further improvement of the classification accuracy by applying data mining (decision trees) and pattern recognition (Support Vector Machines) techniques [31]. Towards the achievement of a reliable emotion-aware application, extended feature fusion from different neuro-physiological modalities was proposed as well as a close connection with the theoretical emotional framework and the independency of the two emotional variables. Moreover, gender specific classifiers were proposed according to [32] in order to further enhance the method\\'s robustness which reached 81.3% for 4 emotional categories.Despite the adequate classification accuracy that was demonstrated by these research efforts, there are several open issues that should be further investigated prior to the introduction of real world emotion aware applications. The proposed discrimination framework was developed for research purposes. It is oriented towards the achievement of the optimal result employing time-consuming computations that reduce its applicability. Moreover, it has been developed as an isolated application under controlled lab environments which may differ from generic real-life applications. So, an integrative approach should be adopted for linking the emotion methodology with the acquisition subsystem as well as with the avatar behavior-generation routines. Then, the proposed system would be able to gather short segments of neuro-physiological data which are processed within fractions of seconds. The user\\'s affective state is recognized and serves as an input to the avatar which adapts its behavior either to mirror or to neutralize the user\\'s affective state.The current study investigates the feasibility of the Graphics Processing Unit (GPU) for the fast processing of neuro-physiological data. Short segments from both the central (ERPs) and the autonomic (SCRs) nervous system serve as an input to the system. These data are parallel processed during the feature extraction stage by algorithmic procedures that were re-designed in order to provide the optimal solution regarding the memory management. So, the aim of this paper is to demonstrate that the adoption of parallel processing may be greatly beneficial for the development of real-time emotion aware applications. Therefore, it is not focused to the extensive description of the parallelization techniques adopted. Moreover, it highlights some significant issues like time consumption on data transfer between host and device that should be taken into consideration during the system design in order to further minimize the execution time. So, the work\\'s contribution lays on the introduction of a framework for the adoption of parallel programming for real time emotion-aware applications.So, the remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture as well as with special programming techniques adopted for the proper parallelization of an algorithm. Then, a brief description of the parallelized algorithms is performed. Within Section 3 results of the algorithms’ implementation and the execution time are presented in Section 4. Finally, the discussion of this paper appears in Section 4.## Section snippets## The NVIDIA GPU architecture – CUDAThe voracious market demand for realtime and high definition 3D graphics led to the introduction of highly parallel, multithreaded, manycore processor Graphic Processor Unit (GPU). Characterized by high memory bandwidth and astounding computational horsepower, the GPU (Fig. 1) serves the demanding requirements of the modern designs and implementations. Its main difference with CPU is that it facilitates compute-intensive and parallel computation. Stemming from the graphics rendering demands, it## ResultsThe features (D2 complexity and SCL values) obtained from the parallel processing of electroencephalographic and autonomic data were analyzed in order to highlight differences among the various emotional states. Each emotional state is characterized by two independent variables (valence and arousal degree).Regarding the multi-channel D2 correlation dimension algorithm, the analysis was performed for each participant and for each one of the four emotional categories. As depicted in Fig. 9 (left## DiscussionThe current work aims to highlight the significant acceleration that may be achieved to emotion aware computing in case of adopting parallel programming on GPU. So, the detailed description of the parallelization techniques are beyond the paper\\'s scope and may be found in [33], [36]. These recent code execution techniques are exploited in boosting complex and time-consuming computations, such as nonlinear dynamic analysis or processing of dense data arrays. Selected results are included in## ConclusionA novel parallel-programming approach based on the CUDA architecture was proposed in order to accelerate the processing of neurophysiological recordings requiring complex computations. It aims to facilitate the already proposed emotion discrimination methodologies with the computing solution needed in order to perform real-time classification. To this end, the importance of this work towards an integrative approach of providing the machines with the capabilities to adapt their behavior## Conflict of interestThe authors do not report any conflict of interest.Special issue articlesRecommended articles- K. Sergerie _et al._### ### Neuroscience and Biobehavioral Reviews(2008)P. Vuilleumier _et al._### ### Nature Neuroscience(2003)- R.W. Picard _et al._### ### IEEE Transactions on Pattern Analysis and Machine Intelligence(2001)- J. Klein _et al._### ### Interacting with Computers(2002)- F. Nasoz _et al._### - C.A. Frantzidis _et al._### - P.J. Lang _et al._### (1997)- C. Amrhein _et al._### ### International Journal of Psychophysiology(2004)- C. Lithari _et al._### ### Brain Topography(2010)- G. Noaje _et al._- K.C. Berridge _et al._### Affective neuroscience of pleasure: reward in humans and animals### Psychopharmacology(2008)A. Bechara _et al._### Emotion, decision making and the orbitofrontal cortex### Cerebral Cortex(2000)M. Pantic _et al._### Toward an affect-sensitive multimodal human–computer interaction### Proceedings of the IEEE(2003)E. Hudlicka### To feel or not to feel: the role of affect in human–computer interaction### International Journal of Human–Computer Studies(2003)R.W. Picard### Affective computing: challenges### International Journal of Human–Computer Studies(2003)B. Kort _et al._### An affective module for an intelligent tutoring systemE. Hudlicka### Affective computing for game designC.A. Frantzidis _et al._### Description and future trends of ICT solutions offered towards independent living: the case of LLM projectP.D. Bamidis _et al._### An integrated approach to emotion recognition for advanced emotional intelligenceB. Reeves _et al._### The Media Equation(1996)R.W. Picard### Affective Computing(1997)View more references- ### 2019, Information FusionCitation Excerpt :Mehmood et al. utilized the late positive potential (LPP)-based feature extraction method and used two classifiers (SVM an KNN) for emotion detection from EEG [44]. Konstantinidis et al. utilized K-nearest neighbors (KNN) classifiers to detect six basic emotions from a three-channel forehead EEG combined with GSR [45]. Recently, Alarcao et al. [10] presented an analysis which shows that 59% of works in the literature used SVM classifiers to detect emotions, 8% of works used different variations of SVM like adaptive SVM etc.; 14% of the works used the kNN, 6.3% of the works used Linear Discriminant Analysis (LDA) and 3.17% of the works used the Naive Bayes (NB) classifier.Show abstractRecently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.- ### 2017, Computer Methods and Programs in BiomedicineShow abstractUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.- ### 2017, Frontiers in NeuroroboticsShow abstractUsing machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features. However, the conventional subject-specific classifiers may induce additional burdens to each subject for preparing multiple-session EEG data as training sets. To this end, we developed a new EEG feature selection approach, transfer recursive feature elimination (T-RFE), to determine a set of the most robust EEG indicators with stable geometrical distribution across a group of training subjects and a specific testing subject. A validating set is introduced to independently determine the optimal hyper-parameter and the feature ranking of the T-RFE model aiming at controlling the overfitting. The effectiveness of the T-RFE algorithm for such cross-subject emotion classification paradigm has been validated by DEAP database. With a linear least square support vector machine classifier implemented, the performance of the T-RFE is compared against several conventional feature selection schemes and the statistical significant improvement has been found. The classification rate and _F_-score achieve 0.7867, 0.7526, 0.7875, and 0.8077 for arousal and valence dimensions, respectively, and outperform several recent reported works on the same database. In the end, the T-RFE based classifier is compared against two subject-generic classifiers in the literature. The investigation of the computational time for all classifiers indicates the accuracy improvement of the T-RFE is at the cost of the longer training time.- ### 2014, Plos OneShow abstractInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method\\'s practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.- ### 2014, IEEE Transactions on Affective Computing- ### 2013, Computational and Mathematical Methods in MedicineCopyright © 2012 Elsevier Ireland Ltd. All rights reserved.The Identity Selector: Persistence Service'}, {'title': 'Towards Emotion-awareness in Programming Education with ...', 'url': 'https://ieeexplore.ieee.org/document/9842520/', 'markdown': \"Loading [MathJax]/extensions/TeX/ieee_stixext.jsTowards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation | IEEE Conference Publication | IEEE XploreSkip to Main Content# Towards Emotion-awareness in Programming Education with Behavior-based Emotion EstimationPublisher: IEEECite ThisPDF; ; ; ; ; All AuthorsSign In or Purchase1Cites inPaper239FullText Views- AlertsAdd to Citation Alerts* * *- Download PDF- Download References- Request Permissions- Save to- Alerts## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learnin...Show More## Metadata## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learning to program, rich emotions are experienced which can provide valuable feedback to their teachers. To accurately model stu-dents' emotions, this paper adopts the well-recognized model of emotions during complex learning that involves four states: engaged, confused, frustrated, and bored. To perform continuous estimation of students' emotions in a non-intrusive manner, this paper proposes to track students' programming behavior and estimate their corresponding emotional states. Compare to the existing approaches on acquiring the students' emotional states with self-reports or bio-sensors, the proposed approach is more feasible in conducting real-world, and large-scale studies for not requiring extensive human interventions or additional devices. Evaluated using data collected from a real-world course project, the proposed approach is showed to be promising for achieving an estimation accuracy of 72.06 % for the above four emotional states. As an enabling technology, the proposed is potentially useful in supporting many applications and improve the quality of programming education in computer science.**Published in:** **Date of Conference:** 27 June 2022 - 01 July 2022**Date Added to IEEE _Xplore_:** 10 August 2022**ISBN Information:****Print on Demand(PoD) ISSN:** 0730-3157**DOI:** Publisher: IEEE**Conference Location:** Los Alamitos, CA, USA## Funding Agency:![]( Contents* * *### I. IntroductionBoth learning and computer programming are high cog-nitively demanding tasks which involve the experience of complex emotions during the process [1]–[3]. Studies from the areas of education [3], psychology [2], and software engineering [1], [4], [5] have more than once shown the significance of one's emotions in affecting the performance and productivity of the learners and developers, respectively. Bringing the above researches together, this paper studies the problem of understanding the students' emotions during the task of learning to program.Sign in to Continue Reading## Authors## Figures## References## Citations## Keywords## Metrics## Footnotes]( Like This2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)Published: 20162024 International Conference on Inventive Computation Technologies (ICICT)Published: 2024Show More**References is not available for this document.**### IEEE Account- - ### Purchase Details- - - ### Profile Information- - - ### Need Help?- **US & Canada:** +1 800 678 4333- **Worldwide:** +1 732 981 0060- - - - - - - - - A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.The Identity Selector: Persistence Service\"}]\n",
      "[{'title': 'AI | 2025 Stack Overflow Developer Survey', 'url': 'https://survey.stackoverflow.co/2025/ai', 'markdown': '### 84% of respondents are using or planning to use AI tools in their development process, an increase over last year (76%). This year we can see 51% of professional developers use AI tools daily.Do you currently use AI tools in your development process?     All RespondentsYes, I use AI tools daily47.1%Yes, I use AI tools weekly17.7%Yes, I use AI tools monthly or infrequently13.7%No, but I plan to soon5.3%No, and I don\\'t plan to16.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,662(68.7%)Professional DevelopersYes, I use AI tools daily50.6%Yes, I use AI tools weekly17.4%Yes, I use AI tools monthly or infrequently12.8%No, but I plan to soon4.6%No, and I don\\'t plan to14.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,004(53%)Learning to CodeYes, I use AI tools daily39.5%Yes, I use AI tools weekly18.7%Yes, I use AI tools monthly or infrequently15.1%No, but I plan to soon7.2%No, and I don\\'t plan to19.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,843(5.8%)Early Career DevsYes, I use AI tools daily55.5%Yes, I use AI tools weekly18.1%Yes, I use AI tools monthly or infrequently11.5%No, but I plan to soon2.5%No, and I don\\'t plan to12.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,360(13%)Early career defined as 1 - 5 years work experienceMid Career DevsYes, I use AI tools daily52.8%Yes, I use AI tools weekly16.8%Yes, I use AI tools monthly or infrequently13.5%No, but I plan to soon3.7%No, and I don\\'t plan to13.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,997(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsYes, I use AI tools daily47.3%Yes, I use AI tools weekly17.2%Yes, I use AI tools monthly or infrequently13%No, but I plan to soon6%No, and I don\\'t plan to16.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 13,001(26.5%)Experienced dev defined as 10+ years work experience### Conversely to usage, positive sentiment for AI tools has decreased in 2025: 70%+ in 2023 and 2024 to just 60% this year. Professionals show a higher overall favorable sentiment (61%) than those learning to code (53%).How favorable is your stance on using AI tools as part of your development workflow?     All RespondentsVery favorable22.9%Favorable36.8%Indifferent17.6%Unsure2.3%Unfavorable10.8%Very unfavorable9.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,412(68.2%)Professional DevelopersVery favorable23.5%Favorable37.7%Indifferent17.4%Unsure1.8%Unfavorable10.6%Very unfavorable9.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,814(52.7%)Learning to CodeVery favorable19.3%Favorable33.5%Indifferent16.6%Unsure4.3%Unfavorable13.6%Very unfavorable12.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,812(5.7%)Early Career DevsVery favorable22.8%Favorable40.3%Indifferent17%Unsure1.3%Unfavorable10.3%Very unfavorable8.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,293(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery favorable23.8%Favorable38.9%Indifferent16.2%Unsure1.5%Unfavorable11%Very unfavorable8.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,957(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery favorable23.9%Favorable36%Indifferent18.1%Unsure2.1%Unfavorable10.3%Very unfavorable9.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,941(26.4%)Experienced devs defined as 10+ years work experience### More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report \"highly trusting\" the output. Experienced developers are the most cautious, with the lowest \"highly trust\" rate (2.6%) and the highest \"highly distrust\" rate (20%), indicating a widespread need for human verification for those in roles with accountability.How much do you trust the accuracy of the output from AI tools as part of your development workflow?     All RespondentsHighly trust3.1%Somewhat trust29.6%Somewhat distrust26.1%Highly distrust19.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,244(67.8%)Professional DevelopersHighly trust2.7%Somewhat trust29.6%Somewhat distrust26.3%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,701(52.4%)Learning to CodeHighly trust6.1%Somewhat trust31.3%Somewhat distrust24.2%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,781(5.7%)Early Career DevsHighly trust3%Somewhat trust31.1%Somewhat distrust25.7%Highly distrust17.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,254(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsHighly trust2.8%Somewhat trust30.3%Somewhat distrust26.1%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,931(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsHighly trust2.5%Somewhat trust28.6%Somewhat distrust26.7%Highly distrust20.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,908(26.3%)Experienced devs defined as 10+ years work experience### In 2024, 35% of professional developers already believed that AI tools struggled with complex tasks. This year, that number has dropped to 29% among professional developers and is consistent amongst experience levels. Complex tasks carry too much risk to spend extra time proving out the efficacy of AI tools.How well do the AI tools you use in your development workflow handle complex tasks?     All RespondentsVery well at handling complex tasks4.4%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.1%Bad at handling complex tasks22%Very poor at handling complex tasks17.6%I don\\'t use AI tools for complex tasks / I don\\'t know16.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,230(67.8%)Professional DevelopersVery well at handling complex tasks3.9%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.2%Bad at handling complex tasks22.8%Very poor at handling complex tasks18.6%I don\\'t use AI tools for complex tasks / I don\\'t know15.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,695(52.4%)Learning to CodeVery well at handling complex tasks7.9%Good, but not great at handling complex tasks25.8%Neither good or bad at handling complex tasks12.4%Bad at handling complex tasks19%Very poor at handling complex tasks16.3%I don\\'t use AI tools for complex tasks / I don\\'t know18.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,779(5.7%)Early Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks28.1%Neither good or bad at handling complex tasks13.4%Bad at handling complex tasks23.6%Very poor at handling complex tasks19.2%I don\\'t use AI tools for complex tasks / I don\\'t know11.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,258(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks25.4%Neither good or bad at handling complex tasks13.8%Bad at handling complex tasks23.9%Very poor at handling complex tasks19.5%I don\\'t use AI tools for complex tasks / I don\\'t know13.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,922(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery well at handling complex tasks3.6%Good, but not great at handling complex tasks23.5%Neither good or bad at handling complex tasks14.9%Bad at handling complex tasks22.1%Very poor at handling complex tasks17.9%I don\\'t use AI tools for complex tasks / I don\\'t know18%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,901(26.3%)Experienced dev career defined as 10+ years work experience### Developers show the most resistance to using AI for high-responsibility, systemic tasks like Deployment and monitoring (76% don\\'t plan to) and Project planning (69% don\\'t plan to).Which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently Mostly AISearch for answers54.1%Generating content or synthetic data35.8%Learning new concepts or technologies33.1%Documenting code30.8%Creating or maintaining documentation24.8%Learning about a codebase20.8%Debugging or fixing code20.7%Testing code17.9%Writing code16.9%Predictive analytics11%Project planning10.8%Committing and reviewing code10.2%Deployment and monitoring6.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,202(22.9%)Currently Partially AISearch for answers55.8%Generating content or synthetic data28.6%Learning new concepts or technologies47.4%Documenting code30.3%Creating or maintaining documentation27.3%Learning about a codebase32.7%Debugging or fixing code47.1%Testing code27.5%Writing code59%Predictive analytics12.7%Project planning17.1%Committing and reviewing code22.6%Deployment and monitoring10.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,991(42.8%)Plan to Partially Use AISearch for answers24%Generating content or synthetic data28%Learning new concepts or technologies27.9%Documenting code30.5%Creating or maintaining documentation32.5%Learning about a codebase34.9%Debugging or fixing code30.9%Testing code34.7%Writing code32.4%Predictive analytics25%Project planning24.8%Committing and reviewing code31.4%Deployment and monitoring25%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,518(45.9%)Plan to Mostly Use AISearch for answers17.2%Generating content or synthetic data28.9%Learning new concepts or technologies15.7%Documenting code28.6%Creating or maintaining documentation31.8%Learning about a codebase23.1%Debugging or fixing code14.8%Testing code25.8%Writing code12.4%Predictive analytics23%Project planning14.3%Committing and reviewing code16.3%Deployment and monitoring15.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,790(26.1%)Don\\'t Plan to Use AI for This TaskSearch for answers19.6%Generating content or synthetic data38.2%Learning new concepts or technologies32.3%Documenting code38.5%Creating or maintaining documentation39.6%Learning about a codebase39.4%Debugging or fixing code36.4%Testing code44.1%Writing code28.9%Predictive analytics65.6%Project planning69.2%Committing and reviewing code58.7%Deployment and monitoring75.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,349(51.7%)### Respondents who said they are currently using mostly AI tools to complete tasks in the development workflow are highly satisfied with and frequently using AI to search for answers or learn new concepts; respondents plan to mostly use AI in the future for documentation and testing tasks and are slightly less satisfied with the tools they are using now.How favorable is your stance on using AI tools as part of your development workflow and which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently mostly AINumber of responses6,053685Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents5.255.35.355.45.455.55.555.65.65%5%10%15%20%25%30%35%40%45%50%55Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,184(22.8%)Currently partially AINumber of responses12,3822,194Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.74.754.84.854.94.9555.055.15.155.25.25%10%15%20%25%30%35%40%45%50%55%60Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,980(42.8%)Plan to partially use AINumber of responses7,8585,400Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents3.73.83.944.14.24.34.44.54.64.74.84.9%24%25%26%27%28%29%30%31%32%33%34%35Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,500(45.9%)Plan to mostly use AINumber of responses4,0561,588Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.64.654.74.754.84.854.94.9555.055.15.155.2%12%14%16%18%20%22%24%26%28%30%32Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,777(26.1%)Don\\'t plan to use AI for this taskNumber of responses19,2114,953Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents2.42.62.833.23.43.63.84%20%25%30%35%40%45%50%55%60%65%70%75%80Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,332(51.7%)### The biggest single frustration, cited by 66% of developers, is dealing with \"AI solutions that are almost right, but not quite,\" which often leads to the second-biggest frustration: \"Debugging AI-generated code is more time-consuming\" (45%)When using AI tools, which of the following problems or frustrations have you encountered? Select all that apply.All RespondentsAI solutions that are almost right, but not quite66%Debugging AI-generated code is more time-consuming45.2%I don’t use AI tools regularly23.5%I’ve become less confident in my own problem-solving20%It’s hard to understand how or why the code works16.3%Other (write in):11.6%I haven’t encountered any problems4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,476(64.2%)### In a future with advanced AI, the #1 reason developers would still ask a person for help is \"When I don’t trust AI’s answers\" (75%). This positions human developers as the ultimate arbiters of quality and correctness.In the future, if AI can do most coding tasks, in which situations would you still want to ask another person for help? Select all that apply.All RespondentsWhen I don’t trust AI’s answers75.3%When I have ethical or security concerns about code61.7%When I want to fully understand something61.3%When I want to learn best practices58.1%When I’m stuck and can’t explain the problem54.6%When I need help fixing complex or unfamiliar code49.8%When I want to compare different solutions44.1%When I need quick help troubleshooting27.5%Other6.1%I don’t think I’ll need help from people anymore4.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 29,163(59.5%)### Most respondents are not vibe coding (72%), and an additional 5% are emphatic it not being part of their development workflow.In your own words, is \"vibe coding\" part of your professional development work? For this question, we define vibe coding according to the , the process of generating software from LLM prompts.     All RespondentsYes, emphatically0.4%Yes11.9%Yes, somewhat2.8%I have tried it2.1%Not sure1.2%No72.2%No, emphatically5.3%Uncategorized4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,564(54.2%)18-24 years oldYes, emphatically0.3%Yes11.6%Yes, somewhat3.2%I have tried it2.4%Not sure1.2%No72.8%No, emphatically5.1%Uncategorized3.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 4,212(8.6%)25-34 years oldYes, emphatically0.4%Yes11.8%Yes, somewhat3.2%I have tried it1.6%Not sure1.3%No72.3%No, emphatically5.7%Uncategorized3.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,526(17.4%)35-44 years oldYes, emphatically0.5%Yes12%Yes, somewhat2.8%I have tried it2.2%Not sure1.1%No72%No, emphatically5.4%Uncategorized4.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 7,607(15.5%)45-54 years oldYes, emphatically0.5%Yes12.7%Yes, somewhat2.5%I have tried it1.9%Not sure1.3%No71.3%No, emphatically5.2%Uncategorized4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,838(7.8%)55-64 years oldYes, emphatically0.8%Yes11.4%Yes, somewhat2%I have tried it3.1%Not sure1.5%No71.3%No, emphatically4.6%Uncategorized5.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 1,657(3.4%)### AI agents are not yet mainstream. A majority of developers (52%) either don\\'t use agents or stick to simpler AI tools, and a significant portion (38%) have no plans to adopt them.Are you using AI agents in your work (development or otherwise)? AI agents refer to autonomous software entities that can operate with minimal to no direct human intervention using artificial intelligence techniques.    All RespondentsYes, I use AI agents at work daily14.1%Yes, I use AI agents at work weekly9%Yes, I use AI agents at work monthly or infrequently7.8%No, but I plan to17.4%No, I use AI exclusively in copilot/autocomplete mode13.8%No, and I don\\'t plan to37.9%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,877(65%)Professional DevelopersYes, I use AI agents at work daily14.9%Yes, I use AI agents at work weekly9.2%Yes, I use AI agents at work monthly or infrequently7.7%No, but I plan to17.2%No, I use AI exclusively in copilot/autocomplete mode14.2%No, and I don\\'t plan to36.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 24,752(50.5%)Learning to CodeYes, I use AI agents at work daily13.2%Yes, I use AI agents at work weekly7.8%Yes, I use AI agents at work monthly or infrequently7.4%No, but I plan to15.6%No, I use AI exclusively in copilot/autocomplete mode12.1%No, and I don\\'t plan to44.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,610(5.3%)Professional AI UsersYes, I use AI agents at work daily17.5%Yes, I use AI agents at work weekly10.8%Yes, I use AI agents at work monthly or infrequently8.9%No, but I plan to18.6%No, I use AI exclusively in copilot/autocomplete mode16.3%No, and I don\\'t plan to27.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,892(42.6%)Learning AI UsersYes, I use AI agents at work daily16.5%Yes, I use AI agents at work weekly9.6%Yes, I use AI agents at work monthly or infrequently8.7%No, but I plan to16.9%No, I use AI exclusively in copilot/autocomplete mode14.7%No, and I don\\'t plan to33.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,019(4.1%)### 52% of developers agree that AI tools and/or AI agents have had a positive effect on their productivity.Have AI tools or AI agents changed how you complete development work in the past year?All RespondentsYes, to a great extent16.3%Yes, somewhat35.3%Not at all or minimally41.4%No, but my development work has significantly changed due to non-AI factors2.6%No, but my development work has changed somewhat due to non-AI factors4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,636(64.5%)### If you happen to be using AI agents at work and you are a software developer, chances are high that you are using agents for software development (84%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.Industry PurposeSoftware engineering83.5%Data and analytics24.9%IT operations18%Business process automation17.6%Decision intelligence11.3%Customer service support11.2%Marketing8.6%Cybersecurity7.4%Robotics3.9%Other2.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,301(25.1%)### TL;DR: Agents used outside of work are mostly used for language processing tasks (49%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.General PurposeLanguage processing49%Integration with external agents and APIs38.3%MCP servers34.4%Agent/multi-agent orchestration28.1%Vector databases for AI applications24.1%Multi-platform search enablement19.4%Personalized agent creation18.3%Other3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,797(11.8%)### The most recognized impacts are personal efficiency gains, and not team-wide impact. Approximately 70% of agent users agree that agents have reduced the time spent on specific development tasks, and 69% agree they have increased productivity. Only 17% of users agree that agents have improved collaboration within their team, making it the lowest-rated impact by a wide margin.To what extent do you agree with the following statements regarding the impact of AI agents on your work as a developer?All Respondents27.3%35.9%21.3%8.2%7.3%AI agents have accelerated my learning about new technologies or codebases.29.3%34.9%22.4%7%6.4%AI agents have helped me automate repetitive tasks.17.1%31.9%25.3%14.2%11.5%AI agents have helped me solve complex problems more effectively.6.6%10.7%40.5%20%22.2%AI agents have improved collaboration within my team.12.2%25.3%32.4%17.1%13.1%AI agents have improved the quality of my code.27.7%41%20.4%6%4.9%AI agents have increased my productivity.29.3%40.8%17.8%6.9%5.1%AI agents have reduced the time spent on specific development tasks.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,823(26.2%)### Is it a learning curve, or is the tech not there yet? 87% of all respondents agree they are concerned about the accuracy, and 81% agree they have concerns about the security and privacy of data.To what extent do you agree with the following statements regarding AI agents?All Respondents57.1%29.8%9.7%2.3%1.1%I am concerned about the accuracy of the information provided by AI agents.56.1%25.3%11.7%4.7%2.2%I have concerns about the security and privacy of data when using AI agents.16.5%29.7%37.3%12.6%3.9%Integrating AI agents with my existing tools and workflows can be difficult.15.5%27.9%31.8%17.8%6.9%It takes significant time and effort to learn how to use AI agents effectively.13.8%14.4%30.6%15%26.2%My company\\'s IT and/or InfoSec teams have strict rules that do not allow me to use AI agent tools or platforms25.4%27.9%31.8%10.3%4.6%The cost of using certain AI agent platforms is a barrier.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 28,930(59%)### When it comes to data management for agents, traditional, developer-friendly tools like Redis (43%) are being repurposed for AI, alongside emerging vector-native databases like ChromaDB (20%) and pgvector (18%).You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent memory or data management in the past year?All RespondentsRedis42.9%GitHub MCP Server42.8%supabase20.9%ChromaDB19.7%pgvector17.9%Neo4j12.3%Pinecone11.2%Qdrant8.2%Milvus5.2%Fireproof5%LangMem4.8%Weaviate4.5%LanceDB4.4%mem04%Zep2.8%Letta2.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,398(6.9%)### The agent orchestration space is currently led by open-source tools. Among developers building agents, Ollama (51%) and LangChain (33%) are the most-used frameworks.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent orchestration or agent frameworks in the past year?All RespondentsOllama51.1%LangChain32.9%LangGraph16.2%Vertex AI15.1%Amazon Bedrock Agents14.5%OpenRouter13.4%Llama Index13.3%AutoGen (Microsoft)12%Zapier11.8%CrewAI7.5%Semantic Kernel6%IBM watsonx.ai5.7%Haystack4.4%Smolagents3.7%Agno3.4%phidata2.1%Smol-AGI1.9%Martian1.7%lyzr1.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,758(7.7%)### Developers are primarily adapting their existing, traditional monitoring tools for this new task, rather than adopting new, AI-native solutions. The most used tools for AI agent observability are staples of the DevOps and application monitoring world: Grafana + Prometheus are used by 43% of agent developers, and Sentry is used by 32%.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent observability, monitoring or security in the past year?All RespondentsGrafana + Prometheus43%Sentry31.8%Snyk18.2%New Relic13%LangSmith12.5%Honeycomb8.8%Langfuse8.8%Wiz6.9%Galileo6.2%Adversarial Robustness Toolbox (ART)5.5%Protect AI5%Vectra AI4.4%arize3.7%helicone3.2%Metero2.7%opik2.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,689(5.5%)### ChatGPT (82%) and GitHub Copilot (68%) are the clear market leaders, serving as the primary entry point for most developers using out-of-the-box AI assistance.You indicated you use or develop AI agents as part of your development work. Have you used any of the following out-of-the-box agents, copilots or assistants?All RespondentsChatGPT81.7%GitHub Copilot67.9%Google Gemini47.4%Claude Code40.8%Microsoft Copilot31.3%Perplexity16.2%v0.dev9.1%Bolt.new6.5%Lovable.dev5.7%AgentGPT5%Tabnine5%Replit5%Auto-GPT4.7%Amazon Codewhisperer3.9%Blackbox AI3.5%Roo code (Roo-Cline)3.4%Cody3%Devin AI2.7%Glean (Enterprise Agents)1.3%OpenHands (formerly OpenDevin)1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,323(17%) '}, {'title': 'AI-Generated Code Stats 2025: How Much Is Written by AI?', 'url': 'https://www.elitebrains.com/blog/aI-generated-code-statistics-2025', 'markdown': \"26 min read08/27/2025# AI-Generated Code Stats 2025: How Much Is Written by AI?#### AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development.Hey there, fellow tech enthusiasts! One thing is crystal clear as we look to the future: artificial intelligence (AI) is rapidly changing how we write code. From building entire functions to completing code snippets in milliseconds, AI is helping developers work faster and smarter. But the big question on everyone’s mind is: **How much of that code is AI-generated?** And more importantly, should developers or  worry about their jobs as AI evolves?As the founder of EliteBrains, where we focus on helping developers stay sharp with coding tests and skills development, I want to break down the AI-generated code stats for 2025, explore the tools that are making this happen, and give you a better understanding of what the future holds for software developers in this new world of AI-assisted coding.As AI takes over more of the repetitive coding tasks, the real question becomes how we measure human skill alongside machine-generated output. That’s exactly where **EliteBrains helps.**Put AI to the test and see which of your developers are truly qualified.\\xa0Mix and match AI-powered coding tests from 120+ programming languages, choose the right seniority level, and instantly spot who rises above the rest.Create your\\xa0**FREE AI coding tests today** **( only\\xa03 free slots available)**## **What Is AI-Generated Code?**AI-generated code is programming written or completed by artificial intelligence tools from natural language prompts, existing context or prior code patterns. In 2025, it typically means an AI assistant drafted the solution or produced most of the final lines that a developer then reviews and refines.## **What Percentage of \\xa0Code Is AI-Generated in 2025?**AI has been in the software development game for a while now, but its role in coding is reaching new heights in 2025. It’s not just about suggesting code anymore. AI tools are writing **massive** amounts of code, optimizing development, and helping developers be more efficient than ever before.Here’s the shocking truth: . That number alone shows just how integrated AI tools like **GitHub Copilot** have become in developers’ workflows. Whether it’s generating entire functions or suggesting small snippets of code, AI is now deeply embedded into the way we code.## **How Much Code Has AI Written So Far?**But just how much code is AI generating? Let’s break it down.![]( **How Does AI Code Generation Affect Developer Jobs and the Economy?**With AI generating such a massive amount of code, one big question looms: What does this mean for job security and the economy?![]( **How Many Lines of Code Have Been Generated by AI?**Buckle up, because here’s where it gets mind-blowing. As of 2024, **256 billion lines of code have already been generated by AI**. Think about that for a second—**256 billion lines**. That’s an insane amount of code, and we’re just getting started. From simple scripts to full-fledged software systems, AI is helping to write an overwhelming amount of code across all industries.This number is so massive that it's hard to grasp. Consider this: the **total lines of code** written by humans in the last decade likely pales in comparison to this explosion of AI-generated code. And we’re still in the early stages of integrating AI tools into the development process. The future will see even more code churned out by AI as the tools become smarter, faster, and more efficient.## **How Is Google Using AI to Write Code in 2025?**Let’s look at how AI is playing a huge role in tech giants like Google. Google, known for its cutting-edge AI innovations, has been using AI to power some of its most important software. According to Sundar Pichai, Google’s CEO, **over a quarter of all new code generated by Google is written by AI**. This includes everything from Google Cloud services to Android apps. AI is also playing a significant role in developing machine learning models that power a large chunk of Google’s products.Pichai’s announcement isn’t just a marketing stunt—it’s a reflection of how deeply integrated AI has become in Google’s development process. Whether it's AI-driven infrastructure management or software development for consumer-facing products, Google is leading the way in showing just how powerful AI can be for developers.Let’s dive deeper into why Google has taken this approach. With over 20,000 engineers working on various products, relying on AI to handle repetitive, time-consuming tasks such as code generation allows the company to save countless hours. This, in turn, frees up developers to focus on more complex tasks that require problem-solving skills, creativity, and innovation.## **Will AI Replace Software Developers in 2025?**As AI continues to make its mark on software development, many developers are wondering if their jobs are at risk. Should they be worried? The short answer is **no**but let’s unpack that.AI isn’t here to replace developers. Far from it. AI tools are designed to assist developers, not eliminate them. These tools automate repetitive and mundane tasks, such as writing boilerplate code or generating simple functions. But when it comes to the creative, high-level work of designing software, making complex decisions, and debugging, developers are still very much in the driver’s seat.For example, imagine a scenario where a developer is working on building a web application. AI can generate the basic structure of the code such as setting up routing, creating a simple login form, or connecting to a database but it cannot make decisions about the best UI layout for user experience or develop the logic that handles complex business rules. That’s where the developer’s creativity and problem-solving skills come into play.AI is an assistant, not a replacement. It helps developers write code faster, but it doesn’t have the creativity, problem-solving skills, or domain knowledge that human developers bring to the table. In fact, AI is actually helping developers become more productive, allowing them to focus on more important aspects of the development process.The key takeaway here is that AI is making software development more efficient, but it still needs a human touch for the final product to succeed.### **How Much Code Is AI-Generated on GitHub in 2025?**GitHub is one of the largest code repositories in the world, and it has seen a massive increase in AI-generated code in recent years. That’s not a small chunk—it’s a significant portion of the code that powers everything from web applications to mobile apps.This doesn’t mean that developers are being replaced. Instead, it shows that developers are using AI to streamline their workflows and focus on more complex tasks. **GitHub Copilot** is perhaps the most popular AI tool used for code generation on the platform. It suggests entire lines of code and can even write entire functions based on developer input. It’s designed to help developers write code faster, with fewer bugs, and more efficiently.By integrating Copilot into their workflow, developers save time and avoid getting bogged down by menial tasks, ultimately improving both speed and quality.## **Which Programming Languages and Frameworks Does AI Write Best?**One of the most exciting things about AI in software development is its ability to generate code across a wide range of programming languages and frameworks. Here’s a breakdown of the most popular ones:#### **Python:**Python has become one of the most popular programming languages, especially in fields like data science, machine learning, and web development. AI tools like GitHub Copilot are particularly skilled at generating Python code for these use cases, whether you’re building machine learning models or working with web frameworks like Django and Flask.#### **JavaScript:**JavaScript is the backbone of web development, and AI tools are excellent at generating code for JavaScript frameworks like React, Angular, and Vue.js. AI can generate entire components, hooks, and functions in JavaScript, saving developers a ton of time and effort.#### **Java:**Java remains one of the most widely used programming languages in enterprise software development. AI can help developers generate Java code for popular frameworks like Spring and Hibernate. AI can also generate code for building microservices and APIs, speeding up development time.#### **C and C++:**AI tools are also capable of generating code in low-level programming languages like C and C++, which are crucial for systems programming and embedded systems. AI is particularly helpful in optimizing low-level code for performance and efficiency.#### **Ruby, Swift, PHP, and Go:**AI is also being used to generate code in languages like Ruby, Swift (for iOS development), PHP, and Go. These languages are frequently used for web development, mobile apps, and cloud applications, and AI is helping developers write faster and more efficient code across these platforms.## **What Are the Best AI Code Generation Tools in 2025?**Now, let’s take a look at the tools that are making this all happen. These tools are transforming the way developers work, making coding faster and more efficient:#### **GitHub Copilot:**GitHub Copilot is arguably the most well-known AI tool for developers. Powered by OpenAI’s Codex, Copilot offers suggestions for entire lines or blocks of code as developers work. It supports a wide range of languages and frameworks, including Python, JavaScript, Java, and more. Copilot is designed to help developers write code faster and with fewer errors.Source: #### **Amazon CodeWhisperer:**Amazon’s CodeWhisperer is another powerful AI tool designed to assist developers in generating code for cloud-based applications. It integrates seamlessly with Amazon Web Services (AWS), allowing developers to write code for serverless applications, APIs, and more. CodeWhisperer is especially useful for developers working in the AWS ecosystem.#### **Tabnine:**Tabnine is an AI-powered code completion tool that integrates with popular code editors like VS Code, JetBrains, and more. It offers context-aware code suggestions to help developers write code more quickly and efficiently. Tabnine works with over 40 programming languages, making it a versatile tool for developers.#### **Replit Ghostwriter:**Replit’s Ghostwriter is another AI tool designed for real-time collaborative coding. It provides developers with AI-generated code suggestions and helps speed up the development process. Whether you’re working on a solo project or collaborating with others, Ghostwriter helps you write code faster and more efficiently.**Should Developers Be Worried About AI in Coding?**So, let’s get back to the big question: should developers worry about their jobs in an AI-driven world? The answer is no, but there’s a catch.While AI is indeed transforming the way we write code, it’s not replacing developers. Instead, it’s empowering them to be more productive and efficient. Developers are still needed for creative problem-solving, high-level system design, and debugging. AI tools can generate code, but they can’t design systems, make architectural decisions, or think creatively about solutions. That’s where developers come in.The same idea holds true across industries. AI can build websites or generate campaigns, but real growth still comes from people who understand the field. Veterinary practices, for example, often lean on expert guidance like this , just as tech companies depend on developers for the creativity and insight that AI cannot replace.What’s really changing is the developer’s role. Instead of getting bogged down with repetitive tasks like boilerplate code, they now have the chance to focus on more interesting, high-value work. Those who embrace AI as a tool rather than view it as a threat will find their jobs not just safe but more important than ever.### **How Can Developers Stay Ahead of AI in Coding with EliteBrains?**At , we understand the importance of staying ahead of the curve. As AI continues to transform the software development industry, we’re here to help developers stay competitive. With our **coding tests**, developers can hone their skills, stay updated on new trends, and ensure they’re prepared for the future of software development.Our platform allows developers to:- **Practice with AI-assisted tools** and sharpen their skills- **Test their knowledge** in real-world coding environments- Stay up to date with **emerging trends in AI and software development**- **Learn how to integrate AI** into their development workflow to increase efficiencyAt EliteBrains, we’re committed to helping developers succeed in an AI-powered world, ensuring they remain competitive and continue to thrive.### **Conclusion: AI and Developers—A Winning Partnership**The rise of AI in software development is reshaping the way we write code, but it’s not here to replace developers. Instead, it’s here to make their jobs easier and more efficient. By embracing AI tools like GitHub Copilot, Amazon CodeWhisperer, and Tabnine, developers can work smarter, not harder.AI is an assistant, not a replacement. The key for developers is to adapt and use these tools to their advantage. The future of software development is collaborative—AI working alongside developers to create better, faster, and more innovative software.With **EliteBrains**, developers can stay ahead of the curve, test their skills, and learn how to leverage AI to boost their productivity. The future of coding is bright, and it’s full of exciting opportunities for those who are willing to embrace the change.01. 02. 03. 04. 05. 06. 07. 08. 09. 10. #### Our clients say!With senior resources from EliteBrains, we are able to drive our digital harmonization agenda: Migration of different reporting platforms to QlikSense analytics and Harmonization of different digital detailing systems into Veeva CLM.!Jiri Matousek, Head of CRM at Hexal AG!EliteBrains provided us with a top PHP developer who has been working for us for more than 4 years. Jan became indispensable part of our team within few weeks.!Dave Etchells, CEO at Imaging-resource.com!Each time we are in a need of new software developer, we just contact the guys from EliteBrains. Their network is huge so they provide us with demanded sources very quickly.!Martin Svach, CEO at Starkys club#### Useful articles]( Do Resumes Still Matter for Backend Developers in 2025? Here’s How to Build One That Gets You Hired Faster 02. 09. 2025 . 5 min read]( AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development. 27. 08. 2025 . 11 min readSign up now to see more tipsSubscribecancel anytime#### Hiring?We picked the best developers for you#### Looking for a job?## This website uses cookiesWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Accept allManage preferencesChat Widget\"}]\n",
      "---------- ToolCallRequestEvent (research_agent) ----------\n",
      "[FunctionCall(id='call_URZ5D9aGNTS35obDjthzg375', arguments='{\"query\": \"emotion-aware programming education IEEE 2022\"}', name='web_search_tool'), FunctionCall(id='call_0QfXVfV9DNu7IVPtgrFNajy7', arguments='{\"query\": \"affective computing programming software engineering mood-based development 2023 2024\"}', name='web_search_tool'), FunctionCall(id='call_d74ZAGK7DVpc4VJFOMsgev51', arguments='{\"query\": \"AI-assisted coding impact study 2024 2025 Stack Overflow Developer Survey\"}', name='web_search_tool'), FunctionCall(id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', arguments='{\"query\": \"emotion-based coding tools 2024 2025\"}', name='web_search_tool'), FunctionCall(id='call_S9VoFTZt8NefRPNRbUH4tZQg', arguments='{\"query\": \"cursor windsour codeium vibe coding 2025 article\"}', name='web_search_tool')]\n",
      "---------- ToolCallExecutionEvent (research_agent) ----------\n",
      "[FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 7, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_URZ5D9aGNTS35obDjthzg375', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Affective Software Engineering\\', \\'url\\': \\'https://affectivese.org/\\', \\'markdown\\': \"!We are a group of computer scientists whose research focuses on using Affective Computing methods in the field of Software Engineering. Our interests include, but are not limited to, the role of emotions in the software development process, reusing datasets from Affective Computing experiments as well as improving the teaching process of future software engineers.We are open to any kind of cooperation. Do you want to do research together? Prepare a paper? Submit a project proposal? If you are conducting research at the intersection of the fields of affective computing and software engineering feel free to contact us via . Take a look at our current research and projects and get in touch!### DevEmo datasetThe DevEmo dataset consists of 217 video clips showing students\\' facial expressions as they solve programming tasks. The recordings were collected in the participants\\' actual work environment. The video clips are annotated with five labels, including four emotions (anger, confusion, happiness and surprise) and a neutral state.In order to facilitate the annotation of a large number of video recordings, we have developed AffecTube. It is a comprehensive solution that leverages crowdsourcing to annotate videos directly on the YouTube platform, resulting in ready-to-use emotion-annotated datasets. AffecTube provides a low-resource environment with an intuitive interface and customizable options, making it a versatile tool applicable not only to emotion annotation, but also to various video-based behavioral annotation processes.If you are interested in using the DevEmo dataset or AffecTube check out the projects\\' websites:Journal papers:### Affective data reausabilityModern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration.In the research conducted so far, we have proposed the ROAD (Recording Ontology for Affective-related Datasets) ontology and its implementation with the graph knowledge base in the form of the GRISERA framework.If you are interested in our approach take a look at:Journal papers:### Emotions in the software development processEmotions play an important role in any human being\\'s life, including software developers. The aim of our research is to determine what emotions are optimal in the software development process, how to recognize them and how (if at all) to influence them.Publications:### Affect-aware software engineering educationWith the Covid-19, the role of distance learning has increased. The aim of the research is to develop methods that, using emotion recognition techniques, will allow for more effective remote learning of programming.Publications:### Affective loop in Socially Assistive Robotics as an intervention tool for children with autismEMBOA project entitled “Affective loop in Socially Assistive Robotics as an intervention tool for children with autism” is a research and didactic project that aims at enhancement of social robot intervention in children with autism with affective computing technologies. The project is implemented in years 2019-2022 under EU Erasmus Plus Strategic Partnership for Higher Education Programme.More information on project website: Wrobel MR., Landowska A., Makuch K., , Proceedings of the 32nd International Conference on Information Systems Development (ISD 2024), 2024Barkana DE., Bartl-Pokorny KD., Kose H, Landowska A., Milling M., Robins B., Schuller BW., Uluer P., Wrobel MR., Zorcec T., , International Journal of Social Robotics, 2024Kulas D., Wrobel MR., , SoftwareX, 2023Manikowska M., Sadowski D., Sowinski A., Wrobel MR, , Applied Sciences, 2023Zawadzka T, Wierciński T, Waloszek W, Wróbel MR, , IEEE Access, 2023Zawadzka T, Wierciński T, Meller G, Rock M, Zwierzycki R, Wróbel MR, , Sensors, 2021Zawadzka T, Waloszek W, Karpus A, Zapałowska S, Wróbel MR, , IEEE Access , 2021Bartl-Pokorny KD, Pykala M, Uluer P, Barkana DE, Baird A, Kose H, Zorcec T, Robins B, Schuller BW, Landowska A, , IEEE Access , 2021Wróbel MR, , IEEE Access , 2020Blincoe K, Springer O, Wrobel MR, , IEEE Software, 2019Wrobel MR, Zielke AW, , 2018 Federated Conference on Computer Science and Information Systems (FedCSIS), 2018Wrobel MR, , Applied Sciences, 2018Landowska A, Brodny G, , IEEE Access, 2018Landowska A , , Applied Sciences , 2018Wrobel MR, , 2016 Federated Conference on Computer Science and Information Systems (FedCSIS), 2016Brodny G, Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 2016 9th International Conference on Human System Interactions (HSI), 2016Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Information Systems Development and Applications, 2015Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Human-Computer Systems Interaction: Backgrounds and Applications 3, 2014Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013\"}, {\\'title\\': \\'Affective Computing: Recent Advances, Challenges, and Future ...\\', \\'url\\': \\'https://spj.science.org/doi/10.34133/icomputing.0076\\', \\'markdown\\': \\'AdvertisementMain content starts hereContents## AbstractAffective computing is a rapidly growing multidisciplinary field that encompasses computer science, engineering, psychology, neuroscience, and other related disciplines. Although the literature in this field has progressively grown and matured, the lack of a comprehensive bibliometric analysis limits the overall understanding of the theory, technical methods, and applications of affective computing. This review presents a quantitative analysis of 33,448 articles published in the period from 1997 to 2023, identifying challenges, calling attention to 10 technology trends, and outlining a blueprint for future applications. The findings reveal that the emerging forces represented by China and India are transforming the global research landscape in affective computing, injecting transformative power and fostering extensive collaborations, while emphasizing the need for more consensus regarding standard setting and ethical norms. The 5 core research themes identified via cluster analysis not only represent key areas of international interest but also indicate new research frontiers. Important trends in affective computing include the establishment of large-scale datasets, the use of both data and knowledge to drive innovation, fine-grained sentiment classification, and multimodal fusion, among others. Amid rapid iteration and technology upgrades, affective computing has great application prospects in fields such as brain–computer interfaces, empathic human–computer dialogue, assisted decision-making, and virtual reality.#### SIGN UP FOR _Intelligent Computing_ eTOC ALERTS!Get the latest SPJ table of contents delivered right to you!GET OUR E-ALERTS## IntroductionAccording to basic emotion theory, emotion is the grammar of social living and serves as a crucial means of exchanging information, maintaining relationships, and communicating ideas between individuals. Moreover, it is a fundamental psychological element that ensures basic human survival while shaping social habits and supporting advanced thinking , ]. Given its central role in numerous human intellectual activities such as perception, learning, decision-making, reasoning, and socializing, emotion is an important force driving the continuous and diverse prosperity of human civilization.The importance of emotions to human beings can be summarized in 5 crucial aspects. First, the survival function is a learned physiological response that allows individuals to adapt positively to their environment ]. Emotions play a pivotal role in strengthening the capacity to adapt to the environment by regulating attention, memory, perception, and other cognitive processes. This ensures a greater chance of survival and development during the evolutionary process. Second, the communication function highlights the importance of emotions for the accurate expression and understanding of human intentions ]. The same words spoken with different emotions carry different connotations. Thus, emotions are inseparable from natural language and are critical for semantic disambiguation. Third, emotions have a decision-making function that manifests in both fast and slow modes of thinking. The commonly used unconscious “System 1” mainly relies on emotions and experiences, while the conscious “System 2” depends on rational deliberation ]. Therefore, emotions are widely involved in higher-level thinking and decision-making processes that profoundly affect the results and efficiency of decisions. Fourth, emotions serve a motivational function in stimulating and sustaining individuals’ behaviors, thereby affecting the degree of resource input, behavioral persistence, and evaluation of outcomes ]. Finally, emotions perform a maintenance function as bonds between members of ethnic groups, families, social circles, social classes, and other groups. During human socialization, emotions serve as the core of low-cost maintenance of social relations, forming potential social interaction contracts, and are closely tied to individual moral constraints and codes of conduct , ]. Hence, the nature and functions of emotions ensure that they are inseparable from human survival and development.As the era of a human–machine symbiotic society approaches, endowing machines with emotional intelligence becomes increasingly crucial. Emotional intelligence represents a fundamental technology and an essential prerequisite for realizing naturalized and anthropomorphic human–computer interaction. It is of great value for opening up the era of intelligence and digitization. Picard is credited with being the first to propose a comprehensive definition of affective computing. In her 1997 book, _Affective computing_, she defined it as “computing that relates to, arises from, or deliberately influences emotions” ]. The goal of affective computing is to create a computing system capable of perceiving, recognizing, and understanding human emotions and responding intelligently, sensitively, and naturally, thus making human–computer interaction more natural. The epochal importance of affective computing lies in its impact on changing how emotions are perceived as abstractions within psychology, making it possible for emotions to be measured, computed, and machine-learned.Affective computing encompasses various disciplines, including computer science, engineering science, brain and psychological science, and social sciences. Computer science and engineering science focus on providing various information technology tools and engineering capabilities to enable digital reconstruction and computational realization of emotion perception, recognition, understanding, and feedback, allowing machines to possess human-like emotional and cognitive functions. The psychological and consciousness aspects of the brain and psychological sciences provide theories on the basic definition of human emotions and the structure of related elements, laying the foundation for modeling emotion theories. Cognitive neuroscience, another branch of the brain and psychological sciences, examines the emotion-processing mechanism of the human brain and establishes a functional network of psychological elements associated with emotions, providing important inspiration and strategic guidance for developing affective computing models. Social and medical sciences offer numerous opportunities for the application of affective computing and serve as a resource for designing application scenarios for such technologies.### Research in affective computingThe research content of affective computing primarily covers 5 aspects. The first aspect is the fundamental theory of emotion, which currently relies on the discrete emotion model and the dimensional emotion model from the field of psychology to define various types of emotions, ranging from basic to compound. The second aspect involves collecting emotional signals, such as text, speech, facial expressions, gestures, and physiological signals, to establish corresponding datasets. The third aspect is sentiment analysis, which utilizes machine-learning and deep-learning algorithms to model and identify emotional signals. The fourth aspect is multimodal fusion, which leverages multimodal emotional features and fusion algorithms to enhance the accuracy of emotional classification. Finally, the fifth aspect is generating and expressing emotions, processes that enable robots to express emotional states through facial expressions, voice intonation, body movements, etc., and facilitates natural, anthropomorphic, and personified human–robot interaction. Figure  illustrates the specific content and development status of these 5 aspects.![](\\\\xa01. Research content of affective computing.![](\\\\xa02. Annual scientific production on “affective computing” from 1997 January 1 to 2023 September 25.#### Basic theory of emotionThe field of affective psychology has numerous grounded theories of emotion and serves as an important source of inspiration for the development of computable emotion models. The discrete emotion model and the dimensional emotion model are the most commonly used theoretical models for artificial intelligence emotion modeling. The discrete emotion model categorizes emotions individually rather than in correlated groups, as does Ekman’s basic emotion classification model, which is based on facial expression analysis ] and comprises happiness, sadness, anger, disgust, surprise, fear, and contempt. Although the discrete emotion model is clearly defined, interpretable, easy to understand, and capable of semantically integrating vocabulary and concepts, it lacks granularity and provides a limited quantitative description of emotions. In contrast, dimensional affective models represent different emotions through multidimensional vectors in affective space. Such models include the valence–arousal affective model ] and the 3-dimensional pleasure–arousal–dominance model , ]. These models are highly quantitative, abstract, and inductive and have continuous emotional value vectors. They are suitable for handling changes in emotional states over time but are not intuitively interpretable; thus, it is difficult for machines to use them to develop rich coping strategies for emotional interactions. The selection of the model depends on the actual application tasks and scene requirements, as both discrete and dimensional emotion models have advantages and disadvantages.#### Collection of emotional signalsTo support data acquisition and the comparison of algorithms in affective computing, numerous open-source databases have been established. They contain datasets that can be categorized as textual, speech/audio, visual, physiological, or multimodal. The characteristics of these databases considerably impact model design and network architecture in affective computing.Text-based resources on various communication carriers serve as massive datasets for emotional text mining ]. Representative datasets include the internet movie database (IMDb) ], the Stanford sentiment treebank, which contains sentences from movie reviews ], and the Multi-Domain Sentiment Dataset, which contains Amazon.com product reviews ]. Speech is another crucial modality for decoding emotions in human intercommunication. Speech signals comprise both the emotional content of the speech and the emotional characteristics of the sound itself. Representative datasets include EmoDB ], the SEMAINE database ], and CSED ]. Visual-emotional signals such as body movements and facial expressions are now more convenient to gather because of low-cost sensors such as cameras and camcorders, and they do not require direct contact with the user ]. This field has vast amounts of data and many related research papers with considerable data collected directly from real-world scenarios, making it more conducive to grounded applications ]. Representative datasets include the Expression-in-the-Wild (ExpW) dataset ], AffectNet ], the Real-world Affective Faces Database (RAF-DB) ], and SMIC, a database of spontaneous microexpressions ].Physiological data have an advantage over signal data such as text, speech, and facial expressions in that they can more directly, objectively, and accurately reflect an individual’s emotional state while being less influenced by subjective consciousness , ]. Consequently, physiological data have become a research hotspot in affective computing. Commonly used physiological data in this field include electroencephalograms (EEGs), skin electricity, cardiac electricity, electromyography (EMG), eye electricity, respiration, skin temperature, and blood volume pulse. However, obtaining physiological data requires the use of complex sensors. Thus, such data are expensive and challenging to collect for use in practical applications. Consequently, the scale of physiological data used in laboratory research is generally small ]. Representative datasets include the Database for Emotion Analysis using Physiological Signals (DEAP) ], the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) ], and WESAD, a dataset for wearable stress and affect detection ].#### Sentiment analysis**Text analysis.** This method focuses on extracting, analyzing, understanding, and generating emotional information in natural language. Early text affective recognition relied mainly on manually constructed affective dictionaries and rules for affective analysis. These methods judge sentiment polarity by matching sentiment words with grammatical rules in a text , ]. However, this approach is limited by emotional lexicon coverage and rules, making it challenging to support multidomain sentiment analysis. With the advancement of machine learning, text emotion recognition methods based on statistical and machine learning algorithms have emerged. By training on large-scale text datasets, machine learning models can automatically learn emotional expression and semantic features, enhancing the accuracy and generalization ability of sentiment classification , ]. In recent years, deep-learning technology has considerably impacted text emotion recognition. Neural network-based models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, bidirectional encoder representation from transformers (BERT), and generative pre-trained transformers (GPT), have been successful in various sentiment analysis tasks – ]. They can capture contextual information and semantic relationships to better understand and analyze sentiments.**Speech analysis.** Speech emotion recognition is the process by which a computer automatically recognizes the emotional state signaled by speech. Speech contains emotional information, such as speech rate and intonation, in addition to semantic information. Speech emotion analysis combines linguistic and acoustics-related technologies to analyze the syntax, semantics, and acoustic feature information related to the speaker’s emotional state ]. This analysis mainly revolves around rhyme, spectrum, and sound quality features. The numerous acoustic features related to affective states include fundamental frequency, duration, speech rate, resonance peaks, pitch, mel-filter bank (MFB), log-frequency power coefficients (LFPC), linear predictive cepstral coefficients (LPCC), and mel-frequency cepstral coefficients (MFCC) – ]. These features are represented as fixed dimensional feature vectors, with each component representing the statistical value of each acoustic parameter, including the mean, variance, maximum or minimum value, and range of variation. Recently, the ability of neural networks to extract suitable feature parameters has received increasing attention. Deep speech emotion features are learned from speech signals or spectrograms through tasks related to speech emotion recognition. Deep speech features learned from large-scale training data are widely used as speech emotion features in speech event detection and speech emotion recognition tasks, as in the VGGish and wav2vec projects , ], for example. In recent years, algorithms such as ConvNet learning ], ConvNet-RNN ], and adversarial learning ] have considerably improved speech emotion recognition performance.**Visual analysis.** Visual emotion recognition research primarily focuses on facial expression recognition (FER) and emotional body gesture recognition. The conventional method involves feature extraction followed by classification. Typically, handcrafted features for static image analysis include local binary pattern (LBP), histogram of oriented gradients (HOG), local phase quantization (LPQ), and Gabor features , ]. Some scholars have proposed dynamic feature extraction methods, such as LBP on three orthogonal planes (LBP-TOP) ]. Features are usually classified using pattern recognition classification methods such as K-nearest neighbors, support vector machines (SVMs), or multi-layer perceptrons (MLPs). Another approach is the feature learning approach, which combines the end-to-end training of feature representations and classifiers on a given task target, typically a combination of the entire connection layer and softmax. The feature-learning method employs features learned from big data through layer-by-layer feature transformation and can describe the intrinsic information of data better than handcrafted features. However, supervised training methods such as deep CNNs are not universal and rely on large amounts of sample data. Therefore, it is too early to abandon traditional feature-extraction methods. In visual emotion analysis, automatic training features can be extracted and integrated with traditional features, which may further improve system performance.**Physiological signal analysis.** Physiological changes that occur with emotions, including brain electrical activity, heart rate changes, electrical skin response, muscle tension, and respiration rate, are supported by mainstream theories, such as the physiological theory of emotion ] and Lange’s theory of emotion ]. By detecting changes in these physiological signals, patterns associated with emotions can be recognized and then used to develop computer systems that can automatically recognize emotions. Physiological signals are more challenging to recognize than text, speech, and facial expression signals mentioned above, and they have unique properties. For example, computing EEG data requires more complex preprocessing, including electrode position localization, bandpass filtering, reference conversion, segment analysis interception, artifact removal, and bad electrode interpolation. Researchers must have cross-field knowledge to apply machine learning or deep learning methods to recognize emotions from physiological signals ].Affective computing mainly employs peripheral nervous system (PNS) features, such as facial EMG, galvanic skin potential (GSP), photoplethysmography (PPG), heart rate variability (HRV), respiratory rate, and electrocardiogram (ECG), whereas central nervous system (CNS) features include EEG, near-infrared, and brain-imaging features. EEG features have dominated the studies published on this topic. For instance, manual feature extraction involves multidimensional feature extraction from EEG signals in the time, frequency, time–frequency, and nonlinear domains for emotion recognition and classification. Recent studies have emphasized the integrity and relevance of these features. To construct functional brain networks, many studies have started defining a channel as a node and quantifying the relationship between individual nodes using phase synchronization, inter-correlation, and mutual information, treating strength as the functional connectivity between the brain regions of the corresponding channel. Complex network measures, including efficiency, clustering coefficients, degree distribution, small-world features, and average shortest distance, are then used to extract functional brain network features. Since 2018, deep learning methods such as CNNs, RNNs, deep belief networks (DBNs), and stacked autoencoders (SAEs) – ] are being increasingly used for emotional computation of EEG data, generalizing sentiment analysis to various physiological signals.#### Multimodal fusionEarly affective computing primarily involved unimodal data analysis and emotion recognition, focusing on a single modality, such as text, speech, facial expression, body movement, or physiological signals. However, this approach fails to conform to the human perception and expression patterns of emotions and has limitations in terms of the information obtained for emotion recognition ]. Humans communicate their emotions through multiple channels, including language, tone of voice, facial expressions, and body movements. Textual, auditory, and visual information together provide more comprehensive emotional information than they do individually, just as the brain relies on multiple sensory input sources to validate events. Moreover, unimodal information is insufficient and can be easily affected by various external factors ]. Emotional signals can be disguised or affected by other signals from a single channel, for example, when facial expressions are obscured or when noise interferes with speech, resulting in a considerable reduction in emotion analysis performance. Multimodal emotion analysis considers the complementarity of emotion expression among modalities and is thus more robust and aligned with natural human behavior expression. Therefore, research on multimodal fusion of affective computation has received increasing attention. Multimodal fusion algorithms integrate information from different modalities into a stable multimodal representation, enabling comprehensive processing and coordinated optimization to identify human emotions as accurately as possible ]. Common multimodal fusion methods can be categorized into feature-, model-, and decision-layer-based fusion depending on the fusion stage ].#### Generation and expression of emotionsAffective computing enables machines to provide empathic feedback based on deep contextual understanding. Robots and other agents can deliver expressions and responses, conveying the emotional temperature to the user through facial expressions, emotional text responses, and body movements , ] by building on the results of sentiment analysis and recognition. Emotional text generation and speech synthesis are the most-studied areas of research. Emotional text generation involves the automatic generation of emotional response content that matches the message of the dialogue and is consistent with the machine’s strategy, which is chosen according to the context ]. For instance, a traffic enforcement robot may exhibit a fundamental difference in the language used for persuasion and the language used for enforcement, a difference that is crucial to obtaining effective practical traffic management results. The goal of emotional text generation is for the model to generate text that conforms to a specified sentiment category, as expressed by emotion-related keywords or techniques such as metaphors ]. Pretrained models such as GPTs are increasingly being utilized as a base for emotionally controllable text generation and achieving powerful results ]. Responding to text content with emotional color is only the first step. The generated text needs to be expressed using a related emotional voice. Emotional coding information is integrated into the speech synthesis model to make human–machine dialogue less cold and mechanical, thereby allowing individuals to perceive “machine empathy” and feel warmth and affinity. Emotional speech synthesis uses a specific voice style and combines text content with emotional tags to provide a robot or agent with a voice that expresses a particular emotion ]. This process inputs textual content and a specific voice style into a neural network that synthesizes an output voice in that style by utilizing the spectral, rhythmic, and linguistic features of human voices that express emotion.### Applications of affective computingAffective computing is a technology that advances according to the actual needs of the industry, which drives progress and iteration. To build up reliability, general applications initially focused on recreation, leisure, or serving people with urgent needs, then gradually expanded to more fields, transforming the technology and contributing to productive endeavors. In 2021, the value of affective computing reached $21.6 billion, and it is expected to double by 2024 , ]. As the industry grows, the creative applications of affective computing technologies will flourish, yielding satisfactory results in various fields.#### EducationIn the field of education, affective computing is primarily used to recognize the emotional state of learners and provide corresponding feedback and adjustment ]. For example, teachers can utilize intelligent emotional teaching systems to better understand students’ engagement levels and adjust the pace and content of their teaching to improve the learning experience. An intelligent system can recommend customized learning content based on the sentiment analysis of students’ interests. Students can provide authentic teaching feedback through intelligent systems to improve the comprehensiveness and accuracy of teaching evaluations. One advantage of an intelligent system is that it can be used in both traditional and online classrooms to strengthen the contextualization of online teaching, enhance emotional interaction between teachers and students, and improve teaching quality. Affective computing techniques are also conducive to the research and development of educational games and robots ], providing improved human–computer interaction and achieving educational objectives more effectively.#### HealthcareAffective computing research has expanded into various psychiatric disorders in the affective disorders category, such as Alzheimer’s ], Parkinson’s ], bipolar disorder ], and post-traumatic stress ], and into healthcare areas including relaxation service healthcare ] and health office systems ]. Affective computing enables the scientific and objective identification and judgment of patients’ emotions, particularly in psychological disorder treatments, providing a useful complement to more subjective traditional diagnostic tools such as behavioral observation and scale filling. Objective data collection can improve personalized and precise medical treatment ]. In addition, affective computing can be used for the initial screening and efficacy assessment of diseases. For instance, patients with social anxiety disorder exhibit important differences in emotional facial processing compared to the normal population, differences that can be identified by automated monitoring of differential features ].#### Business servicesIn marketing, where the consumer experience is highly correlated with emotions, affective computing is widely used to understand and recognize the user’s emotional state. The application of affective computing can reveal the user’s true preferences and improve and streamline the buying process ]. In the field of financial credit, affective computing technologies can be used to analyze the emotional state and moral level of a customer based on voice and tone, determine the probability of the customer lying, and provide a guide for lending decisions. In the field of stock investment, investor decisions are influenced by irrational judgments. The price trend of a stock is determined not only by a company’s fundamentals but also to a large extent by fluctuations in investor emotions. The study of investor sentiment from social media data (e.g., data from X, formerly known as Twitter) can help identify investors’ emotional preferences and cognitive biases for the purpose of predicting the direction of the stock market ].#### Integration of science and artIn the current digital era, image, audio, and video data have become plentiful and important. Extracting useful information from them and retrieving and mining them effectively are crucial. For example, in recommending music to users, resource management and audio search efficiency are essential. Traditional music search methods match content using text (e.g., song title, artist name, or lyrics). Including sentiment, a high-level semantic feature of music, improves the match between user preferences and music, thus aiding in the primary task in music sentiment analysis ]. Affective computing also empowers automated poetry generation, where deep learning methods such as RNNPG, an RNN-based poem generator, and SeqGAN, a sequence generative adversarial network, are gradually replacing Word Salada, genetic algorithms, and statistical machine translation methods – ]. Expressing emotions more richly is key in making generated poetry spiritual, i.e., in moving beyond resemblance of form to resemblance in spirit.### Importance of this studyThe field of affective computing has grown considerably and exploded in popularity in the last decade for 2 reasons: technological developments providing tools for affective computing and the growth and expansion of demand. In the era of human–machine symbiosis, the deepened human understanding of emotional connotation and the improvement of the “double quotient” (i.e., IQ + EQ) of intelligent machines will become a vital innovative force promoting the affective computing discipline, technological evolution, and industrial progress. Despite the rapid development in affective computing, a comprehensive review of research and systematic analysis of hotspots and trends is lacking. Continuous innovation in algorithmic technology, broadening application requirements, and increasing research efforts necessitate that existing research be summarized and future technological directions be identified. Doing so will enable academia and industry to better understand the development of affective computing technology, thus will facilitate affective computing research, empower applications, and benefit society.This study aims to fill the gaps in existing research through a comprehensive review of affective computing from 1997, when Picard formally proposed the concept, up to 2023. We adopted a bibliometric analysis method to accurately portray the current status of the development of the field and provide insights into present challenges and future trends. The main contributions of this study are as follows. (a) Facing the academic frontier, we list the research hotspots and trends that we identified by analyzing full-scale papers. This allows readers to quickly and comprehensively grasp the development dynamics of the field and understand key common and frontier-leading technologies. (b) Facing major needs and the main battlefield of the economy, we provide blueprints for technological development and insights into current applications. This promotes the application and transformation of affective computing, facilitating high-quality economic development and digital transformation. (c) Facing future trends, we introduce challenges and developments in the field of affective computing, along with predictions for future technology and industry application directions. This serves as a forward-looking guide to the field.## Materials and Methods### Data collectionThis study searched for papers published in affective computing from January 1997 to September 2023 in the Web of Science Core Collection (WoSCC), which includes the Science Citation Index Expanded, Social Sciences Citation Index, Arts & Humanities Citation Index, Emerging Sources Citation Index, Conference Proceedings Citation Index—Science (CPCI-S), and Conference Proceedings Citation Index—Social Sciences & Humanities (CPCI-SSH). The search strategy is summarized in Table .| Index field | Search strategy || --- | --- || Theme keywords | “affective recognition” or “mood recognition” or “affective computing” or “artificial emotional intelligence” or “emotion AI” or “expression recognition” or “emotion recognition” or “emotion learning” or “sentiment analysis” or “sentiment recognition” || Literature types | proceedings papers, articles, review articles, early access |Table\\\\xa01. Search strategy for this studyThe reason this study uses 1997 as the starting point of the timeline is that the book _Affective computing_ ], which was published in that year, is regarded as the work that established affective computing as an independent academic research field. Papers outside this time range were not included in the calculation of citation statistics. In the statistics of Chinese papers, Hong Kong, Macau, and Taiwan are included. The results show that 33,448 papers were published worldwide. Among them, 16,097 (48.13%) were conference papers and 17,351 (51.87%) were journal papers. It should be noted that the names of institutions were standardized using machine and manual methods. However, when scientists publish papers, the writing of the names of institutions is not standardized, which may have caused the omission of papers in the statistics and a deviation in the index calculation results.In addition, this study combined the following 3 databases for data acquisition: (a) Incites: This database is based on the publication date of all document types in the major index databases of the WoSCC. It performs publication count and index calculations to provide research performance analysis. (b) Essential Science Indicators (ESI): This is an in-depth analytical research tool based on the Web of Science. ESI can identify influential countries, institutions, papers, and publications, as well as the cutting-edge in a research field. (c) Journal Citation Reports (JCRs): This is a multidisciplinary journal evaluation tool that provides journal evaluation resources based on citation data statistics. By citing and counting references, the JCR can measure the influence of research at the journal level, revealing the relationships between citing and cited journals.### Data analysisStatistical analysis was performed using a bibliometric method. Bibliometrics applies quantitative methods such as mathematics and statistics to the literature of a scientific or other field and processes statistical data based on information science theory. This widely accepted approach provides quantitative analysis pathways and innovative insights into the assessment of research trends based on previous literature , ]. Unlike peer review and expert judgment, bibliometrics can provide quantitative indicators to ensure objectivity through statistical analysis of academic achievements ]. Bibliometric analysis enables monitoring and summarizes the status, hotspots, and trends of a particular topic, helping researchers identify future research directions ]. In this study, we first cleaned and analyzed the data using the Derwent Data Analyzer (DDA, version 10, Clarivate, London, UK), which is well integrated with the source data from the Web of Science platform. DDA was used for multidimensional data mining, preprocessing, standardization, and statistical analysis. Subsequently, the bibliometric analysis and knowledge visualization software tool VOSviewer (version 1.6.15, Leiden University, Leiden, Netherlands) was employed. This analysis tool provides valuable insights into the structure, advancement, and collaboration in the field of affective computing. Notably, its distinctive feature lies in the graphical representation of bibliometric maps, which is particularly suitable for large-scale data analysis ]. VOSviewer was used to visualize the data in this study.## Results### Publication trendsFrom 1997 to 2009, the number of articles published in this field steadily increased, exhibiting an overall growth trend despite occasional fluctuations (Fig.\\\\xa02). From 2010 to 2019, with the rise of deep learning, a rapid development was observed in the field of affective computing, and the number of articles published in the field rose rapidly, indicating an explosive growth stage of research. After 2019, because of a plateau in the innovation of deep learning methods and the impact of the coronavirus disease 2019 (COVID-19) pandemic on academia, research in the field of affective computing also reached a plateau, and the rising trend slowed down.### Comparison of countriesTo analyze the main research positions in the field of affective computing, the country/region fields of all the authors and the first author of the paper were counted. As shown in Table , among the top 20 countries with publications in the field of affective computing, China is the country with the largest number of publications, accounting for 26.2% of all authors and 24.6% of first authors. China, the United States, India, the United Kingdom, and Germany rank among the top 5 in the number of papers published counting all authors or first author, and are the most important in terms of research in the field of affective computing. The United States ranks second in the number of papers published counting all authors, but third in the number of papers published counting only first author, after India.| No. | Country | Number of papers | Country | Number of papers || --- | --- | --- | --- | --- || (All authors) | (All authors) | (First author) | (First author) || --- | --- | --- | --- || 1 | China | 8,780 | China | 8,223 || 2 | USA | 4,715 | India | 3,632 || 3 | India | 3,829 | USA | 3,274 || 4 | UK | 2,535 | UK | 1,432 || 5 | Germany | 1,706 | Germany | 1,253 || 6 | Japan | 1,321 | Italy | 1,022 || 7 | Italy | 1,302 | Japan | 977 || 8 | Australia | 1,234 | South Korea | 931 || 9 | Spain | 1,178 | Spain | 862 || 10 | South Korea | 1,121 | Australia | 788 || 11 | Canada | 1,100 | Canada | 720 || 12 | France | 943 | France | 587 || 13 | Netherlands | 778 | Turkey | 581 || 14 | Saudi Arabia | 765 | Netherlands | 484 || 15 | Turkey | 691 | Malaysia | 479 || 16 | Singapore | 640 | Pakistan | 454 || 17 | Malaysia | 609 | Brazil | 443 || 18 | Pakistan | 595 | Greece | 413 || 19 | Brazil | 522 | Iran | 398 || 20 | Greece | 483 | Singapore | 394 |Table\\\\xa02. The top 20 countries in the field of affective computingIn addition to the 2-year step in 2021–2022, a 4-year step was used to count the publication volume of the top 10 countries in the field of affective computing. The results are shown in Fig. . Given that the concept of “affective computing” originated in the United States, which has been a major research force in this field, we chose the United States as the benchmark. During the entire period, the relative volume of publications by China and the United States changed considerably, as shown in Fig. . From 1997 to 2004, the number of papers published by the United States far exceeded that of China. From 1997 to 2000, the total number of papers published by China was 20% of that of the United States. From 2001 to 2004, the total number of papers published by China rose to 31% of that of the United States. In the period from 2005 to 2008, the number of papers published by China surpassed that of the United States, and the number of papers published by China in 2021–2022 is about 3 times that of the United States. It can be seen that in recent years, China’s research in the field of affective computing has accumulated rapidly, and its large volume of research has certain advantages compared with that of the United States. In addition, in 2021–2022, the number of papers published by India surpassed that of the United States for the first time. India has gradually become a major research center in the field of affective computing because of its advantages in computer science, engineering, and other disciplines.![](\\\\xa03. Comparison between the top 10 countries and the United States in the number of publications.### Main journalsThis section analyzes basic data on journal papers. The 17,351 published papers were distributed in 1,300 journals, among which _IEEE Access_ [impact factor (IF) 3.9] had the most (875), as shown in Table . Across all journals, 1,209 had an IF listed in the 2022 JCRs. The distribution of the IFs of the 1,209 journals is shown in Table . Among them, 54 journals have IFs greater than 10, and the 5 journals with the highest IFs are _World Psychiatry_ (73.3), _Lancet Psychiatry_ (64.3), _Nature Reviews Neuroscience_ (34.7), _Nature Human Behaviour_ (29.2), and _JAMA Psychiatry_ (25.8). The IFs of most journals are distributed in the 2 intervals of 2 ≤ IF < 4 and 4 ≤ IF < 7. It is worth noting that _IEEE Transactions on Affective Computing_ (IF 11.2) is a high-level journal focusing on the field of affective computing. It is a cross-disciplinary and international archive journal aimed at disseminating the results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. In addition, _Expert Systems with Applications_, _Knowledge-Based Systems_, _Information Processing & Management_, _IEEE Transactions on Multimedia_, _Neurocomputing_, _Information Sciences_, _Pattern Recognition_, _Applied Soft Computing_, _Decision Support Systems_, and _Future Generation Computer Systems_ are also high-level journals favored by scholars in the field of affective computing.| No. | Journal | Number of papers || --- | --- | --- || 1 | _IEEE Access_ | 875 || 2 | _Multimedia Tools and Applications_ | 474 || 3 | _IEEE Transactions on Affective Computing_ | 419 || 4 | _Sensors_ | 378 || 5 | _Frontiers in Psychology_ | 362 || 6 | _Applied Sciences-Basel_ | 349 || 7 | _Expert Systems with Applications_ | 290 || 8 | _International Journal of Advanced Computer Science and Applications_ | 272 || 9 | _Neurocomputing_ | 248 || 10 | _Knowledge-Based Systems_ | 226 || 11 | _Psychiatry Research_ | 191 || 12 | _Electronics_ | 167 || 13 | _Journal of Intelligent & Fuzzy Systems_ | 151 || 14 | _Neural Computing & Applications_ | 144 || 15 | _Neuropsychologia_ | 137 || 16 | _Schizophrenia Research_ | 135 || 17 | _Information Processing & Management_ | 132 || 18 | _Computational Intelligence and Neuroscience_ | 114 || 19 | _Cognitive Computation_ | 112 || 20 | _Information Sciences_ | 110 |Table\\\\xa03. Top 20 journals with the largest number of articles in the field of affective computing| Journal impact factor | Number of journals || --- | --- || IF ≥ 10 | 54 || 7 ≤ IF < 10 | 74 || 4 ≤ IF < 7 | 255 || 2 ≤ IF < 4 | 406 || 1 ≤ IF < 2 | 245 || IF ≤ 1 | 175 |Table\\\\xa04. Journal impact factor distribution### High-level international conferencesCombining ESI’s highly cited and hot papers with the “China Computer Federation Recommended International Academic Conferences” and CORE Computer Science Conference Rankings, we identified the high-level international conferences related to affective computing. These include the ACM International Conference on Multimedia (ACM MM), AAAI Conference on Artificial Intelligence (AAAI), Annual Meeting of the Association for Computational Linguistics (ACL), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), and the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).### Discipline distributionThis section analyzes the distribution of research fields based on statistics on the Web of Science categories of papers in the field of affective computing. Studies related to the topic of affective computing involve computer science, communication, engineering, psychology, medicine, and other disciplines, reflecting distinct interdisciplinary characteristics. The top 20 categories with the largest number of publications are listed in Table . The category with the largest proportion is “Computer Science, Artificial Intelligence,” with 12,678 publications (37.93% of the total), followed by “Engineering, Electrical & Electronic,” with 9,820 publications (29.36% of the total).| Web of Science category | Number of papers | Percentage (%) || --- | --- | --- || Computer Science, Artificial Intelligence | 12,687 | 37.90 || Engineering, Electrical & Electronic | 9,820 | 29.36 || Computer Science, Information Systems | 8,714 | 26.05 || Computer Science, Theory & Methods | 8,405 | 25.13 || Computer Science, Interdisciplinary Applications | 3,930 | 11.75 || Telecommunications | 3,133 | 9.37 || Computer Science, Software Engineering | 2,982 | 8.92 || Neurosciences | 2,376 | 7.10 || Psychiatry | 2,100 | 6.28 || Computer Science, Cybernetics | 1,904 | 5.69 || Imaging Science & Photographic Technology | 1,077 | 3.22 || Engineering, Multidisciplinary | 1,045 | 3.12 || Automation & Control Systems | 997 | 2.98 || Computer Science, Hardware & Architecture | 981 | 2.93 || Psychology, Multidisciplinary | 884 | 2.64 || Robotics | 793 | 2.37 || Engineering, Biomedical | 735 | 2.20 || Acoustics | 724 | 2.16 || Linguistics | 637 | 1.90 || Clinical Neurology | 610 | 1.82 |Table\\\\xa05. Top 20 categories with the most papers in the field of affective computing### Technology transfer and conversionThis study searched the Derwent Innovation Index, the world’s most comprehensive database of value-added patent information. Among effective invention patents with transfer records and high value, the transferred patents with an IncoPat patent value of 10 (the highest level) include “Cognitive content display device” (US10902058B2, transferred from IBM to Kyndryl Inc.) and “Signal processing approach to sentiment analysis for entities in documents” (US9436674B2, transferred from Attivio Inc. to Servicenow Inc.). However, the number of patent transfer records related to affective computing is small, indicating that technology transfer activity needs to be improved.### Global distribution of scholarsThis section presents statistical analysis of publications based on the country of the first author to provide a macroscopic understanding of the global distribution of scholars in the field of affective computing. As shown in Table , China has the largest number (4,240), followed by India (2,391) and the United States (2,390). In Fig. , darker shading indicates a larger number of scholars. It can be seen that Asia and North America are the regions with the most concentrated distribution of scholars in the field of affective computing.| No. | Country | Number of scholars | No. | Country | Number of scholars || --- | --- | --- | --- | --- | --- || 1 | China | 4,240 | 11 | Canada | 533 || 2 | India | 2,391 | 12 | France | 425 || 3 | USA | 2,390 | 13 | Turkey | 403 || 4 | UK | 999 | 14 | Netherlands | 349 || 5 | Germany | 825 | 15 | Malaysia | 331 || 6 | Italy | 690 | 16 | Pakistan | 324 || 7 | Japan | 631 | 17 | Brazil | 366 || 8 | South Korea | 514 | 18 | Greece | 248 || 9 | Spain | 545 | 19 | Iran | 270 || 10 | Australia | 496 | 20 | Singapore | 229 |Table\\\\xa06. Number of first authors in the field of affective computing (top 20 countries)![](\\\\xa04. Global distribution of scholars in the field of affective computing.### International collaborationThere is a wide range of international cooperation in the field of affective computing. A count of collaborations between the top 20 countries is shown in Table . The number of articles published by China and the United States is the largest (641), followed by China and the United Kingdom (343). Although cooperation between China and the United States has been challenging in recent years, in the field of affective computing, they remain each other’s largest partners, maintaining a vital and continuous cooperation.|  | C1 | U1 | I1 | U2 | G1 | J | I2 | A | S1 | S2 | C2 | F | N | S3 | T | S4 | M | P | B | G2 || --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- || C1 | / | 641 | 79 | 343 | 79 | 256 | 59 | 218 | 43 | 61 | 137 | 44 | 28 | 61 | 12 | 161 | 38 | 57 | 6 | 4 || U1 | 641 | / | 128 | 232 | 174 | 42 | 120 | 122 | 62 | 83 | 153 | 100 | 105 | 55 | 42 | 71 | 10 | 36 | 53 | 23 || I1 | 79 | 128 | / | 73 | 15 | 15 | 22 | 37 | 19 | 33 | 26 | 26 | 8 | 67 | 10 | 48 | 26 | 11 | 3 | 2 || U2 | 343 | 232 | 73 | / | 294 | 40 | 132 | 119 | 96 | 15 | 60 | 89 | 160 | 77 | 25 | 64 | 30 | 43 | 30 | 48 || G1 | 79 | 174 | 15 | 294 | / | 41 | 69 | 47 | 39 | 11 | 43 | 59 | 99 | 4 | 17 | 17 | 5 | 7 | 13 | 19 || J | 256 | 42 | 15 | 40 | 41 | / | 4 | 20 | 17 | 7 | 27 | 15 | 11 | 6 | 5 | 18 | 16 | 1 | 3 | 1 || I2 | 59 | 120 | 22 | 132 | 69 | 4 | / | 22 | 58 | 13 | 25 | 65 | 53 | 10 | 13 | 37 | 4 | 10 | 6 | 7 || A | 218 | 122 | 37 | 119 | 47 | 20 | 22 | / | 26 | 14 | 30 | 22 | 24 | 27 | 14 | 35 | 25 | 23 | 11 | 3 || S1 | 43 | 62 | 19 | 96 | 39 | 17 | 58 | 26 | / | 15 | 15 | 42 | 46 | 23 | 12 | 11 | 5 | 10 | 24 | 18 || S2 | 61 | 83 | 33 | 15 | 11 | 7 | 13 | 14 | 15 | / | 10 | 16 | 6 | 29 |  | 2 | 9 | 71 | 3 | 3 || C2 | 137 | 153 | 26 | 60 | 43 | 27 | 25 | 30 | 15 | 10 | / | 45 | 19 | 40 | 10 | 9 | 2 | 12 | 17 | 3 || F | 44 | 100 | 26 | 89 | 59 | 15 | 65 | 22 | 42 | 16 | 45 | / | 39 | 11 | 4 | 2 | 8 | 16 | 18 | 10 || N | 28 | 105 | 8 | 160 | 99 | 11 | 53 | 24 | 46 | 6 | 19 | 39 | / |  | 19 | 8 | 3 | 1 | 11 | 14 || S3 | 61 | 55 | 67 | 77 | 4 | 6 | 10 | 27 | 23 | 29 | 40 | 11 |  | / | 8 | 4 | 41 | 120 | 1 | 3 || T | 12 | 42 | 10 | 25 | 17 | 5 | 13 | 14 | 12 |  | 10 | 4 | 19 | 8 | / | 5 | 7 | 3 | 2 | 1 || S4 | 161 | 71 | 48 | 64 | 17 | 18 | 37 | 35 | 11 | 2 | 9 | 2 | 8 | 4 | 5 | / | 4 | 2 |  | 1 || M | 38 | 10 | 26 | 30 | 5 | 16 | 4 | 25 | 5 | 9 | 2 | 8 | 3 | 41 | 7 | 4 | / | 40 |  | 3 || P | 57 | 36 | 11 | 43 | 7 | 1 | 10 | 23 | 10 | 71 | 12 | 16 | 1 | 120 | 3 | 2 | 40 | / | 3 |  || B | 6 | 53 | 3 | 30 | 13 | 3 | 6 | 11 | 24 | 3 | 17 | 18 | 11 | 1 | 2 |  |  | 3 | / |  || G2 | 4 | 23 | 2 | 48 | 19 | 1 | 7 | 3 | 18 | 3 | 3 | 10 | 14 | 3 | 1 | 1 | 3 |  |  | / |Table\\\\xa07. Collaborations between the top 20 countries in the field of affective computingNote: C1, China; U1, USA; I1, India; U2, UK; G1, Germany; J, Japan; I2, Italy; A, Australia; S1, Spain; C2, Canada; S2, South Korea; F, France; N, Netherlands; T, Turkey; S3, Saudi Arabia; S4, Singapore; M, Malaysia; P, Pakistan; B, Brazil; G2, Greece.### Important research institutionsThe top 10 institutions in the world by number of publications (counting all authors) are listed in Table . This study used indicators such as Citation Impact, Category Normalized Citation Impact (CNCI), and Highly Cited Papers to further evaluate the influence of various institutions in the field of affective computing. Among them, CNCI is a valuable and unbiased impact indicator that excludes the influence of publication year, subject field, and document type. A CNCI value of 1 indicates that the cited performance of a group of papers is equivalent to the global average level, a value greater than 1 indicates higher performance, and a value less than 1 indicates lower performance; a value of 2 indicates performance twice as high as the global average. The top 5 institutions according to CNCI rankings were Nanyang Technological University (5.06), Imperial College London (3.58), Tsinghua University (3.23), the Chinese Academy of Sciences (3.15), and the University of California System (2.77).| No. | Institution | Number of papers | Citation impact | Category Normalized Citation Impact | H-index | Percentage in Q1 journals | Country || --- | --- | --- | --- | --- | --- | --- | --- || 1 | Chinese Academy of Sciences | 699 | 20.97 | 3.15 | 60 | 59.87 | China || 2 | University of London | 443 | 50.26 | 2.29 | 77 | 69.45 | UK || 3 | UDICE-French Research Universities | 388 | 18.86 | 1.37 | 42 | 50.43 | France || 4 | Centre National de la Recherche Scientifique (CNRS) | 377 | 19.33 | 1.36 | 42 | 51.56 | France || 5 | University of California System | 371 | 40.83 | 2.77 | 64 | 58.72 | USA || 6 | National Institute of Technology (NIT System) | 364 | 9.68 | 1.46 | 29 | 26.43 | India || 7 | Indian Institute of Technology System (IIT System) | 360 | 13.51 | 1.99 | 36 | 44.7 | India || 8 | Nanyang Technological University | 350 | 46.35 | 5.06 | 69 | 68.99 | Singapore || 9 | Tsinghua University | 302 | 24.87 | 3.23 | 44 | 62.93 | China || 10 | Imperial College London | 300 | 41.00 | 3.58 | 49 | 70.75 | UK |Table\\\\xa08. Institutions with a top 10 publication in affective computingNotes: 1. Citation impact: The citation impact of a set of documents is calculated by dividing the total number of citations of the set of documents by the number of documents. Citation impact shows the average number of citations received by a document in the group. 2. Category Normalized Citation Impact (CNCI): The CNCI of a document is obtained by dividing the actual number of citations by the expected number of citations of documents of the same type, publication year, and subject. When a document is classified into multiple subject areas, the average value of the ratio of actual citations to expected citations is used. The CNCI of a country is the average of the CNCIs of the publications of that country.### Citation network analysisThis section analyzes the direct citations of all authors in the field of affective computing. To highlight the key authors, 40 authors who had published no fewer than 30 papers were selected for analysis. The results are shown in Fig. . Authors in clusters of the same color have strong correlations and inheritance in research content. Representative scholars from the 5 clusters are listed in Table .![](\\\\xa05. Citation network of scholars.| Scholar | Organization | Research fields || --- | --- | --- || Baoliang Lu | Shanghai Jiaotong University, China | Brain-like computing, neural networks, deep learning, emotion AI, affective brain–computer interface || Bjoern Schuller | Imperial College London, UK | Machine intelligence, signal processing, affective computing, digital health, speech recognition || Erik Cambria | Nanyang Technological University, Singapore | Affective computing, sentiment analysis, commonsense reasoning, natural language understanding || Fuji Ren | The University of Tokushima, Japan the University of Electronic Science and Technology of China, China | Natural language processing, artificial intelligence, affective computing, and emotional robots || Wenming Zheng | Southeast University, China | Multimodal affective computing, neural computation, pattern recognition, machine learning, and computer vision |Table\\\\xa09. Representative scholars in the citation network### Word frequency analysisWord frequency refers to the number of times a word occurs in the document being analyzed. In scientometric research, word frequency dictionaries can be established for specific subject areas to quantify the analysis of scientists’ creative activities. Word frequency analysis is the method of extracting keywords or subject words that express the core content of the articles in the literature, to study the development trends and research hotspots of the field through the frequency distribution of these words. The results of conducting frequency and co-occurrence analysis on keywords assigned to papers by authors in the field of affective computing are shown in Table .| No. | Number of occurrences | Technical keyword | Number of co-occurrences with other keywords | Time period | Proportion of occurrences within last 3 years (%) || --- | --- | --- | --- | --- | --- || 1 | 7,621 | Sentiment analysis | Machine learning [958]; Opinion mining [936]; Natural language processing [829] | 2006–2023 | 21 || 2 | 4,566 | Emotion recognition | Feature extraction [422]; Affective computing [397]; Deep learning [372] | 1997–2023 | 24 || 3 | 2,457 | Affective computing | Emotion recognition [397]; Machine learning [191]; Emotion [137] | 2000–2023 | 15 || 4 | 2,232 | Deep learning | Sentiment analysis [691]; Emotion recognition [372]; Machine learning [268] | 2012–2023 | 40 || 5 | 2,054 | Machine learning | Sentiment analysis [958]; Natural language processing [275]; Deep learning [268] | 2002–2023 | 27 || 6 | 1,816 | Facial expression recognition | Deep learning [182]; Feature extraction [150]; Face recognition [109] | 1997–2023 | 18 || 7 | 1,348 | Natural language processing | Sentiment analysis [829]; Machine learning [275]; Deep learning [209] | 2006–2023 | 30% of 1,348 || 8 | 1,214 | Feature extraction | Emotion recognition [422]; Sentiment analysis [213]; Task analysis [181] | 2003–2023 | 32 || 9 | 1,209 | Opinion mining | Sentiment analysis [936]; Natural language processing [159]; Machine learning [151] | 2006–2023 | 11 || 10 | 1,067 | Emotion | Affective computing [137]; Emotion recognition ]; Facial expression ] | 1999–2023 | 13 || 11 | 1,007 | Twitter | Sentiment analysis [770]; Machine learning [160]; Social media [145] | 2011–2023 | 18 || 12 | 975 | Speech emotion recognition | Deep learning ]; Feature extraction ]; Emotion recognition ] | 2006–2023 | 29 || 13 | 852 | Social media | Sentiment analysis [587]; Twitter [145]; Machine learning [105] | 2009–2023 | 21 || 14 | 732 | Social cognition | Schizophrenia [193]; Emotion recognition [184]; Theory of mind [179] | 2001–2023 | 16 || 15 | 657 | Text mining | Sentiment analysis [486]; Natural language processing ]; Opinion mining ] | 2006–2023 | 15 || 16 | 635 | EEG | Emotion recognition [357]; Affective computing ]; Emotion ] | 2004–2023 | 27 || 17 | 620 | Classification | Sentiment analysis [208]; Machine learning ]; Emotion recognition ] | 2003–2023 | 19 || 18 | 618 | Facial expression | Emotion recognition [175]; Emotion ]; Affective computing ] | 1998–2023 | 15 || 19 | 582 | Convolutional neural network | Deep learning [146]; Facial expression recognition ]; Emotion recognition ] | 2003–2023 | 30 || 20 | 535 | Schizophrenia | Social cognition [193]; Emotion recognition ]; Theory of mind ] | 1998–2023 | 8 || 21 | 478 | Support vector machine | Sentiment analysis [123]; Facial expression recognition ]; Emotion recognition ] | 2002–2023 | 9 || 22 | 470 | Feature selection | Sentiment analysis [119]; Emotion recognition ]; Feature extraction ] | 2001–2023 | 16 || 23 | 423 | Face recognition | Feature extraction [155]; Emotion recognition [124]; Facial expression recognition [109] | 1997–2023 | 29 || 24 | 422 | Transfer learning | Emotion recognition ]; Deep learning ]; Sentiment analysis ] | 2009–2023 | 40 || 25 | 404 | Data mining | Sentiment analysis [251]; Feature extraction ]; Machine learning ] | 2006–2023 | 22 |Table\\\\xa010. Frequency analysis of top 25 keywords in affective computingThe Thomson Data Analyzer was used to automatically and manually clean the keywords assigned by the authors of papers in the dataset. Subsequently, VOSviewer was used to cluster the core (high-frequency) subject words and set a certain co-occurrence frequency and co-occurrence intensity according to the size of the dataset to cluster the keywords. Combined with expert interpretation, each cluster was named and interpreted, and the topics of the journal articles were identified and analyzed. After keyword cleaning, 613 keywords appearing more than 20 times were selected as analysis objects for cluster calculation. Five clusters were obtained by clustering the core subject words with the highest co-occurrence intensity, as shown in Table  and Fig. .| No. | Research theme | Number of core subject words | Average number of citations | Average correlation strength || --- | --- | --- | --- | --- || 1 | Natural language processing techniques used for affective computing and opinion mining | 153 | 10.41 | 197.80 || 2 | Facial expression and micro-expression recognition and analysis | 134 | 15.89 | 178.77 || 3 | Affective computing studies in human–computer interaction | 121 | 18.69 | 110.38 || 4 | Applied research of affective computing in affective disorder analysis | 30 | 33.5 | 165.59 || 5 | Multimodal sentiment analysis based on deep learning | 81 | 9.8 | 260.95 |Table\\\\xa011. Five research themes in affective computing![](\\\\xa06. Five research themes in affective computing.The average number of citations of a research theme is the average number of times that a paper containing these subject words has been cited since publication, and the average correlation strength of a research theme indicates the closeness of the connection between the core subject words contained in this theme concept. The greater the correlation strength, the greater the co-occurrence intensity between the core subject words and the more concentrated the research. In contrast, relatively lower correlation is associated with more scattered research. Research on the application of affective computing in the analysis of affective disorders has the highest average citation frequency, which shows that interdisciplinary research involving affective computing and medicine, especially research on affective disorders and depression recognition, has a greater influence. The average correlation strength of multimodal sentiment analysis based on deep learning is the largest, which shows that the research on this topic is the most concentrated.## DiscussionThis paper presents a comprehensive analysis and review of systematically collected data on papers and major intellectual property rights in the field of affective computing. The results reveal that over the past 25 years, affective computing has experienced rapid growth in the number of published papers, representing a vibrant academic ecology and an interdisciplinary character with a wide range of disciplines. Additionally, scholars worldwide actively participate in a relatively close cooperation network. In particular, Chinese scholars have led the world in terms of the number of publications, scholars, and collaborative papers in this field. Among important research institutions, Tsinghua University and the Chinese Academy of Sciences stand out, with CNCI values indicating that the average number of citations of their papers was more than twice the global average. Citation network analysis showed that Chinese scholars are representative and have become essential nodes in the citation network, indicating that China is constructing a large-scale talent team for affective computing and progressing in both the quantity and quality of research. However, China also faces disadvantages in academic journals, international conferences, and other aspects, leading to weak dominance, which restricts China’s academic discourse improvement in this field. Notably, in recent years, India’s publication volume has exceeded that of the United States for the first time, revealing a robust development potential linked to its advantages in computing. Nonetheless, India still has room for growth in terms of research quality and paper impact as it lacks representative scholars in the field of affective computing.### Challenges and technology development trends#### Modeling of cultural contextsThis study found that affective computing researchers are distributed across various countries globally and have a wide range of cultural backgrounds. While emotional expression has a degree of consistency across humanity, it is considerably influenced by cultural background. Cultural norms and values determine the different emotional experiences of individuals and how others perceive these emotions. Therefore, affective computing systems developed using a single cultural group may fail in other cultural contexts. For example, Chinese, Germans, and Japanese express emotions relatively implicitly, whereas Americans, British, and Brazilians express emotions more overtly. This indicates that emotion agents must match emotion calculation rules with the cultural context. Many Western cultural standards may not necessarily apply in Eastern contexts. For example, Japanese researchers tend to develop robots that can express emotions implicitly because overly direct expressions of emotions may cause user dissatisfaction ]. Therefore, cultural characteristics must be considered in developing universal cross-cultural emotional agents for people from different cultural backgrounds. Hofstede defined culture in terms of 5 measures—power distance, identity, gender, uncertainty avoidance, and long-term orientation—which can be used to summarize the typical rules of emotional expression in different cultural contexts ]. When it is challenging to obtain culture-specific empirical affective data, it is more feasible to design affective computational models using cultural theories and rules.#### Emotion generation techniquesThe cluster analysis of topic terms in affective computing revealed 5 important core topics, including “natural language processing techniques for affective computing and opinion mining” and “facial expression and micro-expression recognition and analysis.” Current research focuses more on emotion recognition, with relatively limited attention accorded to emotion generation. Emotion recognition and generation are both essential aspects of affective computing and constitute an important technical basis for the closed loop of human–computer interaction. To enable machines to provide more anthropomorphic and natural feedback, it is crucial to focus on the following 2 research areas. (a) Generation of facial expressions. The fact that human emotions are expressed through visual (55%), voice (38%), and verbal (7%) signals is also known as the “3V rule,” which reflects the importance of human facial expressions in emotion analysis ]. Appropriate use of facial expressions by avatars and robots can enhance human–robot interaction. Thus, current research aims to build a lexicon of facial expressions that can translate communicative intent into associated expressive morphology and dynamic features to express various meanings. Meanwhile, a team of animation experts is required to achieve realistic facial rendering effects, including lighting and muscle textures. (b) Generation of emotional body movement. This requires the design of embodied agents using computer models of body expression. This area involves studying human kinematics; however, researchers have yet to determine how to characterize the organic combination of body parts, movement strength, and posture of specific emotional states.#### Fine-grained sentiment classification modelsEkman’s basic emotion theory model is a widely used classification model for emotion computation ]. However, in real life, people’s emotions often exist in a mixed state. For example, people often simultaneously express surprise and joy, sadness and pain, etc. Du et\\\\xa0al. ] proposed the concept of mixed emotions based on research conducted using the Facial Action Coding System (FACS). They suggested that the combination of 2 basic emotions creates mixed emotions and defined different types using scenario examples. Using a FACS-based face recognition algorithm model, microvariations in facial muscles can be analyzed to accurately discriminate between different types of mixed emotions. Martinez ] assessed whether mixed emotions can be semantically labeled correctly. The test tasks included prioritization and forced selection of mixed emotion labels, and the results showed that subjects performed consistent and accurate categorization. Mixed emotion is an essential research direction for expression-based fine-grained emotion classification. This concept extends the core idea of FACS, aiming to reveal the relationship between mixed and basic emotions. It offers a better solution to the problem of differentiation of emotions and clarifies the relationship between differentiated emotions and their original emotions, providing traceable clues and measurement possibilities for the generation, development, and change of emotions. It summarizes complex emotional changes into a logical dynamic composite form with similar configuration effects, resulting in strong interpretability, logic, and unity.#### Code of ethics and technical standardsRecording an individual’s emotional state has implications for privacy, particularly when it comes to recording video or audio. Subjects may not agree to provide researchers with authentic and naturalistic emotional data and may feel uncomfortable being monitored in daily life. For example, the results of AI emotion monitoring tools may be analyzed alongside employee performance evaluations, predictions of the risk of leaving the job, and patterns of employee–team interactions for predicting behavior. Although the use of such technology reduces employee turnover and saves costs for organizations ], employees may experience constant psychological stress, leading to burnout ]. Additionally, individuals may lose autonomy as they become more hesitant to display emotions in public, instead choosing to use a “poker face.” While there should be openness in the use of affective computing, appropriate regulation is necessary to assess potential risks involving privacy and security, and the technology should be reviewed and documented for each industry to maximize benefits while minimizing harm, risks, and costs. Ethical issues are more likely to be overlooked in computing and engineering than in psychology. The collection of individual data, particularly physiological data, should be regulated by human research ethics committees, which are best suited to managing informed consent and privacy issues.Efforts should be made to strengthen the development of international standards in the field of affective computing to form a universally accepted specification. Currently, the available standard is “Information technology—Affective computing user interface (AUI)” (standard number ISO/IEC 30150-1:2022). The first part, “Model,” was released in June 2022, and the second part, “Affective Characteristics,” is under construction. However, there is a lack of standards for data collection, data security, and personal privacy protection in the field of affective computing. Therefore, the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union (ITU) should improve relevant standards and unify them for global use.#### Cognitive neuroscience-inspired affective computingJust as CNN architectures are inspired by biological visual processing and reinforcement learning methods are inspired by behaviorist theories in psychology, impulse network models are inspired by neuroplasticity. Cognitive neuroscience has also developed theories on affective circuits ], multiple-wave models ], embodied cognition ], and other related areas, providing brain-inspired insights into the design of affective computation models. Studies on the physiological representations of different emotions offer theoretical foundations and guidelines for feature extraction in affective computing based on facial expressions, psychophysiological measurements, and neuroimaging. Further human research in the field of cognitive neuroscience will ultimately affect the development of affective computing and artificial intelligence as a whole. The cognitive process of human brain emotion processing, its neural mechanism, and its anatomical basis provide essential inspiration for the development of affective computing models. However, to ensure that machines have genuine emotions rather than just appearing to have emotions, further research in cognitive neuroscience is required. This research may involve exploring the neural basis for the generation of human consciousness, the neural mechanism for the construction of human values, and other key scientific issues. Based on this neural theoretical foundation, simulation and machine implementation are feasible options for providing machines with authentic emotions.#### Construction of large-scale multimodal datasetsThe development of affective computing is highly dependent on the construction of large-scale open datasets. Three major trends are described below. The first trend predicts that dataset sizes will continue to grow to meet the demands of deep learning algorithm training. Deep-learning models have a substantial number of parameters, and the selection of these parameters requires samples that are typically 100 times the number of parameters. A larger dataset size enables the trained model to avoid overfitting, which improves model learning. However, the challenge lies in labeling these massive datasets. Thus, it is necessary to explore active, weakly supervised, and unsupervised learning methods to label the meaningful data in large unlabeled datasets or train machines for labeling. The second trend highlights the need for the collection of multimodal data, the accumulation of richer modal information, and fine-grained alignment between different modalities. At this stage, machines differ from human beings in 2 critical aspects: First, humans exist in a multimodal social environment, as evidenced by their joint expression of intentions and emotions through language, facial expressions, speech, and actions; second, humans can switch between modalities for emotional reasoning when dealing with emotions. They can also switch between different modalities to search for clues, eliminate ambiguities, and conduct emotional reasoning through interconnections. Therefore, creating a large-scale multimodal emotion dataset can contribute to the development of human-like emotion intelligence technology and the realization of more accurate emotion recognition. The third trend focuses on collecting natural-scene data, as emotional data collected in performance or evoked mode may not accurately represent real-life scenarios. However, collecting high-quality labeled emotional-physiological data in daily life remains a challenge due to the lack of hardware collection devices that are sufficiently comfortable and resistant to interference.#### Multimodal fusion technology innovationMultimodal fusion combines information from multiple modalities using multimodal representations for sentiment classification. It can enhance the performance of sentiment-computing models by playing a complementary and disambiguating role ]. Multimodal fusion methods can be classified as model-independent or model-based. Model-independent fusion methods do not rely on a specific deep-learning method, whereas model-based fusion methods do.There are 3 categories of model-independent fusion methods: early fusion (feature-based fusion), late fusion (decision-based fusion), and hybrid fusion (combination of the 2). Early fusion integrates features immediately after they are extracted and uses multiple signals to create a single feature vector, which is then modeled using machine-learning algorithms. The larger the number of features and the greater the variation in these features, the more challenging feature-level fusion becomes and the easier it is to overfit the training data. In contrast, late fusion performs integration only after each model outputs the results (e.g., classification or regression results). It can better handle overfitting but does not allow the classifier to train on all data simultaneously. The Dempster–Shafer theory of evidence is a generalization of Bayesian theory to subjective probability. It is widely used in late fusion models because of its ability to model uncertain knowledge and combine beliefs from different sources to obtain new beliefs that take into account all available evidence. Hybrid fusion combines the outputs of earlier fusion methods and unimodal predictors. Although it is flexible, careful design is required to determine the timing, modalities, and method of fusion based on the specific application problem and research content. Researchers must select the appropriate approach at their discretion.Model-based fusion methods address the multimodal fusion problem through implementation techniques and models, using 3 common methods: multiple kernel learning (MKL), graphical models (GMs), and neural networks (NNs). As these methods easily exploit the spatial and temporal structure of the data, they are particularly suitable for time-related modeling tasks. Additionally, they allow human expert knowledge to be embedded in the model, thereby enhancing interpretability. However, their disadvantage is that they are computationally expensive and challenging to train.Research has shown that synesthesia is generated not only in the cerebral cortex but also in the subcortical limbic system, including the thalamus, amygdala, and hippocampus, which are closely related to emotional processing ]. Inspired by the multistage fusion phenomenon that integrates multisensory information in the brain, a multistage multimodal emotion fusion method can be developed. This would first involve training a unimodal model, splicing it as an implicit state with another modal feature, training the bimodal model similarly, and continuing with this process until a multimodal model is obtained. In conclusion, multimodal fusion technology effectively utilizes the synergistic complementarity of different modal information ], enhances emotional understanding and expression, and improves model robustness and performance. This represents an important direction for future research.#### Data- and knowledge-driven technological innovationIn its early stages, affective computing research relied heavily on collected data to make inferences. However, this data-driven approach is both inefficient and ineffective at the application level. For humans to understand data fully, they must activate other associated information, such as potential knowledge or common sense. The human brain can seamlessly combine this information to enable more generalized, intelligent, and frugal computation for complex problems. Therefore, affective computing requires not only big data and extensive computing power but also the integration of knowledge. Knowledge guidance and inspiration can compensate for insufficient or uneven data quality while conserving computational power. For instance, in constructing a multidisciplinary and multi-faceted emotional knowledge map, fine-grained emotional knowledge integrated through emotional commonsense associations is used to enable the modeling of hierarchical logical relationships between aspect words and emotional words. This approach facilitates the dynamic correlation, aggregation, and reasoning of domain, aspect, and emotional knowledge. It provides an optimal solution for various applications of affective computing, such as efficient real-time online sentiment analysis, emotion-injected dialogue systems, and emotion-injected story generation. These applications provide dynamic and accurate domain-adaptive sentiment knowledge.#### Group affective computingCurrent research in affective computing primarily focuses on sentiment analysis at the individual level, neglecting the potential value of group-affective computing. For instance, emotions felt by individual employees can aggregate and spread to create “collective emotions” in the workplace. These shared emotions can considerably affect the organization by offering insights into absenteeism, intra-team communication, team cohesion and performance, and organizational citizenship behavior. As such, affective computing research could expand its focus from individual to collective affect analysis and the propagation of affect across people. Furthermore, group affective computing can predict consumer behavior. EEG-based hyperscanning technology, which explores dynamic brain activity between 2 or more interacting customers and their underlying neuroemotional activities, can be used to anticipate shared consumption intentions, panic buying, and group-buying marketing effects. Although group affective computing currently lacks a well-established research methodology, it is a promising direction for future studies.#### Unique emotional carriersEmotions are ubiquitous in human political, economic, and cultural life, and the carriers of emotions are continually increasing in number, making them a popular research topic. Several areas have been identified as key carriers of emotions. (a) Political speeches: CORPS is a corpus that contains political speeches with markers indicating audience reactions such as applause, standing ovations, and boos ]. Researchers can use this information to predict emotion-evoking actions and persuasive content that may induce empathy and sympathy in audiences. (b) Music and drama: Affective computing in music and drama provides a basis for the categorized retrieval of relevant emotional carriers. Advancements in artificial intelligence-generated content (AIGC) technology have made machine-generated music possible, and affective computing can enhance the generation of music to conform to emotional classifications. (c) Oil painting: As a representative art form, oil painting allows creators to express their innermost emotions. Its charm lies not in the degree of realism but in the emotions it conveys. Combining affective computing with oil painting would enable the exploration of artificial intelligence methods for emotional expression, the integration of technology and art, and the establishment of a library of emotion-inducing materials for oil paintings, thereby providing resources for the development of affective computing disciplines.### Outlook for future applications#### Affective brain–computer interfacesAffective brain–computer interfaces (aBCIs) are primarily designed to measure emotional states through neurological measurements and to recognize and/or regulate human emotions. Currently, aBCIs are one of the main methods of realizing emotional intelligence. At this stage, the most commonly used physiological signals for emotional brain–computer interfaces are EEG signals, which map closely to an individual’s emotional state. As in motor brain–computer interfaces, the human brain plays the role of a controller for the entire system. The first step involves decoding an individual’s initial emotional state and then recognizing and understanding their emotions. Subsequently, a control strategy or system is designed to achieve the target emotion using control signals or parameters that provide feedback to the brain, thereby forming a closed-loop system.Unlike facial expressions, physiological signals such as EEG signals are difficult to disguise and provide an accurate reflection of the real emotional state of the individual. As a result, affective brain–computer interfaces play a crucial role in clinical diagnostics and therapy. Their uses include detecting workload and mental state, using neurofeedback for stress relief, aiding in the diagnosis of social anxiety and other disorders ], and enabling objective assessment and intervention in depression. Furthermore, affective brain–computer interfaces have considerable potential for military applications. They can help maximize the physiological capabilities of individual soldiers, enhance their endurance and tolerance to extreme environments, and improve their overall physical and mental fitness. These objectives are achieved by installing electroencephalography electrodes inside combat helmets to detect threats and emotional signals emitted by the brain. The signals are then converted into computer language using computer algorithms, analyzed, and confirmed by combat command. Subsequently, threat warnings and reminders about emotional regulation are sent to the affected soldiers, and signals to cooperate in combat are transmitted to surrounding soldiers. In addition, direct transcranial current stimulation, transcranial electromagnetic stimulation, and deep brain cortex stimulation can act on the brain to eliminate fatigue, reduce stress and anxiety, control pain sensation, and enhance cognitive ability. This system helps improve the situational awareness of soldiers on the battlefield, thereby improving their ability to survive.The primary obstacle to the application of affective brain–computer interfaces is their unstable performance. Cross-modal affective models that rely on heterogeneous transfer learning (HTL) may be necessary for establishing reliable and robust aBCI technology in complex real-world environments. To address the missing-modalities problem, cross-modal emotion models comprehensively analyze signals from multiple modalities and extract correlation characteristics during the training process. In the testing stage, predictions are made based on partial modal information. For example, correlating EEG signals with eye movement enables the use of eye movement alone to assess emotions in scenarios where collecting EEG signals is difficult. The HTL approach ensures that performance degradation in the absence of modalities is acceptable, thereby improving model robustness. In addition, transfer-learning techniques based on deep and generative adversarial networks can solve the problem of individual differences. These techniques enable generalization from the source domain to the target domain, thereby expanding the scope of possible applications of affective brain–computer interfaces.#### Empathic human–computer dialogueThere have been 4 waves of change in the way people interact with machines. The first wave, represented by Microsoft, involved the organic fusion of the user interface, operating system, keyboard, and mouse. This greatly reduced the difficulty of human–computer interaction and contributed to the rapid popularization of the personal computer. The second wave, represented by Google, involved the organic integration of search engine and internet technologies. This integration broke down information silos and considerably expanded the boundaries of interaction. The third wave, represented by Apple, involved the miniaturization of computing represented by the smartphone. This breakthrough removed the physical space limitations of human–computer interaction, enabling interconnectivity anytime, anywhere. Currently, we are in the fourth wave, represented by OpenAI. This wave involves the comprehensive application of a human–computer dialogue system that makes human–computer interaction more anthropomorphic and naturalized.The essence of human–computer dialogue is to make human–computer interaction more human-like. Humans exchange information through natural language and multiple senses, and human–computer interaction can imitate this process through multimodal information for joint analysis and decision-making. Human–computer dialogue involves a diverse range of signals, including speech, text, and images (such as individual facial expressions and body movements), conveying information in both the rational and perceptual dimensions. Linguistic text serves as the ontology of intent understanding, but emotional information conveyed through voice intonation, facial expressions, and body movements plays a crucial role in disambiguation, which is essential for in-depth communication between humans and machines. The use of different emotional colors to express the same sentence results in entirely different connotations. As Nobel Prize winner Simon noted, emotion recognition is crucial for the communication and understanding of information. Therefore, affective computing offers machines the ability to achieve deep contextual understanding.In advanced technology fields, research has expanded to include machine expression and action generation, referred to as “multimodal emotional expression generation.” A current focus area is the development of a “virtual human” interface that not only appears human-like but also simulates human demeanor and behavior. For instance, voice-driven facial-expression animation generation technology can create virtual humans with facial expressions and lip, head, and body movements that closely resemble those of real people. The virtual human no longer has an empty skin but appears more 3-dimensional and vivid. The personalization of human–computer interaction lays the crucial foundations for future applications in areas such as elderly companions, intelligent customer service, and mayor hotlines, revealing important prospects for practical use.#### Emotion-assisted decision-makingHuman–computer interaction involves both shallow and deep levels. At the shallow level, machines are equipped with the ability to read and speak, whereas at the deep level, they are capable of thinking and making decisions like humans. Nobel Prize winner Kahneman described human decision-making as entailing 2 processes: fast (System 1) and slow (System 2). The unconscious “System 1” relies on emotions, experience, and rapid judgments, while the conscious “System 2” relies on rational deliberation. Emotions play an important role in advanced human thinking and decision-making. The book “Descartes’ Error” emphasizes that emotions are crucial for rational decision-making and behavior ]. Numerous studies have indicated that purely rational decision-making may not always be the optimal solution for humans when dealing with problems due to the complexity of the social environment. Incorporating emotional factors into the decision-making process may help individuals identify better solutions. Therefore, inputting emotional variables can enable machines to make decisions in a more human-like manner. In building a harmonious human–machine symbiotic society, it is essential to master this high-level function, which is also an important direction in affective computing research. The modeling of machine agents has begun to incorporate patterns of emotional influence on human rational decision-making and mechanisms for deciding and interrupting behaviors based on goals , ].Emotion-assisted decision-making abilities can be applied widely across various fields of human–machine collaboration. For example, in production tool manipulation, the operator’s emotional state regarding operation specifications, safety awareness, and accurate judgment has an impact. Monitoring and early warning of negative emotions, psychological stress, fatigue, and drowsiness, etc., can help identify potential anthropogenic risks to production safety. Machines can then optimize management decisions, intervene early, and intervene intelligently to avoid major accidents. In assisted driving, negative emotions such as anger and anxiety can seriously affect the driver’s concentration and may lead to traffic accidents. Emotion-assisted decision making can be incorporated into driver monitoring systems (DMS) that use facial-expression recognition technology and wearable devices to provide real-time monitoring of the driver’s emotional state. This approach equips the vehicle with enhanced safety performance and improves the overall driving experience ].#### Affective virtual realityThe metaverse is generating considerable interest in both industrial and academic circles as the next generation of immersive, full-fledged internet. It is considered a theme park for digitized human beings, a virtual complex resulting from the development of cutting-edge technologies, and a utopia where the human body and consciousness can cross physical time and space. As a new type of future living space, the development of the metaverse cannot be limited to creating a virtual space parallel to the real world. It should exist in human life like air, enabling humans to shuttle freely between the virtual and real worlds. Affective virtual reality is crucial for constructing the metaverse because it can considerably enhance an individual’s experience of bodily ownership, sense of agency, and situational awareness. In particular, an individual’s avatar in the metaverse, which is a core element of the metaverse construction, includes voice tone, facial expressions, body movements, and gestures that richly and 3-dimensionally express the individual’s emotions and create scenes and spaces for emotional twins ]. As in movies and literature, complex and emotionally rich avatar characters engage audiences more than simple and stable characters do. This appeal creates the illusion that avatars are alive and pass the Turing test, which enhances the audience’s interest and engagement in the virtual world ]. Affective virtual reality has considerable potential for applications in virtual reality socialization, virtual reality anchors, and virtual reality marketing.### LimitationsThis bibliometric analysis has several limitations that should be acknowledged. First, the basic processing unit of information in this study is the article in its entirety, and the full content of the literature has not been systematically broken down, which may result in incomplete analysis and conclusions. Second, the assumption that the articles contain information of equal quality makes it difficult to consider the objective differences in the value of the literature. In future research, a combination of bibliometrics and content analysis could be used to enhance the reliability and accuracy of the analytical results.## ConclusionAffective computing is a rapidly developing field with broad prospects. Emerging forces such as China and India are injecting strong momentum into the field. However, the field of affective computing also faces challenges and development trends in 10 aspects, including cultural background modeling, ethical and moral norms, and multimodal integration. Affective computing has great potential for application in 4 major fields and requires the joint efforts of researchers and industry practitioners. These efforts can make affective computing beneficial to the progress of human society by building a more anthropomorphic, harmonious, and natural human–computer symbiotic social form.## Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.## Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.## References1Keltner D, Sauter D, Tracy J, Cowen A. Emotional expression: Advances in basic emotion theory. _J Nonverbal Behav_. 2019; **43**(2):133–160.2Soleymani M, Garcia D, Jou B, Schuller B, Chang S-F, Pantic M. A survey of multimodal sentiment analysis. _Image Vis Comput_. 2017; **65**:3–14.3Bach DR, Dayan P. Algorithms for survival: A comparative perspective on emotions. _Nat Rev Neurosci_. 2017; **18**:311–319.4Chen L, Zhou M, Wu M, She J, Liu Z, Dong F, Hirota K. Three-layer weighted fuzzy support vector regression for emotional intention understanding in human–robot interaction. _IEEE Trans Fuzzy Syst_. 2018; **26**(5):2524–2538.5Kahneman D. _Thinking_, _fast and slow_. Macmillan, London, UK: Farrar, Straus and Giroux; 2011.6Fanselow MS. Emotion, motivation and function. _Curr Opin Behav Sci_. 2018; **19**:105–109.7Lopes PN, Salovey P, Coté S, Beers M. Emotion regulation abilities and the quality of social interaction. _Emotion_. 2005; **5**:113–118.8Suvilehto JT, Glerean E, Dunbar RIM, Hari R, Nummenmaa L. Topography of social touching depends on emotional bonds between humans. _Proc Natl Acad Sci U S A_. 2015; **112**:13811–13816.9Picard RW. _Affective computing_. Cambridge (MA): MIT Press; 1997.10Ekman P. Are there basic emotions? _Psychol Rev_. 1992; **99**(3):550–553.11Russell JA. A circumplex model of affect. _J Pers Soc Psychol_. 1980; **39**:1161–1178.12Mehrabian A. Framework for a comprehensive description and measurement of emotional states. _Genet Soc Gen Psychol Monogr_. 1995; **121**(3):339–361.13Bakker I, Van Der Voordt T, Vink P, De Boon J. Pleasure, arousal, dominance: Mehrabian and russell revisited. _Curr Psychol_. 2014; **33**:405–421.14Pozzi FA, Fersini E, Messina E, Liu B. Chapter\\\\xa01—Challenges of sentiment analysis in social networks: An overview. In: Pozzi FA, Fersini E, Messina E, Liu B, editors, _Sentiment analysis in social networks_. Boston: Morgan Kaufmann; 2017. p. 1–11.15Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. Poster presented at: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies; Portland, Oregon, USA; 2011. p. 142–150.16Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing; 2013; Seattle, WA, USA. p. 1631–1642.17Blitzer J, Dredze M, Pereira F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Poster presented at: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics; 2007; Prague, Czech Republic. p. 440–447.18Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of German emotional speech. _Interspeech_. 2005; **5**:1517–1520.19McKeown G, Valstar M, Cowie R, Pantic M, Schroder M. The SEMAINE Database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE Trans Affect Comput_. 2011; **3**(1):5–17.20Xu L, Xu M, Yang D. Chinese emotional speech database for the detection of emotion variations. _J Tsinghua Univ Nat Sci._ 2009;49(S1):1413–1418 _._21Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: From unimodal analysis to multimodal fusion. _Inf Fusion_. 2017; **37**:98–125.22Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, Gao S, Sun Y, Ge W, Zhang W, et al. A systematic review on affective computing: Emotion models, databases, and recent advances. _Inf Fusion_. 2022; **83–84**:19–52.23Zhang Z, Luo P, Loy CC, Tang X. From facial expression recognition to interpersonal relation prediction. _Int J Comput Vis_. 2018; **126**:550–569.24Mollahosseini A, Hasani B, Mahoor MH. AffectNet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Trans Affect Comput_. 2019; **10**:18–31.25Li S, Deng W, Du J. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017; . Honolulu, HI. p. 2584–2593.26Li X, Pfister T, Huang X, Zhao G, Pietikäinen M. A spontaneous micro-expression database: Inducement, collection and baseline. Paper presented at: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG); 2013; Shanghai, China. p. 1–6.27Galvão F, Alarcão SM, Fonseca MJ. Predicting exact valence and arousal values from EEG. _Sensors (Basel)_. 2021; **21**(10):3414.28Shalbaf A, Bagherzadeh S, Maghsoudi A. Transfer learning with deep convolutional neural network for automated detection of schizophrenia from EEG signals. _Phys Eng Sci Med_. 2020; **43**(4):1229–1239.29Shirahama K, Grzegorzek M. Emotion recognition based on physiological sensor data using codebook approach. In: Piętka E, Badura P, Kawa J, Wieclawek W, editors. _Information technologies in medicine_. Cham: Springer International Publishing; 2016. p. 27–39.30Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I. DEAP: A database for emotion analysis\\\\xa0using physiological signals. _IEEE Trans Affect Comput_. 2012; **3**(1):18–31.31Duan R-N, Zhu J-Y, Lu B-L. Differential entropy feature for EEG-based emotion classification. Paper presented at: 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER); 2013; San Diego, CA, USA. p. 81–84.32Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K. Introducing WESAD, a multimodal dataset for wearable stress and affect detection. Paper presented at: Proceedings of the 20th ACM International Conference on Multimodal Interaction; 2018; Boulder, CO, USA. p. 400–408.33Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. _Comput Linguist_. 2011; **37**(2):267–307.34Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. Paper presented at: Proceedings of the International Conference on Web Search and Web Data Mining—WSDM ’08; 2008; Palo Alto, CA, USA. p. 231.35Mullen T, Collier N. Sentiment analysis using support vector machines with diverse information sources. Paper presented at: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing; 2004; Barcelona, Spain. p. 412–418.36Pak A, Paroubek P. Text representation using dependency tree subgraphs for sentiment analysis. In: Xu J, Yu G, Zhou S, Unland R, editors. _Database systems for advanced applications_. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. p. 323–332.37Deng J, Ren F. A survey of textual emotion recognition and its challenges. _IEEE Trans Affect Comput_. 2023; **14**(1):49–67.38Heaton CT, Schwartz DM. Language models as emotional classifiers for textual conversation. Paper presented at: Proceedings of the 28th ACM International Conference on Multimedia; 2020; Seattle, WA, USA. p. 2918–2926.39Mao R, Liu Q, He K, Li W, Cambria E. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. _IEEE Trans Affect Comput_. 2022; **14**(3):1743–1753.40Lee CM, Narayanan SS. Toward detecting emotions in spoken dialogs. _IEEE Trans Audio Speech Lang Process_. 2005; **13**(2):293–303.41Lugger M, Yang B. The relevance of voice quality features in speaker independent emotion recognition. Paper presented at: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing—ICASSP ’07; 2007; Honolulu, HI, USA. p. IV-17–IV–20.42Likitha MS, Gupta SRR, Hasitha K, Raju AU. Speech based human emotion recognition using MFCC.Paper presented at: 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); 2017; Chennai, India. p. 2257–2260.43Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. _Speech Commun_. 2010; **52**(7–8):613–625.44Alisamir S, Ringeval F. On the evolution of speech representations for affective computing: A brief history and critical overview. _IEEE Signal Process. Mag_. 2021; **38**(6):12–21.45Stappen L, Baird A, Schumann L, Schuller B. The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements. _IEEE Trans Affect Comput_. 2023; **14**(2):1334–1350.46Huang Z, Dong M, Mao Q, Zhan Y. Speech emotion recognition using CNN. Paper presented at: Proceedings of the 22nd ACM International Conference on Multimedia; 2014; New York, NY, USA. p. 801–804.47Neumann M, Vu NT. Improving speech emotion recognition with unsupervised representation learning on unlabeled speech. Paper presented at: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2019; Brighton, UK. p. 7390–7394.48Abdelwahab M, Busso C. Domain adversarial for acoustic emotion recognition. _IEEE/ACM Trans Audio Speech Lang Process_. 2018; **26**(12):2423–2435.49Shan C, Gong S, McOwan PW. Facial expression recognition based on Local Binary Patterns: A comprehensive study. _Image Vis Comput_. 2009; **27**(6):803–816.50Chao W-L, Ding J-J, Liu J-Z. Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection. _Signal Process_. 2015; **117**:1–10.51James W. Review of la pathologie des emotions by Ch. Féré. _Philos Rev_. 1893; **2**:333–336.52Cannon WB. The James-Lange theory of emotions: A critical examination and an alternative theory. _Am J Psychol_. 1987; **100**:567–586.53Kim M-K, Kim M, Oh E, Kim S-P. A review on the computational methods for emotional state estimation from the human EEG. _Comput Math Methods Med_. 2013; **2013**: Article e573734.54Craik A, He Y, Contreras-Vidal JL. Deep learning for electroencephalogram (EEG) classification tasks: A review. _J Neural Eng_. 2019; **16**(3): Article 031001.55Maria MA, Akhand MAH, Shimamura T. Emotion recognition from EEG with normalized mutual information and convolutional neural network. Paper presented at: 2022 12th International Conference on Electrical and Computer Engineering (ICECE); 2022; Dhaka, Bangladesh. p. 372–375.56Rahman MM, Sarkar AK, Hossain MA, Hossain MS, Islam MR, Hossain MB, Quinn JMW, Moni MA. Recognition of human emotions using EEG signals: A review. _Comput Biol Med_. 2021; **136**: Article 104696.57D’mello SK, Kory J. A review and meta-analysis of multimodal affect detection systems. _ACM Comput Surv_. 2015; **47**(3):1–36.58He Z, Li Z, Yang F, Wang L, Li J, Zhou C, Pan J. Advances in multimodal emotion recognition based on brain–computer interfaces. _Brain Sci_. 2020; **10**(10):687.59Filippini C, Perpetuini D, Cardone D, Chiarelli AM, Merla A. Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review. _Appl Sci_. 2020; **10**(8):2924.60Spezialetti M, Placidi G, Rossi S. Emotion recognition for human-robot interaction: Recent advances and future perspectives. _Front Robot AI_. 2020; **7**: Article 532279.61Peng Y, Fang Y, Xie Z, Zhou G. Topic-enhanced emotional conversation generation with attention mechanism. _Knowl Based Syst_. 2019; **163**:429–437.62Dybala P, Ptaszynski M, Rzepka R, Araki K, Sayama K. Metaphor, humor and emotion processing in human-computer interaction. _Int J Comput Linguist Res_. 2013.63Goswamy T, Singh I, Barkati A, Modi A. Adapting a language model for controlled affective text generation. Paper presented at: Proceedings of the 28th International Conference on Computational Linguistics; 2020; Barcelona, Spain. p. 2787–2801.64Lei Y, Yang S, Wang X, Xie L. MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. _IEEE/ACM Trans Audio Speech Lang Process_. 2022; **30**:853–864.65Crawford K. Time to regulate AI that interprets human emotions. _Nature_. 2021; **592**(7853):167.66Ho M-T, Mantello P, Nguyen H-KT, Vuong Q-H. Affective computing scholarship and the rise of China: A view from 25 years of bibliometric data. _Humanit Soc Sci Commun_. 2021; **8**: Article 282.67Yadegaridehkordi E, Noor NFBM, Ayub MNB, Affal HB, Hussin NB. Affective computing in education: A systematic review and future research. _Comput Educ_. 2019; **142**: Article 103649.68Wu C-H, Huang Y-M, Hwang J-P. Review of affective computing in education/learning: Trends and challenges. _Br J Educ Technol_. 2016; **47**(6):1304–1323.69Liberati G, Veit R, Kim S, Birbaumer N, von Arnim C, Jenner A, Lulé D, Ludolph AC, Raffone A, Belardinelli MO, da Rocha JD, Sitaram R. Development of a binary fMRI-BCI for Alzheimer patients: A semantic conditioning paradigm using affective unconditioned stimuli. Paper presented at: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction; 2013; Geneva, Switzerland. p. 838–842.70Yuvaraj R, Murugappan M, Mohamed Ibrahim N, Iqbal Omar M, Sundaraj K, Mohamad K, Palaniappan R, Mesquita E, Satiyan M. On the analysis of EEG power, frequency and asymmetry in Parkinson’s disease during emotion processing. _Behav Brain Funct_. 2014; **10**:12.71Baki P, Kaya H, Çiftçi E, Güleç H, Salah AA. A multimodal approach for mania level prediction in bipolar disorder. _IEEE Trans Affect Comput_. 2022; **13**(4):2119–2131.72Mohammadi-Ziabari SS, Treur J. Integrative biological, cognitive and affective modeling of a drug-therapy for a post-traumatic stress disorder. In: Fagan D, Martín-Vide C, O’Neill M, Vega-Rodríguez MA, editors. _Theory and practice of natural computing_. Cham: Springer International Publishing; 2018. p. 292–304.73Tivatansakul S, Ohkura M. Healthcare system focusing on emotional aspects using augmented reality—Implementation of breathing control application in relaxation service. Paper presented at: 2013 International Conference on Biometrics and Kansei Engineering; 2013; Tokyo, Japan. p. 218–222.74Zenonos A, Khan A, Kalogridis G, Vatsikas S, Lewis T, Sooriyabandara M. HealthyOffice: Mood recognition at work using smartphones and wearable sensors. Paper presented at: 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops); 2016; Sydney, NSW, Australia. p. 1–6.75Weziak-Bialowolska D, Bialowolski P, Lee MT, Chen Y, VanderWeele TJ, McNeely E. Psychometric properties of flourishing scales from a comprehensive well-being assessment. _Front Psychol_. 2021; **12**: Article 652209.76Pei G, Xiao Q, Pan Y, Li T, Jin J. Neural evidence of face processing in social anxiety disorder: A systematic review with meta-analysis. _Neurosci Biobehav Rev_. 2023; **152**: Article 105283.77Pei G, Li T. A literature review of EEG-based affective computing in marketing. _Front Psychol_. 2021; **12**: Article 602843.78Valle-Cruz D, Fernandez-Cortez V, López-Chau A, Sandoval-Almazán R. Does twitter affect stock market decisions? Financial sentiment analysis during pandemics: A comparative study of the H1N1 and the COVID-19 periods. _Cognit Comput_. 2022; **14**(1):372–387.79Gómez LM, Cáceres MN. Applying data mining for sentiment analysis in music. In: De la Prieta F, Vale Z, Antunes L, Pinto T, Campbell AT, Julián V, Neves AJR, Moreno MN, editors. _Trends in cyber-physical multi-agent systems_. Cham: Springer International Publishing; 2018. p. 198–205.80Yu L, Zhang W, Wang J, Yu Y. SeqGAN: Sequence generative adversarial nets with policy gradient. Paper presented at: Proceedings of the AAAI Conference on Artificial Intelligence; 2017; San Francisco, CA, USA. p. 31.81Oliveira HG. A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation. Paper presented at: Proceedings of the 10th International Conference on Natural Language Generation; 2017; Santiago de Compostela, Spain. p. 11–20.82Zhang X, Lapata M. Chinese Poetry Generation with Recurrent Neural Networks. Paper presented at: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2014; Doha, Qatar. p. 670–680.83Mao G, Liu X, Du H, Zuo J, Wang L. Way forward for alternative energy research: A bibliometric analysis during 1994–2013. _Renew Sustain Energy Rev_. 2015; **48**:276–286.84Haustein S, Larivière V. The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects. In: Welpe I, Wollersheim J, Ringelhan S, Osterloh M, editors. _Incentives and performance: Governance of research organizations_. Cham: Springer International Publishing; 2014. p. 121–139.85Hammarfelt B, Rushforth AD. Indicators as judgment devices: An empirical study of citizen bibliometrics in research evaluation. _Res Eval_. 2017; **26**(3):169–180.86Wang J, Veugelers R, Stephan P. Bias against novelty in science: A cautionary tale for users of bibliometric indicators. _Res Policy_. 2017; **46**(8):1416–1436.87Van Eck NJ, Waltman L. Software survey: VOSviewer, a computer program for bibliometric mapping. _Scientometrics_. 2010; **84**(2):523–538.88Šabanović S. Robots in society, society in robots. _Int J of Soc Robotics_. 2010; **2**:439–450.89Hofstede G. _Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations_. London, UK: Sage; 2001.90Mehrabian A. _Communication without words. Communication theory. 2nd ed._ London, UK: Routledge; 2008.91Du S, Tao Y, Martinez AM. Compound facial expressions of emotion. _Proc Natl Acad Sci U S A_. 2014; 111(15):E1454–E1462.92Martinez AM. Computational models of face perception. _Curr Dir Psychol Sci_. 2017; **26**(3):263–269.93Dragano N, Lunau T. Technostress at work and mental health: Concepts and research results. _Curr Opin Psychiatry_. 2020; **33**(4):407–413.94LeDoux J. _The emotional brain: The mysterious underpinnings of emotional life_. New York, NY, USA: Simon and Schuster; 1998.95Pessoa L, Adolphs R. Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. _Nat Rev Neurosci_. 2010; **11**(11):773–782.96Price TF, Peterson CK, Harmon-Jones E. The emotive neuroscience of embodiment. _Motiv Emot_. 2012; **36**:27–37.97Cytowic RE. _Synesthesia: A union of the senses_. Cambridge, MA, USA: MIT Press; 2002.98Guerini M, Strapparava C, Stock O. CORPS: A corpus of tagged political speeches for persuasive communication processing. _J Inf Technol Politics_. 2008; **5**(1):19–32.99Damasio AR. _Descartes’ error_. New York, NY, USA: Random House; 2006.100Scheutz M. The inherent dangers of unidirectional emotional bonds between humans and social robots. In: Lin P, Abney K, Bekey GA, editors. _Robot ethics: The ethical and social implications of robotics_. Cambridge (MA): MIT Press; 2011. p. 205.101Scheutz M, Schermerhorn P. Dynamic robot autonomy: Investigating the effects of robot decision-making in a human-robot team task. Paper presented at: Under review for the 4th ACM International Conference on Human-Robot Interaction; 2009; La Jolla, CA, USA.102Gill R, Singh J. A review of neuromarketing techniques and emotion analysis classifiers for visual-emotion mining. Paper presented at: 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART); 2020; Moradabad, India. p. 103–108.103Pei G, Li B, Li T, Xu R, Dong J, Jin J. Decoding emotional valence from EEG in immersive virtual reality. Paper presented at: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC); 2022; Chiang Mai, Thailand. p. 1469–1476.104Ochs M, Sadek D, Pelachaud C. A formal model of emotions for an empathic rational dialog agent. _Auton Agent Multi-Agent Syst_. 2012; **24**:410–440.### Information#### Published In![]( ComputingVolume 3#### CopyrightCopyright © 2024 Guanxiong Pei et\\\\xa0al.Exclusive licensee Zhejiang Lab. No claim to original U.S. Government Works. Distributed under a .#### Article versions#### Submission history**Received**: 15 October 2023**Accepted**: 6 December 2023**Published online**: 5 January 2024#### Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.#### Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.### Authors#### AffiliationsGuanxiongPei Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.HaiyingLi National Science Library, Chinese Academy of Sciences, Beijing, China.YandiLu Center for Psychological Sciences, Zhejiang University, Hangzhou, China.YanleiWangDe.InnoScience, Deloitte, Shanghai, China.ShizhenHuaResearch Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.TaihaoLi  Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.#### Funding Informationthe Zhejiang Provincial Natural Science Foundation of China : LQ22C090007the National Natural Science Foundation of China : T2241018the National Science and Technology Major Project of the Ministry of Science and Technology of China : 2021ZD0114303the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2023KFKT003the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2022KFKT002#### Notes*Address correspondence to: ### Metrics#### Article UsageArticle Metrics- - No data available.05001,0001,500Jan 2024Apr 2024Jul 2024Oct 2024Jan 2025Apr 2025Jul 202520,97629- - - - Total number ofdownloads and citation**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.Citation information is sourced from  service.#### Altmetrics]( more details]( up by **10** news outlets]( by **7** X users]( **1** Facebook pages]( readers on Mendeley#### Dimensions32Total citations32Recent citationsn/aField Citation Ration/aRelative Citation Ratio]( 10 citation in Web of Science- 29 citation in Crossref#### scite![]( Citations22010Citing PublicationsSupportingMentioningContrastingSee how this article has been cited at scite shows how a scientific paper has been cited by providing the context of the citation, a classification describing whether it supports, mentions, or contrasts the cited claim, and a label indicating in which section the citation was made.### Citations#### Cite as- Guanxiong Pei,- Haiying Li,- Yandi Lu,- Yanlei Wang,- Shizhen Hua,- Taihao Li.Affective Computing: Recent Advances, Challenges, and Future Trends._Intell Comput._2024;3:0076.DOI: #### Export citationSelect the format you want to export the citation of this publication.Please select one from the listRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorksDirect import01. - Jonas Fritsch,    Sticking with Affect in HCI and Design: from Interaction to Relation, Proceedings of the sixth decennial Aarhus conference: Computing X Crisis, (202-215), (2025).     02. - Yuan Zhou,    - Siamak Khatibi,Exploring Artificial Personality Grouping Through Decision Making in Feature Spaces, AI, **6**, 8, (184), (2025). 03. - FNU Rakhi,    Modeling Students’ Emotions in Computing Education: A Context-Specific Multi-Modal Approach, Proceedings of the 2025 ACM Conference on International Computing Education Research V.2, (59-60), (2025).     04. - Jialin Sun,    Research on the Design of Intelligent Voice Interaction System Based on Affective Computing, International Scientific Technical and Economic Research, (1-15), (2025).     05. - Susanne Draheim,    - Kai von Luck,    - Larissa Putzar,Working with the Gaps in Understanding: Revisiting an Approach to Affective Computing Research in the EmotionBike Project, Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments, (596-601), (2025). 06. - Ken Nee Chee,    Affectagogy Unleashed, Pedagogical Approaches to Bridging Emotional and Cognitive Learning, (277-292), (2025).     07. - Joaquim C. Reis,    - Luzia Travado,    - Michael H. Antoni,Advancing health psychology research in oncology: Biobehavioral models, stress pathways, and stress-management interventions for cancer patients, International Journal of Clinical and Health Psychology, **25**, 3, (100615), (2025). 08. - Manuel Paniagua-Gómez,    - Manuel Fernandez-Carmona,Trends and Challenges in Real-Time Stress Detection and Modulation: The Role of the IoT and Artificial Intelligence, Electronics, **14**, 13, (2581), (2025). 09. - Thuy Thi Tran,    - Thang Van Le,    - Nam Hoang Le,    - Anh Van Thi Dam,    - Thao Thi Nguyen,    - Anh Thi Tuan Nguyen,    - Hue Thi Nguyen,Emotional attachment to artificial intelligence and perceived social isolation among university students: An application of Sternberg’s triangular theory of love, Multidisciplinary Science Journal, **7**, 12, (2025662), (2025). 10. - Jan Kincl,    - Marc T. P. Adam,    - Tanja Pavleska,The Role of\\\\xa0Human Factors in\\\\xa0Cybersecurity Incident Management, HCI for Cybersecurity, Privacy and Trust, (35-53), (2025). 11. See moreLoading...### View options#### PDF formatDownload this article as a PDF file### Share#### Copy the article linkCopy LinkCopied!Copying failed.#### Share on social media        | Now Reading:- - \")- - - - - -  xrefBack.goToxrefBack.goToRequest permissionsExpand AllCollapseExpand for moreShow all referencesSHOW ALL BOOKSCopy link✓Thanks for sharing!Find any serviceA2A![](\\'}]', name='web_search_tool', call_id='call_0QfXVfV9DNu7IVPtgrFNajy7', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 6, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_d74ZAGK7DVpc4VJFOMsgev51', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Best Vibe Coding Tools 2025: Top Picks for Better Coding\\', \\'url\\': \\'https://azumo.com/artificial-intelligence/ai-insights/best-vibe-coding-tools\\', \\'markdown\\': \\'- - - # Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunExplore the 10 best vibe coding tools for 2025, designed to make coding a more enjoyable and efficient experience. These tools enhance developer productivity with features like AI assistance, fast performance, intuitive design, and smooth collaboration. Learn what makes a tool have \"good vibes\" and why selecting the right coding environment is essential for long-term success. Whether you\\\\\\'re looking for minimal setup friction or an elegant interface, these tools will improve your coding journey. From Cursor IDE to GitHub Copilot, find the best fit for your workflow and elevate your development process.Written by:September 2, 2025![]( Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunLet\\\\\\'s be real for a second. You can absolutely build amazing software with basic text editors and command-line tools. But why would you want to? The days of suffering through clunky interfaces and frustrating workflows are long gone. Today\\\\\\'s coding tools aren\\\\\\'t just about functionality anymore; they\\\\\\'re about creating an experience that makes you actually excited to write code.Think about it this way: you probably spend 8+ hours a day staring at your development environment. Shouldn\\\\\\'t it feel good? Shouldn\\\\\\'t it make you more productive instead of fighting you every step of the way? That\\\\\\'s exactly what we\\\\\\'re talking about when we say \"vibe\" in coding tools.The numbers back this up, too. According to Stack Overflow\\\\\\'s 2024 Developer , 76% of developers are now using or planning to use AI coding assistants. The entire software development tools market is projected to hit . This isn\\\\\\'t just about new features anymore; it\\\\\\'s about developer happiness.At Azumo, we’ve worked with leading companies like Meta, Wolters Kluwer, and Wine Enthusiast to deliver cutting-edge AI-driven solutions, helping teams optimize their workflows and enhance business outcomes.Our deep expertise in  and our  give us a unique perspective on the best tools available in the coding space. We understand that the right tools can dramatically improve a developer\\\\\\'s experience and productivity, which is why we’re excited to share our insights on the best vibe coding tools for 2025.## What Makes a Coding Tool Have \"Good Vibes\"?Before we jump into the tools themselves, let\\\\\\'s talk about what we actually mean by \"good vibes\" in a development environment. It\\\\\\'s not just about pretty colors or trendy interfaces (though those don\\\\\\'t hurt).**Smooth Performance That Doesn\\\\\\'t Get in Your Way:** You know that feeling when you\\\\\\'re in the zone and your editor starts lagging? Total vibe killer. Great tools respond instantly to your input, handle large files without breaking a sweat, and never make you wait around while they \"think.\"**Intuitive Design That Feels Natural:** The best coding tools feel like they were designed by someone who actually codes. They put the most important features right where you expect them, use shortcuts that make sense, and don\\\\\\'t make you hunt through menus for basic functionality.**Smart Assistance Without Being Pushy:** Modern tools are getting really good at predicting what you want to do next. The good ones offer helpful suggestions and automate tedious tasks. The great ones do it without interrupting your flow or making you feel like the computer is trying to do your job for you.**Beautiful Aesthetics That Reduce Eye Strain:** Look, we\\\\\\'re not being shallow here. When you\\\\\\'re staring at code for hours, having clean typography, thoughtful color schemes, and well-organized layouts actually reduces cognitive load. Your brain doesn\\\\\\'t have to work as hard to parse what you\\\\\\'re looking at.**Strong Community and Ecosystem:** Tools with great vibes usually have passionate communities behind them. That means better documentation, more extensions, helpful tutorials, and people who actually want to help when you run into problems.**Flexibility to Match Your Workflow:** Everyone codes differently. Some people love keyboard shortcuts, others prefer clicking around. Some want everything in one window, others like to spread things across multiple monitors. The best tools adapt to how you work, not the other way around.**Minimal Setup Friction:** Nothing kills enthusiasm faster than spending three hours trying to configure a tool before you can write your first line of code. Great tools either work beautifully out of the box or make customization genuinely enjoyable.This combination of factors creates what developers often describe as \"flow state\"—that magical zone where you\\\\\\'re completely absorbed in solving problems and everything just works. That\\\\\\'s what we\\\\\\'re looking for in our tool selection.## Top 10 Best Vibe Coding Tools### 1. !If we\\\\\\'re talking about tools that completely change how coding feels, Cursor has to top the list. This isn\\\\\\'t just another editor with AI bolted on; it\\\\\\'s been built from the ground up around the idea of AI-human collaboration.What makes Cursor special is how naturally the AI assistance flows into your regular coding process. Instead of feeling like you\\\\\\'re constantly asking a chatbot for help, it\\\\\\'s more like having a really smart pair programming partner who knows exactly what you\\\\\\'re trying to build. Cursor IDE has emerged as particularly noteworthy, making waves as one of the best AI coding tools available due to its superior auto-completion features and integration capabilities.#### Key Features:- Context-aware code completion that actually understands your project- Natural language to code conversion that works surprisingly well- Built-in chat interface for explaining complex code sections- Multi-file editing with AI understanding the relationships between files- Beautiful, clean interface that stays out of your way**Why It Has Great Vibes:** Cursor feels like coding in the future. The AI suggestions are smart enough to be helpful but not so aggressive that they interrupt your thought process. Plus, the interface is clean and modern without being distracting.### 2. !VS Code basically won the editor wars, and for good reason. Visual Studio Code continues to dominate the IDE landscape, establishing itself as the cornerstone development environment regardless of programming language, whether JavaScript, Python, or C++.What\\\\\\'s amazing about VS Code isn\\\\\\'t just that it\\\\\\'s free and powerful; it\\\\\\'s that Microsoft somehow made it feel lightweight and fast despite having more features than most full IDEs. The extension ecosystem is incredible, which means you can customize it to feel exactly right for any type of project.#### Key Features:- Massive extension marketplace covering every language and framework- Integrated terminal that actually works well- Git integration that makes version control painless- IntelliSense that\\\\\\'s smart without being annoying- Cross-platform consistency**Why It Has Great Vibes:** VS Code strikes the perfect balance between power and simplicity. It\\\\\\'s familiar enough that you can jump right in, but deep enough that you\\\\\\'ll keep discovering new features months later. The community support means there\\\\\\'s always someone who\\\\\\'s solved whatever problem you\\\\\\'re facing.### 3. !GitHub Copilot isn\\\\\\'t an editor itself, but it transforms whatever editor you\\\\\\'re using. According to , GitHub Copilot stands as the second most popular AI coding tool, having been tried by 40% of developers and regularly used by 26% of respondents.What\\\\\\'s wild about Copilot is how it actually learns your coding style over time. It\\\\\\'s not just suggesting random code snippets—it\\\\\\'s suggesting code that looks like something you would write, just faster than you could type it.#### Key Features:- Real-time code suggestions as you type- Support for dozens of programming languages- Context awareness that spans multiple files- Integration with the most popular editors- Comment-to-code generation**Why It Has Great Vibes:** Copilot feels like having a coding buddy who\\\\\\'s really good at remembering syntax and common patterns. It handles the tedious stuff so you can focus on the interesting problems. Plus, when it suggests exactly what you were about to type, it\\\\\\'s genuinely satisfying.### 4. !Zed is the new kid on the block, but it\\\\\\'s making waves for all the right reasons. Built in Rust, it\\\\\\'s designed from the ground up for performance and collaboration. If you\\\\\\'re tired of editors that slow down as your projects get bigger, Zed might be exactly what you need.#### Key Features:- Lightning-fast startup and response times- Real-time collaborative editing- Beautiful, minimalist interface- Built-in AI assistance- Memory-efficient architecture**Why It Has Great Vibes:** Zed feels incredibly snappy and responsive. There\\\\\\'s something satisfying about an editor that never makes you wait. The collaborative features are particularly smooth—you can literally watch your teammates code in real-time without any of the lag or sync issues you get with other tools.### 5.  (via Command Line)!According to , the best AI developer tools in 2025 include Aider, Cursor, Zed, Claude Code, Windsurf, and GitHub Copilot, representing the cutting edge of AI-assisted development.Claude Code brings AI assistance directly to your terminal, which is perfect if you\\\\\\'re someone who lives in the command line. It\\\\\\'s particularly good at understanding complex codebases and making suggestions that actually make sense in context.#### Key Features:- Terminal-native AI assistance- Deep understanding of project structure- Excellent at refactoring and code optimization- Natural language query capabilities- Git integration for tracking changes**Why It Has Great Vibes:** If you\\\\\\'re a terminal person, Claude Code feels like having a really smart assistant who speaks your language. It doesn\\\\\\'t try to force you into a GUI—it works within the workflow you already love.### 6. !Windsurf represents the next generation of AI-integrated development environments. It\\\\\\'s designed around the principle that AI and human developers should work together seamlessly, not as separate entities.#### Key Features:- Deep AI integration across all features- Intelligent project understanding- Advanced debugging assistance- Collaborative AI agent capabilities- Modern, intuitive interface**Why It Has Great Vibes:** Windsurf feels like what development environments will look like in five years. The AI integration is so smooth that it doesn\\\\\\'t feel like you\\\\\\'re using an AI tool—it just feels like you\\\\\\'ve gotten really good at coding really fast.### 7. !Don\\\\\\'t sleep on Sublime Text. According to , Sublime Text maintains its reputation for exceptional performance, known for quick response times and the ability to handle large files with ease, featuring innovative multi-caret editing capabilities that allow developers to edit multiple lines simultaneously.While everyone else has been adding AI and collaborative features, Sublime Text has been perfecting the core editing experience. Sometimes you just want an editor that\\\\\\'s incredibly fast, looks beautiful, and never gets in your way.#### Key Features:- Incredibly fast performance, even with huge files- Multi-caret editing for complex refactoring- Clean, distraction-free interface- Powerful search and replace capabilities- Minimal memory usage**Why It Has Great Vibes:** Sublime Text feels like a precision instrument. Every interaction is crisp and immediate. It\\\\\\'s the editor equivalent of a really good mechanical keyboard: simple, reliable, and satisfying to use.### 8. !Aider is a command-line AI coding assistant that\\\\\\'s particularly good at making changes to existing codebases. If you work with large, complex projects and need AI help that actually understands the bigger picture, Aider is worth checking out.#### Key Features:- Git-aware AI assistance- Excellent at understanding large codebases- Command-line focused workflow- Automated testing integration- Smart refactoring capabilities**Why It Has Great Vibes:** Aider respects your existing workflow while making it significantly more powerful. It doesn\\\\\\'t try to change how you work—it just makes you better at what you\\\\\\'re already doing.### 9. !Replit has transformed from a simple online coding playground into a full-featured development environment. What makes it special is how quickly you can go from idea to running code, especially for collaborative projects.#### Key Features:- Zero-setup development environment- Real-time collaboration features- Built-in deployment and hosting- AI-powered code assistance- Educational-friendly features**Why It Has Great Vibes:** Replit removes all the friction from getting started. You can literally go from having an idea to sharing a working prototype in minutes. It\\\\\\'s particularly great for experimenting with new technologies or collaborating with people who might not have development environments set up.### 10. !Tabnine focuses specifically on AI-powered code completion, and it\\\\\\'s really good at it. What sets it apart is the focus on privacy and the ability to train on your own codebase to provide more relevant suggestions.#### Key Features:- Privacy-focused AI completion- Team training capabilities- Multi-language support- Integration with the most popular editors- On-premises deployment options**Why It Has Great Vibes:** Tabnine feels thoughtful and respectful of your work. The privacy focus means you can use it on sensitive projects, and the team training features help it get better at suggesting code that matches your team\\\\\\'s style and standards.## Productivity Enhancement Features That MatterBeyond the core editing experience, the tools that have the best vibes integrate seamlessly with the rest of your development workflow. According to , IDEs substantially boost developer productivity by consolidating all necessary coding tools into one cohesive workspace, providing access to features like built-in error detection and version control integration rather than requiring manual coding in simple text editors.**Version Control Integration:** The best coding tools make Git feel natural instead of scary. They show you what\\\\\\'s changed, make it easy to stage and commit files, and help you resolve merge conflicts without wanting to throw your computer out the window.**Project Management Connections:** According to , popular project and task management tools that enhance the coding experience include Jira, Trello, Asana, and Microsoft Planner, while continuous integration and deployment tools have become essential components of modern software development practices. When your editor can show you which issues you\\\\\\'re working on or automatically update task status based on your commits, it creates a much smoother workflow.**CI/CD Pipeline Integration:** Nothing kills momentum like having to switch between your editor and a web browser to check if your build passed. The best tools give you build status right in your editor and make it easy to see what went wrong when something breaks.**Real-Time Collaboration:** Whether you\\\\\\'re pair programming or just need to show someone a bug, tools that make collaboration feel natural rather than awkward create much better team experiences.## The Future of Vibe-Focused Development ToolsWe\\\\\\'re in the middle of a major shift in how development tools work. According to , product innovation in the software development tools market continues to expand rapidly, with nearly 57% of newly introduced tools now supporting low-code or no-code development approaches.AI integration is becoming table stakes, but the focus is shifting from \"AI that can code\" to \"AI that helps you code better.\" We\\\\\\'re seeing tools that understand not just syntax but intent, that can refactor entire codebases while maintaining your coding style, and that can explain complex code in plain English.The developer community is also driving innovation in ways we haven\\\\\\'t seen before. According to , which received over 49,000 responses from 177 countries, provides crucial insights into the evolving developer landscape. The survey focuses on 314 different technologies, with new emphasis on AI agent tools, LLMs, and community platforms, indicating where the industry is heading.Visual design and user experience are becoming just as important as functionality. Developers are no longer willing to use tools that feel clunky or outdated, even if they\\\\\\'re technically powerful.## Choose Your Coding VibeHere\\\\\\'s the thing about coding tools—the \"best\" one is the one that fits how you work and think. If you love keyboard shortcuts and never want to touch a mouse, Sublime Text or Vim-style editors might be your jam. If you\\\\\\'re working on a team and need seamless collaboration, Cursor or Zed could be game-changers. If you\\\\\\'re just getting started and want something that works well out of the box, VS Code is still hard to beat.The good news is that most of these tools are free or have generous trial periods, so you can actually try them out and see what clicks. Don\\\\\\'t just read reviews—actually use them for a few days on real projects. You\\\\\\'ll know pretty quickly which ones feel right.The software development industry continues to grow at an incredible pace, with the developer community expanding from 27 million to a projected  in the next decade. As this community grows, the demand for tools that not only function well but actually make coding enjoyable will only increase.Remember, you\\\\\\'re going to spend thousands of hours with whatever tools you choose. Pick ones that make those hours feel productive, creative, and maybe even fun. The best coding tool is the one that gets out of your way and lets you focus on building amazing things.Try a few of these tools this week. Set up Cursor for your next AI-assisted project, give Zed a spin if you want something blazingly fast, or explore how GitHub Copilot could speed up your current workflow. The future of coding is here, and it feels pretty good.**Ready to level up your development experience?** At , we understand that great software comes from developers who have access to the best tools and environments. Our nearshore development teams use cutting-edge tools and AI-powered workflows to build intelligent applications that drive real business results. Whether you\\\\\\'re looking to augment your existing team or build something completely new, we\\\\\\'d love to show you how the right combination of talent and tools can transform your development process.\\\\u200d![]( the Author:ML/AI & Backend EngineerGuillermo Germade, Data Science Expert at Azumo, specializes in building machine learning models and AI systems, focusing on consumer tech, entertainment, and big data. reCAPTCHASelect all images with **cars** Click verify once there are none left|     |     |     || --- | --- | --- || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( |Please try again.Please select all matching images.Please also check the new images.Please select around the object, or reload if there are none.Verify\\'}, {\\'title\\': \\'Top 10 Vibe Coding Tools That Feel Like Magic in 2025\\', \\'url\\': \\'https://dev.to/therealmrmumba/top-10-vibe-coding-tools-that-feel-like-magic-in-2025-1md\\', \\'markdown\\': \\'AI is no longer just assisting developers—it’s transforming how we build software. Whether you\\\\\\'re a solo indie hacker, a startup engineer, or part of an enterprise team, there’s now an AI tool that aligns with your workflow. The best ones don’t simply automate tasks; they feel intuitive, context-aware, and remarkably efficient.* * *## Why These Tools Matter in 2025Software development has evolved. We’ve moved past the era of simple code autocomplete and template generators. Today, developers demand tools that integrate deeply with their tech stack, adapt to their workflow, and help them ship faster without compromising quality.Here are three key trends that shape this new landscape:1. **Autonomous Assistance** – Developers now rely on AI tools that can complete multi-step tasks with minimal guidance, from writing modules to fixing bugs.2. **Context-Aware Coding** – Tools that understand your full codebase, API structure, and data flow are no longer a luxury—they’re a necessity.3. **Real-Time Collaboration** – AI is becoming a collaborative partner in the development process, offering suggestions, generating code, and even reviewing pull requests in real-time.]( list focuses on tools that exemplify these traits and offer genuine improvements to the way developers work in 2025.* * *## The Top 10 Vibe Coding Tools in 2025### 1. Apidog MCP Server**The missing link between your API and AI coding assistants** bridges the gap between your backend and tools like Cursor, Cline, and Copilot Workspace. Once your OpenAPI definitions are connected, you can:- Autogenerate API logic and DTOs based on updated schema- Let AI tools access real-time API documentation for better suggestions- Streamline backend/frontend development across large teams]( especially powerful in teams that maintain frequently changing APIs or use domain-driven design.* * *### 2. Cursor]( code editor designed for pair programming with GPT-4** is built on VS Code but reimagines the editing experience with AI at the center. Unlike plugins, Cursor deeply integrates AI into every layer:- Offers context-aware autocomplete that understands your full project- Supports inline debugging and refactoring with natural language prompts- Can explain, rewrite, or extend code based on your cursor positionIt’s ideal for fast iteration, onboarding into unfamiliar codebases, or accelerating solo projects.* * *### 3. Copilot Workspace]( task-oriented Copilot that tackles full development cycles**Copilot Workspace is a major leap from GitHub Copilot. Rather than suggesting code line by line, it can:- Understand a GitHub issue and break it into subtasks- Write full PRs for those subtasks- Summarize what it did and wait for your feedbackThis makes it perfect for junior developers, product engineers, or teams with many open tickets to manage.* * *### 4. Sweep]( that turns GitHub issues into functioning pull requests** is a bot that integrates directly into your GitHub repo. You describe an issue, and it:- Scans the entire repo to understand the current architecture- Writes a PR that resolves the issue- Updates associated files and testsFor lean teams or solo founders managing multiple products, Sweep can save hours of context-switching.* * *### 5.Fine by Fine.dev]( is the AI assistant that startup CTOs have been dreaming of. This AI-powered development platform automates tasks throughout the software development lifecycle, from coding and debugging to testing and code review.What makes Fine special is its ability to work autonomously—it can create and implement code, conduct pull request reviews, generate tests, and handle common issues without constant human input. The GitHub integration and support for asynchronous work make it particularly suitable for distributed teams.* * *### 6. Continue]( with your codebase in any editor** is an open-source extension that works in VS Code and JetBrains. It allows you to:- Chat with your entire codebase- Ask for refactors, explanations, or generation tasks- Use local models or connect with remote LLMs like Claude, GPT-4, or MistralIt’s the go-to for developers who want full AI capabilities without being locked into a closed ecosystem.* * *### 7. The Windsurf Editor by Codeium]( to Keep you in flow state** is revolutionizing the IDE concept by seamlessly integrating AI as a collaborator rather than just a tool. The platform offers a unique blend of AI copilots and autonomous agents that work together with developers to enhance productivity.The Cascade feature, which provides deep contextual awareness across your codebase, has helped me understand and navigate unfamiliar codebases much faster than traditional methods. The multi-file editing and predictive actions keep me in a constant flow state, which has noticeably improved my productivity.* * *### 8. Lazy AI]( app creation with AI-powered workflows** makes building applications almost effortless. Designed for speed and simplicity, it lets users create full-stack apps without writing code. Its standout features include:- Pre-built workflows for common developer and automation tasks- One-click deployment for both frontend and backend projects- Customizable templates for bots, AI tools, dev utilities, and marketing apps- Minimal skill requirement—ideal for rapid prototyping or solo makersIt’s the perfect fit for anyone looking to bring ideas to life without the overhead of traditional development.* * *### 9. Devika**Your open-source AI software engineer** is an open-source autonomous AI agent that takes high-level instructions and breaks them down into actionable development tasks. Designed to behave like a junior developer, it combines:- Advanced planning and reasoning algorithms- Contextual keyword extraction for precision- Dynamic agent tracking and decision-making- Web browsing to supplement gaps in knowledgeDevika stands out as a community-driven alternative to commercial AI coding platforms. It’s ideal for developers looking to delegate complex coding work while retaining full control and transparency over the process.* * *### 10. Cody by Sourcegraph]( code understanding with full repo context** is designed to:- Answer questions across massive codebases- Perform intelligent refactors based on architecture- Search, navigate, and explain with precisionIts integration with Sourcegraph gives it superpowers in monorepos, enterprise codebases, and multi-language projects.### AI Coding Tools in Practice: Use Case ScenariosTo better understand how these AI coding tools can be applied in real-world settings, here are a few practical scenarios that demonstrate their potential across different developer environments:#### **Solo Developer: Boosting Productivity with Cursor + Continue**Independent developers often juggle multiple roles—from writing code to debugging and managing small deployments. Tools like **Cursor** (an AI-native code editor) combined with **Continue** (a Copilot-like extension that integrates into IDEs) offer a significant productivity boost. Together, they streamline code suggestions, automate refactoring, and provide quick answers without leaving the editor, allowing solo devs to maintain momentum and ship faster.#### **Startup Team: Automating Ticket Resolutions with Sweep + Copilot Workspace**Early-stage startups typically operate with lean engineering teams that need to move fast. Tools like **Sweep**, which auto-generates pull requests from GitHub issues, and **Copilot Workspace**, which can create and manage entire features autonomously, allow small teams to maintain high output. These tools handle boilerplate tasks and bug fixes, freeing up developers to focus on high-impact product features.#### **Enterprise: Using Cody + Tabnine for Large, Secure Codebases**For enterprises with massive codebases and stricter security requirements, **Cody** (from Sourcegraph) and **Tabnine** offer scalable AI integration. Cody excels at understanding large repositories and referencing documentation or historical code changes. Tabnine, known for its privacy-first model, ensures that proprietary code doesn’t leave company boundaries, making it ideal for secure enterprise environments.#### **Data Scientist: Speeding Up EDA with Cogram + CodeSquire**Data scientists benefit greatly from tools that can reduce time spent on repetitive analysis. **Cogram** helps generate SQL queries and Jupyter code based on natural language, while **CodeSquire** personalizes code suggestions based on the user’s unique style and workflows. These tools accelerate exploratory data analysis (EDA) and modeling, letting scientists focus more on insights and less on syntax.* * *## Final Thoughts2025 isn’t about whether you use AI as a developer—it’s about how. The tools listed above aren’t just trendy—they’re reshaping the development experience by embedding intelligence into your daily workflow. Some extend your capabilities. Others remove tedious work. The best ones do both.Choose the ones that match your stack, integrate with your workflow, and make you feel like a better developer—not just a faster one.* * *Sentry]( PromotedDropdown menu- - * * *- ]( Read More!Templates let you quickly answer FAQs or store snippets for re-use.SubmitPreview CollapseExpand]( David]( David]( David![]( of Postiz, an open-source social media scheduling tool.Running Gitroom, the best place to learn how to grow open-source tools.- EducationDidn\\\\\\'t finish high school :(- PronounsNev/Nevo- WorkOSS Chief @ Gitroom- JoinedFeb 23, 2022• Dropdown menu- - Hide- Neat how coding is getting easier, like having a little helper doing chores for you. But if AI keeps taking on bigger parts of the job, what skills should people still work on to stay sharp in tech?CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Totally! As AI handles more, staying sharp in problem-solving, system design, and critical thinking is key. Knowing why things work still sets you apart.CollapseExpand]( Sečkár]( Sečkár]( Sečkár![]( am from Czechia, born in 1988. Studied computer science at KIT VŠE, Prague. Currently working as senior Java & JS developer for Eviden.- Email- LocationPrague, Czechia- EducationKIT VŠE, Prague- PronounsHe- WorkSenior application developer @ Eviden- JoinedOct 21, 2023• Dropdown menu- - Hide- business analysis, software architecture, observability, scalability, security...to name a fewCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- And to continue on the reply of , convert abstract requirements to good software requirements is still key, AI can only hallucinate solutions that most likely be wrong or wrongly translated.Software architecture and applying best practices is also still important, AI only generate some code which might not scale very well. Or doesn\\\\\\'t fit the overal design, or worse it\\\\\\'s using deprecated functionality. So a human developer should be really keen on it and not just accept all the code. How well it even might look like. It\\\\\\'s still just a tool.CollapseExpand]( Lopez]( LopezAudrey Lopez]( JoinedMar 6, 2025• Dropdown menu- - Hide- Nice post, Emmanuel Mumba!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Thanks Lopez! Glad you enjoyed it.CollapseExpand]( Sebastian]( SebastianRalph Sebastian]( JoinedAug 10, 2023• Dropdown menu- - Hide- Nice list. By the way, Windsurf has free GPT 4.1 access for now (it will not last long), worth trying out.CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Oh nice, thanks for the heads-up! I’ll definitely check Windsurf out before the window closes.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Openai is indeed delivering free gpt 4.1 while in preview.But also o4-mini models I believe. Mini-high, Etc.CollapseExpand]( Lee]( LeeJennie Lee]( JoinedDec 18, 2024• Dropdown menu- - Hide- OK, vibe coding is getting hot this time!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Haha, for real! 🔥 Coding’s definitely having its moment, More like the vibe is leveling up every week.CollapseExpand]( the Coder]( the CoderJosh the Coder]( Coder that is Interested in AI, No Code. Learning forever!- JoinedNov 27, 2023• Dropdown menu- - Hide- OK, this is a good list. I am using Cursor but also willing to explore some alternatives. Good work Emmanuel!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Appreciate that! 🙌 Cursor’s solid, always cool to explore what else is out there too. Let me know if you find any gems.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Windsurf is really good. Since they at least providea decent deb file setup, causing less issues on my linux setup than the stupid appimages from cursor.CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- I\\\\\\'ve spent more time trying to fix our juniors vibe coded problems than I can imagine. And what\\\\\\'s worse is having to explain why. They have no idea what the code they are pumping out does. It\\\\\\'s a horrifying trend. Hopefully this doesn\\\\\\'t spread to all industries and we end up with vibe built airplanes LOLCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Haha I was already afraid this will happen 2 years ago. And now we are at this stage where agents are commonly available and used in editors.It\\\\\\'s powerful for experienced devs. But juniors should really not just using it and blindly accept it. That is not how you grow in your knowledge or carrière. Instead of you fixing their issues, they should take full responsibility of the code they put out for review.CollapseExpand]( Orion]( OrionAshe Orion]( JoinedAug 12, 2024• Dropdown menu- - Hide- Okay so as a developer in the making, at what point in my learning process will these tools stop being a hindrance to my education and start being actual real usable tools and not like Nintendo 64 cheat codes because right now, having nothing but HTML CSS and some very intermediate vanilla JavaScript at my disposal (and only as it relates to the front-end. Getting at back end soon) even just the Codium Auto-complete in VS was detracting from my ability to build a solid foundation by typing through my full lines of code. I noticed that when I had auto complete enabled, pretty much immediately anytime I was working in other environments without an autocomplete buddy by my side my syntax absolutely suffered because my monkey brain wasn\\\\\\'t getting the muscle memory of typing out the proper syntax and to be honest it was a little embarrassing in Zoom meetings with my tutors to just sit there and fumble multiple times on things like spacing or back ticks or, dare I out myself, neglected line termination in ; and }However I definitely see the value of not having to type out very obvious return statements or whatever other repetitive this.thing = thething; -esque scenarios that when you add them all up really do burn giant holes your productive work hours. I\\\\\\'m just so terrified of using it too early, leaving the safety of being an \\\\\\'aspiring engineer\\\\\\' for the reality of the job market, finally landing an interview only to realize that while I thought I was learning to code I\\\\\\'d just been surfing the vibe.CollapseExpand]( Enlightened]( EnlightenedAaron Enlightened]( Developer using AI tools for programming but not depending on them completely!- JoinedApr 18, 2025• Dropdown menu- - Hide- LOL ... I know exactly how you feel! I am at a similar junction in my programming journey and this is what I do to not feel like an imposter later on - Open a plain .txt file and just type some HTML and CSS code (along with vanilla JavaScript if you want to) into it, then save the file as \"index.html\", and then open this file in Google Chrome browser and see the result. To modify or edit any code that you feel is giving unexpected results, just use the Chrome Developer tools. This will give you confidence in YOUR coding prowess with ZERO autocomplete using any type of AI. Do this for just 15-20 minutes every day for 1 month and you will have solved the imposter syndrome to a large extent.For all the rest of your tutorials use the autocomplete feature on VSCode Editor, but try to ask ChatGPT \"why\" was some particular code snippet suggested by the AI autocomplete. This will help you in understanding the WHY of AI coding and will help make you a better programmer.BTW, I signed up on DEV just to post this comment. So, thank you for that. ;-)CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- It\\\\\\'s a tough gig for new devs, you are entering a world where tools exist that actually hinder your understanding of what\\\\\\'s going on.At the risk of showing my age, id say it\\\\\\'s akin to why the monk will make the initiate carry pales of water to then monastery before he learns the king fu. To generate an understanding of the fundamentals of movement, patience and discipline.Some people a lot older than me would say using an IDE is not true development and that real devs can code in notepad.Tools evolve and as long as they fill the purpose of making coding easier, but still allow you to learn and gain the knowledge and understanding I wouldn\\\\\\'t see a problem with using them. But as you pointed out, it can be a hindrance, and you actually realizing that and accepting it means you will succeed where many of your peers won\\\\\\'t.AI is just another tool, it\\\\\\'s great when you already have an understanding of how systems should work and just want some snippets. But the problem we are seeing is a misuse of these tools. The blind copy paste vibe coding is not creating developers it\\\\\\'s creating slop.Here\\\\\\'s my ultimate advice though, AI doesn\\\\\\'t have to be just a tool, it can be an amazing teacher. It can explain things to you in anyway you want , if you ask it to simplify and breakdown topics it will. It can explain things to you as if you were a toddler. USE THIS.Vibe code your heart out but every single time it gives you code you do not understand, ask it to explain it you. You will be amazed how quickly it goes from hindrance to game changing.CollapseExpand]( jones]( jonessam jones]( JoinedApr 16, 2025• Dropdown menu- - Hide- Very informative article. Although not at all what I expected from title \"Vibe Coding\". In industry, Vibe Coding is being used to signify low code/no code or citizen development. Basically, the same holy grail that the industry has been searching for since Oracle Forms or earlier.BUt putting the Title aside, looking forward to trying out some of your suggestions.CollapseExpand]( Mottley]( MottleyDaley Mottley]( Consultant | Full Stack Web Developer- Email- LocationBarbados- JoinedOct 8, 2024• Dropdown menu- - Hide- Nice post, Emmanuel!Have you tried ?It\\\\\\'s a pretty decent vibe coding tool. I love it... 😅Thanks for sharing 👍CollapseExpand]( Green]( GreenStuart Green]( JoinedApr 18, 2025• Dropdown menu- - Hide- Crazy times ahead. Can only imagine the downstream impact these tools will have (personalized software, rise of individual creators, etc).I also love platforms like  and  for \"vibe coding\" without the actual code. Though harder to build more advanced features than with Cursor, etc.Some comments may only be visible to logged-in visitors.  to view all comments.Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\\\\\'s .Hide child comments as wellConfirmFor further actions, you may consider blocking this person and/or Embeddable]( PromotedDropdown menu- - * * *- ]( Embed in minutes, load in milliseconds, extend infinitely. Import any chart, connect to any database, embed anywhere. Scale elegantly, monitor effortlessly, CI/CD & version control.👋 Kindness is contagiousDropdown menu- - * * *- CloseDive into this thoughtful piece, beloved in the supportive DEV Community. **Coders of every background** are invited to share and elevate our collective know-how.A sincere \"thank you\" can brighten someone\\\\\\'s day—leave your appreciation below!On DEV, **sharing knowledge smooths our journey** and tightens our community bonds. Enjoyed this? A quick thank you to the author is hugely appreciated.### !We\\\\\\'re a place where coders share, stay up-to-date and grow their careers. ![](\\'}]', name='web_search_tool', call_id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 8, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_S9VoFTZt8NefRPNRbUH4tZQg', is_error=True)]\n",
      "---------- ToolCallSummaryMessage (research_agent) ----------\n",
      "Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 7, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "[{'title': 'Affective Software Engineering', 'url': 'https://affectivese.org/', 'markdown': \"!We are a group of computer scientists whose research focuses on using Affective Computing methods in the field of Software Engineering. Our interests include, but are not limited to, the role of emotions in the software development process, reusing datasets from Affective Computing experiments as well as improving the teaching process of future software engineers.We are open to any kind of cooperation. Do you want to do research together? Prepare a paper? Submit a project proposal? If you are conducting research at the intersection of the fields of affective computing and software engineering feel free to contact us via . Take a look at our current research and projects and get in touch!### DevEmo datasetThe DevEmo dataset consists of 217 video clips showing students' facial expressions as they solve programming tasks. The recordings were collected in the participants' actual work environment. The video clips are annotated with five labels, including four emotions (anger, confusion, happiness and surprise) and a neutral state.In order to facilitate the annotation of a large number of video recordings, we have developed AffecTube. It is a comprehensive solution that leverages crowdsourcing to annotate videos directly on the YouTube platform, resulting in ready-to-use emotion-annotated datasets. AffecTube provides a low-resource environment with an intuitive interface and customizable options, making it a versatile tool applicable not only to emotion annotation, but also to various video-based behavioral annotation processes.If you are interested in using the DevEmo dataset or AffecTube check out the projects' websites:Journal papers:### Affective data reausabilityModern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration.In the research conducted so far, we have proposed the ROAD (Recording Ontology for Affective-related Datasets) ontology and its implementation with the graph knowledge base in the form of the GRISERA framework.If you are interested in our approach take a look at:Journal papers:### Emotions in the software development processEmotions play an important role in any human being's life, including software developers. The aim of our research is to determine what emotions are optimal in the software development process, how to recognize them and how (if at all) to influence them.Publications:### Affect-aware software engineering educationWith the Covid-19, the role of distance learning has increased. The aim of the research is to develop methods that, using emotion recognition techniques, will allow for more effective remote learning of programming.Publications:### Affective loop in Socially Assistive Robotics as an intervention tool for children with autismEMBOA project entitled “Affective loop in Socially Assistive Robotics as an intervention tool for children with autism” is a research and didactic project that aims at enhancement of social robot intervention in children with autism with affective computing technologies. The project is implemented in years 2019-2022 under EU Erasmus Plus Strategic Partnership for Higher Education Programme.More information on project website: Wrobel MR., Landowska A., Makuch K., , Proceedings of the 32nd International Conference on Information Systems Development (ISD 2024), 2024Barkana DE., Bartl-Pokorny KD., Kose H, Landowska A., Milling M., Robins B., Schuller BW., Uluer P., Wrobel MR., Zorcec T., , International Journal of Social Robotics, 2024Kulas D., Wrobel MR., , SoftwareX, 2023Manikowska M., Sadowski D., Sowinski A., Wrobel MR, , Applied Sciences, 2023Zawadzka T, Wierciński T, Waloszek W, Wróbel MR, , IEEE Access, 2023Zawadzka T, Wierciński T, Meller G, Rock M, Zwierzycki R, Wróbel MR, , Sensors, 2021Zawadzka T, Waloszek W, Karpus A, Zapałowska S, Wróbel MR, , IEEE Access , 2021Bartl-Pokorny KD, Pykala M, Uluer P, Barkana DE, Baird A, Kose H, Zorcec T, Robins B, Schuller BW, Landowska A, , IEEE Access , 2021Wróbel MR, , IEEE Access , 2020Blincoe K, Springer O, Wrobel MR, , IEEE Software, 2019Wrobel MR, Zielke AW, , 2018 Federated Conference on Computer Science and Information Systems (FedCSIS), 2018Wrobel MR, , Applied Sciences, 2018Landowska A, Brodny G, , IEEE Access, 2018Landowska A , , Applied Sciences , 2018Wrobel MR, , 2016 Federated Conference on Computer Science and Information Systems (FedCSIS), 2016Brodny G, Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 2016 9th International Conference on Human System Interactions (HSI), 2016Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Information Systems Development and Applications, 2015Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Human-Computer Systems Interaction: Backgrounds and Applications 3, 2014Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013\"}, {'title': 'Affective Computing: Recent Advances, Challenges, and Future ...', 'url': 'https://spj.science.org/doi/10.34133/icomputing.0076', 'markdown': 'AdvertisementMain content starts hereContents## AbstractAffective computing is a rapidly growing multidisciplinary field that encompasses computer science, engineering, psychology, neuroscience, and other related disciplines. Although the literature in this field has progressively grown and matured, the lack of a comprehensive bibliometric analysis limits the overall understanding of the theory, technical methods, and applications of affective computing. This review presents a quantitative analysis of 33,448 articles published in the period from 1997 to 2023, identifying challenges, calling attention to 10 technology trends, and outlining a blueprint for future applications. The findings reveal that the emerging forces represented by China and India are transforming the global research landscape in affective computing, injecting transformative power and fostering extensive collaborations, while emphasizing the need for more consensus regarding standard setting and ethical norms. The 5 core research themes identified via cluster analysis not only represent key areas of international interest but also indicate new research frontiers. Important trends in affective computing include the establishment of large-scale datasets, the use of both data and knowledge to drive innovation, fine-grained sentiment classification, and multimodal fusion, among others. Amid rapid iteration and technology upgrades, affective computing has great application prospects in fields such as brain–computer interfaces, empathic human–computer dialogue, assisted decision-making, and virtual reality.#### SIGN UP FOR _Intelligent Computing_ eTOC ALERTS!Get the latest SPJ table of contents delivered right to you!GET OUR E-ALERTS## IntroductionAccording to basic emotion theory, emotion is the grammar of social living and serves as a crucial means of exchanging information, maintaining relationships, and communicating ideas between individuals. Moreover, it is a fundamental psychological element that ensures basic human survival while shaping social habits and supporting advanced thinking , ]. Given its central role in numerous human intellectual activities such as perception, learning, decision-making, reasoning, and socializing, emotion is an important force driving the continuous and diverse prosperity of human civilization.The importance of emotions to human beings can be summarized in 5 crucial aspects. First, the survival function is a learned physiological response that allows individuals to adapt positively to their environment ]. Emotions play a pivotal role in strengthening the capacity to adapt to the environment by regulating attention, memory, perception, and other cognitive processes. This ensures a greater chance of survival and development during the evolutionary process. Second, the communication function highlights the importance of emotions for the accurate expression and understanding of human intentions ]. The same words spoken with different emotions carry different connotations. Thus, emotions are inseparable from natural language and are critical for semantic disambiguation. Third, emotions have a decision-making function that manifests in both fast and slow modes of thinking. The commonly used unconscious “System 1” mainly relies on emotions and experiences, while the conscious “System 2” depends on rational deliberation ]. Therefore, emotions are widely involved in higher-level thinking and decision-making processes that profoundly affect the results and efficiency of decisions. Fourth, emotions serve a motivational function in stimulating and sustaining individuals’ behaviors, thereby affecting the degree of resource input, behavioral persistence, and evaluation of outcomes ]. Finally, emotions perform a maintenance function as bonds between members of ethnic groups, families, social circles, social classes, and other groups. During human socialization, emotions serve as the core of low-cost maintenance of social relations, forming potential social interaction contracts, and are closely tied to individual moral constraints and codes of conduct , ]. Hence, the nature and functions of emotions ensure that they are inseparable from human survival and development.As the era of a human–machine symbiotic society approaches, endowing machines with emotional intelligence becomes increasingly crucial. Emotional intelligence represents a fundamental technology and an essential prerequisite for realizing naturalized and anthropomorphic human–computer interaction. It is of great value for opening up the era of intelligence and digitization. Picard is credited with being the first to propose a comprehensive definition of affective computing. In her 1997 book, _Affective computing_, she defined it as “computing that relates to, arises from, or deliberately influences emotions” ]. The goal of affective computing is to create a computing system capable of perceiving, recognizing, and understanding human emotions and responding intelligently, sensitively, and naturally, thus making human–computer interaction more natural. The epochal importance of affective computing lies in its impact on changing how emotions are perceived as abstractions within psychology, making it possible for emotions to be measured, computed, and machine-learned.Affective computing encompasses various disciplines, including computer science, engineering science, brain and psychological science, and social sciences. Computer science and engineering science focus on providing various information technology tools and engineering capabilities to enable digital reconstruction and computational realization of emotion perception, recognition, understanding, and feedback, allowing machines to possess human-like emotional and cognitive functions. The psychological and consciousness aspects of the brain and psychological sciences provide theories on the basic definition of human emotions and the structure of related elements, laying the foundation for modeling emotion theories. Cognitive neuroscience, another branch of the brain and psychological sciences, examines the emotion-processing mechanism of the human brain and establishes a functional network of psychological elements associated with emotions, providing important inspiration and strategic guidance for developing affective computing models. Social and medical sciences offer numerous opportunities for the application of affective computing and serve as a resource for designing application scenarios for such technologies.### Research in affective computingThe research content of affective computing primarily covers 5 aspects. The first aspect is the fundamental theory of emotion, which currently relies on the discrete emotion model and the dimensional emotion model from the field of psychology to define various types of emotions, ranging from basic to compound. The second aspect involves collecting emotional signals, such as text, speech, facial expressions, gestures, and physiological signals, to establish corresponding datasets. The third aspect is sentiment analysis, which utilizes machine-learning and deep-learning algorithms to model and identify emotional signals. The fourth aspect is multimodal fusion, which leverages multimodal emotional features and fusion algorithms to enhance the accuracy of emotional classification. Finally, the fifth aspect is generating and expressing emotions, processes that enable robots to express emotional states through facial expressions, voice intonation, body movements, etc., and facilitates natural, anthropomorphic, and personified human–robot interaction. Figure  illustrates the specific content and development status of these 5 aspects.![](\\xa01. Research content of affective computing.![](\\xa02. Annual scientific production on “affective computing” from 1997 January 1 to 2023 September 25.#### Basic theory of emotionThe field of affective psychology has numerous grounded theories of emotion and serves as an important source of inspiration for the development of computable emotion models. The discrete emotion model and the dimensional emotion model are the most commonly used theoretical models for artificial intelligence emotion modeling. The discrete emotion model categorizes emotions individually rather than in correlated groups, as does Ekman’s basic emotion classification model, which is based on facial expression analysis ] and comprises happiness, sadness, anger, disgust, surprise, fear, and contempt. Although the discrete emotion model is clearly defined, interpretable, easy to understand, and capable of semantically integrating vocabulary and concepts, it lacks granularity and provides a limited quantitative description of emotions. In contrast, dimensional affective models represent different emotions through multidimensional vectors in affective space. Such models include the valence–arousal affective model ] and the 3-dimensional pleasure–arousal–dominance model , ]. These models are highly quantitative, abstract, and inductive and have continuous emotional value vectors. They are suitable for handling changes in emotional states over time but are not intuitively interpretable; thus, it is difficult for machines to use them to develop rich coping strategies for emotional interactions. The selection of the model depends on the actual application tasks and scene requirements, as both discrete and dimensional emotion models have advantages and disadvantages.#### Collection of emotional signalsTo support data acquisition and the comparison of algorithms in affective computing, numerous open-source databases have been established. They contain datasets that can be categorized as textual, speech/audio, visual, physiological, or multimodal. The characteristics of these databases considerably impact model design and network architecture in affective computing.Text-based resources on various communication carriers serve as massive datasets for emotional text mining ]. Representative datasets include the internet movie database (IMDb) ], the Stanford sentiment treebank, which contains sentences from movie reviews ], and the Multi-Domain Sentiment Dataset, which contains Amazon.com product reviews ]. Speech is another crucial modality for decoding emotions in human intercommunication. Speech signals comprise both the emotional content of the speech and the emotional characteristics of the sound itself. Representative datasets include EmoDB ], the SEMAINE database ], and CSED ]. Visual-emotional signals such as body movements and facial expressions are now more convenient to gather because of low-cost sensors such as cameras and camcorders, and they do not require direct contact with the user ]. This field has vast amounts of data and many related research papers with considerable data collected directly from real-world scenarios, making it more conducive to grounded applications ]. Representative datasets include the Expression-in-the-Wild (ExpW) dataset ], AffectNet ], the Real-world Affective Faces Database (RAF-DB) ], and SMIC, a database of spontaneous microexpressions ].Physiological data have an advantage over signal data such as text, speech, and facial expressions in that they can more directly, objectively, and accurately reflect an individual’s emotional state while being less influenced by subjective consciousness , ]. Consequently, physiological data have become a research hotspot in affective computing. Commonly used physiological data in this field include electroencephalograms (EEGs), skin electricity, cardiac electricity, electromyography (EMG), eye electricity, respiration, skin temperature, and blood volume pulse. However, obtaining physiological data requires the use of complex sensors. Thus, such data are expensive and challenging to collect for use in practical applications. Consequently, the scale of physiological data used in laboratory research is generally small ]. Representative datasets include the Database for Emotion Analysis using Physiological Signals (DEAP) ], the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) ], and WESAD, a dataset for wearable stress and affect detection ].#### Sentiment analysis**Text analysis.** This method focuses on extracting, analyzing, understanding, and generating emotional information in natural language. Early text affective recognition relied mainly on manually constructed affective dictionaries and rules for affective analysis. These methods judge sentiment polarity by matching sentiment words with grammatical rules in a text , ]. However, this approach is limited by emotional lexicon coverage and rules, making it challenging to support multidomain sentiment analysis. With the advancement of machine learning, text emotion recognition methods based on statistical and machine learning algorithms have emerged. By training on large-scale text datasets, machine learning models can automatically learn emotional expression and semantic features, enhancing the accuracy and generalization ability of sentiment classification , ]. In recent years, deep-learning technology has considerably impacted text emotion recognition. Neural network-based models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, bidirectional encoder representation from transformers (BERT), and generative pre-trained transformers (GPT), have been successful in various sentiment analysis tasks – ]. They can capture contextual information and semantic relationships to better understand and analyze sentiments.**Speech analysis.** Speech emotion recognition is the process by which a computer automatically recognizes the emotional state signaled by speech. Speech contains emotional information, such as speech rate and intonation, in addition to semantic information. Speech emotion analysis combines linguistic and acoustics-related technologies to analyze the syntax, semantics, and acoustic feature information related to the speaker’s emotional state ]. This analysis mainly revolves around rhyme, spectrum, and sound quality features. The numerous acoustic features related to affective states include fundamental frequency, duration, speech rate, resonance peaks, pitch, mel-filter bank (MFB), log-frequency power coefficients (LFPC), linear predictive cepstral coefficients (LPCC), and mel-frequency cepstral coefficients (MFCC) – ]. These features are represented as fixed dimensional feature vectors, with each component representing the statistical value of each acoustic parameter, including the mean, variance, maximum or minimum value, and range of variation. Recently, the ability of neural networks to extract suitable feature parameters has received increasing attention. Deep speech emotion features are learned from speech signals or spectrograms through tasks related to speech emotion recognition. Deep speech features learned from large-scale training data are widely used as speech emotion features in speech event detection and speech emotion recognition tasks, as in the VGGish and wav2vec projects , ], for example. In recent years, algorithms such as ConvNet learning ], ConvNet-RNN ], and adversarial learning ] have considerably improved speech emotion recognition performance.**Visual analysis.** Visual emotion recognition research primarily focuses on facial expression recognition (FER) and emotional body gesture recognition. The conventional method involves feature extraction followed by classification. Typically, handcrafted features for static image analysis include local binary pattern (LBP), histogram of oriented gradients (HOG), local phase quantization (LPQ), and Gabor features , ]. Some scholars have proposed dynamic feature extraction methods, such as LBP on three orthogonal planes (LBP-TOP) ]. Features are usually classified using pattern recognition classification methods such as K-nearest neighbors, support vector machines (SVMs), or multi-layer perceptrons (MLPs). Another approach is the feature learning approach, which combines the end-to-end training of feature representations and classifiers on a given task target, typically a combination of the entire connection layer and softmax. The feature-learning method employs features learned from big data through layer-by-layer feature transformation and can describe the intrinsic information of data better than handcrafted features. However, supervised training methods such as deep CNNs are not universal and rely on large amounts of sample data. Therefore, it is too early to abandon traditional feature-extraction methods. In visual emotion analysis, automatic training features can be extracted and integrated with traditional features, which may further improve system performance.**Physiological signal analysis.** Physiological changes that occur with emotions, including brain electrical activity, heart rate changes, electrical skin response, muscle tension, and respiration rate, are supported by mainstream theories, such as the physiological theory of emotion ] and Lange’s theory of emotion ]. By detecting changes in these physiological signals, patterns associated with emotions can be recognized and then used to develop computer systems that can automatically recognize emotions. Physiological signals are more challenging to recognize than text, speech, and facial expression signals mentioned above, and they have unique properties. For example, computing EEG data requires more complex preprocessing, including electrode position localization, bandpass filtering, reference conversion, segment analysis interception, artifact removal, and bad electrode interpolation. Researchers must have cross-field knowledge to apply machine learning or deep learning methods to recognize emotions from physiological signals ].Affective computing mainly employs peripheral nervous system (PNS) features, such as facial EMG, galvanic skin potential (GSP), photoplethysmography (PPG), heart rate variability (HRV), respiratory rate, and electrocardiogram (ECG), whereas central nervous system (CNS) features include EEG, near-infrared, and brain-imaging features. EEG features have dominated the studies published on this topic. For instance, manual feature extraction involves multidimensional feature extraction from EEG signals in the time, frequency, time–frequency, and nonlinear domains for emotion recognition and classification. Recent studies have emphasized the integrity and relevance of these features. To construct functional brain networks, many studies have started defining a channel as a node and quantifying the relationship between individual nodes using phase synchronization, inter-correlation, and mutual information, treating strength as the functional connectivity between the brain regions of the corresponding channel. Complex network measures, including efficiency, clustering coefficients, degree distribution, small-world features, and average shortest distance, are then used to extract functional brain network features. Since 2018, deep learning methods such as CNNs, RNNs, deep belief networks (DBNs), and stacked autoencoders (SAEs) – ] are being increasingly used for emotional computation of EEG data, generalizing sentiment analysis to various physiological signals.#### Multimodal fusionEarly affective computing primarily involved unimodal data analysis and emotion recognition, focusing on a single modality, such as text, speech, facial expression, body movement, or physiological signals. However, this approach fails to conform to the human perception and expression patterns of emotions and has limitations in terms of the information obtained for emotion recognition ]. Humans communicate their emotions through multiple channels, including language, tone of voice, facial expressions, and body movements. Textual, auditory, and visual information together provide more comprehensive emotional information than they do individually, just as the brain relies on multiple sensory input sources to validate events. Moreover, unimodal information is insufficient and can be easily affected by various external factors ]. Emotional signals can be disguised or affected by other signals from a single channel, for example, when facial expressions are obscured or when noise interferes with speech, resulting in a considerable reduction in emotion analysis performance. Multimodal emotion analysis considers the complementarity of emotion expression among modalities and is thus more robust and aligned with natural human behavior expression. Therefore, research on multimodal fusion of affective computation has received increasing attention. Multimodal fusion algorithms integrate information from different modalities into a stable multimodal representation, enabling comprehensive processing and coordinated optimization to identify human emotions as accurately as possible ]. Common multimodal fusion methods can be categorized into feature-, model-, and decision-layer-based fusion depending on the fusion stage ].#### Generation and expression of emotionsAffective computing enables machines to provide empathic feedback based on deep contextual understanding. Robots and other agents can deliver expressions and responses, conveying the emotional temperature to the user through facial expressions, emotional text responses, and body movements , ] by building on the results of sentiment analysis and recognition. Emotional text generation and speech synthesis are the most-studied areas of research. Emotional text generation involves the automatic generation of emotional response content that matches the message of the dialogue and is consistent with the machine’s strategy, which is chosen according to the context ]. For instance, a traffic enforcement robot may exhibit a fundamental difference in the language used for persuasion and the language used for enforcement, a difference that is crucial to obtaining effective practical traffic management results. The goal of emotional text generation is for the model to generate text that conforms to a specified sentiment category, as expressed by emotion-related keywords or techniques such as metaphors ]. Pretrained models such as GPTs are increasingly being utilized as a base for emotionally controllable text generation and achieving powerful results ]. Responding to text content with emotional color is only the first step. The generated text needs to be expressed using a related emotional voice. Emotional coding information is integrated into the speech synthesis model to make human–machine dialogue less cold and mechanical, thereby allowing individuals to perceive “machine empathy” and feel warmth and affinity. Emotional speech synthesis uses a specific voice style and combines text content with emotional tags to provide a robot or agent with a voice that expresses a particular emotion ]. This process inputs textual content and a specific voice style into a neural network that synthesizes an output voice in that style by utilizing the spectral, rhythmic, and linguistic features of human voices that express emotion.### Applications of affective computingAffective computing is a technology that advances according to the actual needs of the industry, which drives progress and iteration. To build up reliability, general applications initially focused on recreation, leisure, or serving people with urgent needs, then gradually expanded to more fields, transforming the technology and contributing to productive endeavors. In 2021, the value of affective computing reached $21.6 billion, and it is expected to double by 2024 , ]. As the industry grows, the creative applications of affective computing technologies will flourish, yielding satisfactory results in various fields.#### EducationIn the field of education, affective computing is primarily used to recognize the emotional state of learners and provide corresponding feedback and adjustment ]. For example, teachers can utilize intelligent emotional teaching systems to better understand students’ engagement levels and adjust the pace and content of their teaching to improve the learning experience. An intelligent system can recommend customized learning content based on the sentiment analysis of students’ interests. Students can provide authentic teaching feedback through intelligent systems to improve the comprehensiveness and accuracy of teaching evaluations. One advantage of an intelligent system is that it can be used in both traditional and online classrooms to strengthen the contextualization of online teaching, enhance emotional interaction between teachers and students, and improve teaching quality. Affective computing techniques are also conducive to the research and development of educational games and robots ], providing improved human–computer interaction and achieving educational objectives more effectively.#### HealthcareAffective computing research has expanded into various psychiatric disorders in the affective disorders category, such as Alzheimer’s ], Parkinson’s ], bipolar disorder ], and post-traumatic stress ], and into healthcare areas including relaxation service healthcare ] and health office systems ]. Affective computing enables the scientific and objective identification and judgment of patients’ emotions, particularly in psychological disorder treatments, providing a useful complement to more subjective traditional diagnostic tools such as behavioral observation and scale filling. Objective data collection can improve personalized and precise medical treatment ]. In addition, affective computing can be used for the initial screening and efficacy assessment of diseases. For instance, patients with social anxiety disorder exhibit important differences in emotional facial processing compared to the normal population, differences that can be identified by automated monitoring of differential features ].#### Business servicesIn marketing, where the consumer experience is highly correlated with emotions, affective computing is widely used to understand and recognize the user’s emotional state. The application of affective computing can reveal the user’s true preferences and improve and streamline the buying process ]. In the field of financial credit, affective computing technologies can be used to analyze the emotional state and moral level of a customer based on voice and tone, determine the probability of the customer lying, and provide a guide for lending decisions. In the field of stock investment, investor decisions are influenced by irrational judgments. The price trend of a stock is determined not only by a company’s fundamentals but also to a large extent by fluctuations in investor emotions. The study of investor sentiment from social media data (e.g., data from X, formerly known as Twitter) can help identify investors’ emotional preferences and cognitive biases for the purpose of predicting the direction of the stock market ].#### Integration of science and artIn the current digital era, image, audio, and video data have become plentiful and important. Extracting useful information from them and retrieving and mining them effectively are crucial. For example, in recommending music to users, resource management and audio search efficiency are essential. Traditional music search methods match content using text (e.g., song title, artist name, or lyrics). Including sentiment, a high-level semantic feature of music, improves the match between user preferences and music, thus aiding in the primary task in music sentiment analysis ]. Affective computing also empowers automated poetry generation, where deep learning methods such as RNNPG, an RNN-based poem generator, and SeqGAN, a sequence generative adversarial network, are gradually replacing Word Salada, genetic algorithms, and statistical machine translation methods – ]. Expressing emotions more richly is key in making generated poetry spiritual, i.e., in moving beyond resemblance of form to resemblance in spirit.### Importance of this studyThe field of affective computing has grown considerably and exploded in popularity in the last decade for 2 reasons: technological developments providing tools for affective computing and the growth and expansion of demand. In the era of human–machine symbiosis, the deepened human understanding of emotional connotation and the improvement of the “double quotient” (i.e., IQ + EQ) of intelligent machines will become a vital innovative force promoting the affective computing discipline, technological evolution, and industrial progress. Despite the rapid development in affective computing, a comprehensive review of research and systematic analysis of hotspots and trends is lacking. Continuous innovation in algorithmic technology, broadening application requirements, and increasing research efforts necessitate that existing research be summarized and future technological directions be identified. Doing so will enable academia and industry to better understand the development of affective computing technology, thus will facilitate affective computing research, empower applications, and benefit society.This study aims to fill the gaps in existing research through a comprehensive review of affective computing from 1997, when Picard formally proposed the concept, up to 2023. We adopted a bibliometric analysis method to accurately portray the current status of the development of the field and provide insights into present challenges and future trends. The main contributions of this study are as follows. (a) Facing the academic frontier, we list the research hotspots and trends that we identified by analyzing full-scale papers. This allows readers to quickly and comprehensively grasp the development dynamics of the field and understand key common and frontier-leading technologies. (b) Facing major needs and the main battlefield of the economy, we provide blueprints for technological development and insights into current applications. This promotes the application and transformation of affective computing, facilitating high-quality economic development and digital transformation. (c) Facing future trends, we introduce challenges and developments in the field of affective computing, along with predictions for future technology and industry application directions. This serves as a forward-looking guide to the field.## Materials and Methods### Data collectionThis study searched for papers published in affective computing from January 1997 to September 2023 in the Web of Science Core Collection (WoSCC), which includes the Science Citation Index Expanded, Social Sciences Citation Index, Arts & Humanities Citation Index, Emerging Sources Citation Index, Conference Proceedings Citation Index—Science (CPCI-S), and Conference Proceedings Citation Index—Social Sciences & Humanities (CPCI-SSH). The search strategy is summarized in Table .| Index field | Search strategy || --- | --- || Theme keywords | “affective recognition” or “mood recognition” or “affective computing” or “artificial emotional intelligence” or “emotion AI” or “expression recognition” or “emotion recognition” or “emotion learning” or “sentiment analysis” or “sentiment recognition” || Literature types | proceedings papers, articles, review articles, early access |Table\\xa01. Search strategy for this studyThe reason this study uses 1997 as the starting point of the timeline is that the book _Affective computing_ ], which was published in that year, is regarded as the work that established affective computing as an independent academic research field. Papers outside this time range were not included in the calculation of citation statistics. In the statistics of Chinese papers, Hong Kong, Macau, and Taiwan are included. The results show that 33,448 papers were published worldwide. Among them, 16,097 (48.13%) were conference papers and 17,351 (51.87%) were journal papers. It should be noted that the names of institutions were standardized using machine and manual methods. However, when scientists publish papers, the writing of the names of institutions is not standardized, which may have caused the omission of papers in the statistics and a deviation in the index calculation results.In addition, this study combined the following 3 databases for data acquisition: (a) Incites: This database is based on the publication date of all document types in the major index databases of the WoSCC. It performs publication count and index calculations to provide research performance analysis. (b) Essential Science Indicators (ESI): This is an in-depth analytical research tool based on the Web of Science. ESI can identify influential countries, institutions, papers, and publications, as well as the cutting-edge in a research field. (c) Journal Citation Reports (JCRs): This is a multidisciplinary journal evaluation tool that provides journal evaluation resources based on citation data statistics. By citing and counting references, the JCR can measure the influence of research at the journal level, revealing the relationships between citing and cited journals.### Data analysisStatistical analysis was performed using a bibliometric method. Bibliometrics applies quantitative methods such as mathematics and statistics to the literature of a scientific or other field and processes statistical data based on information science theory. This widely accepted approach provides quantitative analysis pathways and innovative insights into the assessment of research trends based on previous literature , ]. Unlike peer review and expert judgment, bibliometrics can provide quantitative indicators to ensure objectivity through statistical analysis of academic achievements ]. Bibliometric analysis enables monitoring and summarizes the status, hotspots, and trends of a particular topic, helping researchers identify future research directions ]. In this study, we first cleaned and analyzed the data using the Derwent Data Analyzer (DDA, version 10, Clarivate, London, UK), which is well integrated with the source data from the Web of Science platform. DDA was used for multidimensional data mining, preprocessing, standardization, and statistical analysis. Subsequently, the bibliometric analysis and knowledge visualization software tool VOSviewer (version 1.6.15, Leiden University, Leiden, Netherlands) was employed. This analysis tool provides valuable insights into the structure, advancement, and collaboration in the field of affective computing. Notably, its distinctive feature lies in the graphical representation of bibliometric maps, which is particularly suitable for large-scale data analysis ]. VOSviewer was used to visualize the data in this study.## Results### Publication trendsFrom 1997 to 2009, the number of articles published in this field steadily increased, exhibiting an overall growth trend despite occasional fluctuations (Fig.\\xa02). From 2010 to 2019, with the rise of deep learning, a rapid development was observed in the field of affective computing, and the number of articles published in the field rose rapidly, indicating an explosive growth stage of research. After 2019, because of a plateau in the innovation of deep learning methods and the impact of the coronavirus disease 2019 (COVID-19) pandemic on academia, research in the field of affective computing also reached a plateau, and the rising trend slowed down.### Comparison of countriesTo analyze the main research positions in the field of affective computing, the country/region fields of all the authors and the first author of the paper were counted. As shown in Table , among the top 20 countries with publications in the field of affective computing, China is the country with the largest number of publications, accounting for 26.2% of all authors and 24.6% of first authors. China, the United States, India, the United Kingdom, and Germany rank among the top 5 in the number of papers published counting all authors or first author, and are the most important in terms of research in the field of affective computing. The United States ranks second in the number of papers published counting all authors, but third in the number of papers published counting only first author, after India.| No. | Country | Number of papers | Country | Number of papers || --- | --- | --- | --- | --- || (All authors) | (All authors) | (First author) | (First author) || --- | --- | --- | --- || 1 | China | 8,780 | China | 8,223 || 2 | USA | 4,715 | India | 3,632 || 3 | India | 3,829 | USA | 3,274 || 4 | UK | 2,535 | UK | 1,432 || 5 | Germany | 1,706 | Germany | 1,253 || 6 | Japan | 1,321 | Italy | 1,022 || 7 | Italy | 1,302 | Japan | 977 || 8 | Australia | 1,234 | South Korea | 931 || 9 | Spain | 1,178 | Spain | 862 || 10 | South Korea | 1,121 | Australia | 788 || 11 | Canada | 1,100 | Canada | 720 || 12 | France | 943 | France | 587 || 13 | Netherlands | 778 | Turkey | 581 || 14 | Saudi Arabia | 765 | Netherlands | 484 || 15 | Turkey | 691 | Malaysia | 479 || 16 | Singapore | 640 | Pakistan | 454 || 17 | Malaysia | 609 | Brazil | 443 || 18 | Pakistan | 595 | Greece | 413 || 19 | Brazil | 522 | Iran | 398 || 20 | Greece | 483 | Singapore | 394 |Table\\xa02. The top 20 countries in the field of affective computingIn addition to the 2-year step in 2021–2022, a 4-year step was used to count the publication volume of the top 10 countries in the field of affective computing. The results are shown in Fig. . Given that the concept of “affective computing” originated in the United States, which has been a major research force in this field, we chose the United States as the benchmark. During the entire period, the relative volume of publications by China and the United States changed considerably, as shown in Fig. . From 1997 to 2004, the number of papers published by the United States far exceeded that of China. From 1997 to 2000, the total number of papers published by China was 20% of that of the United States. From 2001 to 2004, the total number of papers published by China rose to 31% of that of the United States. In the period from 2005 to 2008, the number of papers published by China surpassed that of the United States, and the number of papers published by China in 2021–2022 is about 3 times that of the United States. It can be seen that in recent years, China’s research in the field of affective computing has accumulated rapidly, and its large volume of research has certain advantages compared with that of the United States. In addition, in 2021–2022, the number of papers published by India surpassed that of the United States for the first time. India has gradually become a major research center in the field of affective computing because of its advantages in computer science, engineering, and other disciplines.![](\\xa03. Comparison between the top 10 countries and the United States in the number of publications.### Main journalsThis section analyzes basic data on journal papers. The 17,351 published papers were distributed in 1,300 journals, among which _IEEE Access_ [impact factor (IF) 3.9] had the most (875), as shown in Table . Across all journals, 1,209 had an IF listed in the 2022 JCRs. The distribution of the IFs of the 1,209 journals is shown in Table . Among them, 54 journals have IFs greater than 10, and the 5 journals with the highest IFs are _World Psychiatry_ (73.3), _Lancet Psychiatry_ (64.3), _Nature Reviews Neuroscience_ (34.7), _Nature Human Behaviour_ (29.2), and _JAMA Psychiatry_ (25.8). The IFs of most journals are distributed in the 2 intervals of 2 ≤ IF < 4 and 4 ≤ IF < 7. It is worth noting that _IEEE Transactions on Affective Computing_ (IF 11.2) is a high-level journal focusing on the field of affective computing. It is a cross-disciplinary and international archive journal aimed at disseminating the results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. In addition, _Expert Systems with Applications_, _Knowledge-Based Systems_, _Information Processing & Management_, _IEEE Transactions on Multimedia_, _Neurocomputing_, _Information Sciences_, _Pattern Recognition_, _Applied Soft Computing_, _Decision Support Systems_, and _Future Generation Computer Systems_ are also high-level journals favored by scholars in the field of affective computing.| No. | Journal | Number of papers || --- | --- | --- || 1 | _IEEE Access_ | 875 || 2 | _Multimedia Tools and Applications_ | 474 || 3 | _IEEE Transactions on Affective Computing_ | 419 || 4 | _Sensors_ | 378 || 5 | _Frontiers in Psychology_ | 362 || 6 | _Applied Sciences-Basel_ | 349 || 7 | _Expert Systems with Applications_ | 290 || 8 | _International Journal of Advanced Computer Science and Applications_ | 272 || 9 | _Neurocomputing_ | 248 || 10 | _Knowledge-Based Systems_ | 226 || 11 | _Psychiatry Research_ | 191 || 12 | _Electronics_ | 167 || 13 | _Journal of Intelligent & Fuzzy Systems_ | 151 || 14 | _Neural Computing & Applications_ | 144 || 15 | _Neuropsychologia_ | 137 || 16 | _Schizophrenia Research_ | 135 || 17 | _Information Processing & Management_ | 132 || 18 | _Computational Intelligence and Neuroscience_ | 114 || 19 | _Cognitive Computation_ | 112 || 20 | _Information Sciences_ | 110 |Table\\xa03. Top 20 journals with the largest number of articles in the field of affective computing| Journal impact factor | Number of journals || --- | --- || IF ≥ 10 | 54 || 7 ≤ IF < 10 | 74 || 4 ≤ IF < 7 | 255 || 2 ≤ IF < 4 | 406 || 1 ≤ IF < 2 | 245 || IF ≤ 1 | 175 |Table\\xa04. Journal impact factor distribution### High-level international conferencesCombining ESI’s highly cited and hot papers with the “China Computer Federation Recommended International Academic Conferences” and CORE Computer Science Conference Rankings, we identified the high-level international conferences related to affective computing. These include the ACM International Conference on Multimedia (ACM MM), AAAI Conference on Artificial Intelligence (AAAI), Annual Meeting of the Association for Computational Linguistics (ACL), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), and the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).### Discipline distributionThis section analyzes the distribution of research fields based on statistics on the Web of Science categories of papers in the field of affective computing. Studies related to the topic of affective computing involve computer science, communication, engineering, psychology, medicine, and other disciplines, reflecting distinct interdisciplinary characteristics. The top 20 categories with the largest number of publications are listed in Table . The category with the largest proportion is “Computer Science, Artificial Intelligence,” with 12,678 publications (37.93% of the total), followed by “Engineering, Electrical & Electronic,” with 9,820 publications (29.36% of the total).| Web of Science category | Number of papers | Percentage (%) || --- | --- | --- || Computer Science, Artificial Intelligence | 12,687 | 37.90 || Engineering, Electrical & Electronic | 9,820 | 29.36 || Computer Science, Information Systems | 8,714 | 26.05 || Computer Science, Theory & Methods | 8,405 | 25.13 || Computer Science, Interdisciplinary Applications | 3,930 | 11.75 || Telecommunications | 3,133 | 9.37 || Computer Science, Software Engineering | 2,982 | 8.92 || Neurosciences | 2,376 | 7.10 || Psychiatry | 2,100 | 6.28 || Computer Science, Cybernetics | 1,904 | 5.69 || Imaging Science & Photographic Technology | 1,077 | 3.22 || Engineering, Multidisciplinary | 1,045 | 3.12 || Automation & Control Systems | 997 | 2.98 || Computer Science, Hardware & Architecture | 981 | 2.93 || Psychology, Multidisciplinary | 884 | 2.64 || Robotics | 793 | 2.37 || Engineering, Biomedical | 735 | 2.20 || Acoustics | 724 | 2.16 || Linguistics | 637 | 1.90 || Clinical Neurology | 610 | 1.82 |Table\\xa05. Top 20 categories with the most papers in the field of affective computing### Technology transfer and conversionThis study searched the Derwent Innovation Index, the world’s most comprehensive database of value-added patent information. Among effective invention patents with transfer records and high value, the transferred patents with an IncoPat patent value of 10 (the highest level) include “Cognitive content display device” (US10902058B2, transferred from IBM to Kyndryl Inc.) and “Signal processing approach to sentiment analysis for entities in documents” (US9436674B2, transferred from Attivio Inc. to Servicenow Inc.). However, the number of patent transfer records related to affective computing is small, indicating that technology transfer activity needs to be improved.### Global distribution of scholarsThis section presents statistical analysis of publications based on the country of the first author to provide a macroscopic understanding of the global distribution of scholars in the field of affective computing. As shown in Table , China has the largest number (4,240), followed by India (2,391) and the United States (2,390). In Fig. , darker shading indicates a larger number of scholars. It can be seen that Asia and North America are the regions with the most concentrated distribution of scholars in the field of affective computing.| No. | Country | Number of scholars | No. | Country | Number of scholars || --- | --- | --- | --- | --- | --- || 1 | China | 4,240 | 11 | Canada | 533 || 2 | India | 2,391 | 12 | France | 425 || 3 | USA | 2,390 | 13 | Turkey | 403 || 4 | UK | 999 | 14 | Netherlands | 349 || 5 | Germany | 825 | 15 | Malaysia | 331 || 6 | Italy | 690 | 16 | Pakistan | 324 || 7 | Japan | 631 | 17 | Brazil | 366 || 8 | South Korea | 514 | 18 | Greece | 248 || 9 | Spain | 545 | 19 | Iran | 270 || 10 | Australia | 496 | 20 | Singapore | 229 |Table\\xa06. Number of first authors in the field of affective computing (top 20 countries)![](\\xa04. Global distribution of scholars in the field of affective computing.### International collaborationThere is a wide range of international cooperation in the field of affective computing. A count of collaborations between the top 20 countries is shown in Table . The number of articles published by China and the United States is the largest (641), followed by China and the United Kingdom (343). Although cooperation between China and the United States has been challenging in recent years, in the field of affective computing, they remain each other’s largest partners, maintaining a vital and continuous cooperation.|  | C1 | U1 | I1 | U2 | G1 | J | I2 | A | S1 | S2 | C2 | F | N | S3 | T | S4 | M | P | B | G2 || --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- || C1 | / | 641 | 79 | 343 | 79 | 256 | 59 | 218 | 43 | 61 | 137 | 44 | 28 | 61 | 12 | 161 | 38 | 57 | 6 | 4 || U1 | 641 | / | 128 | 232 | 174 | 42 | 120 | 122 | 62 | 83 | 153 | 100 | 105 | 55 | 42 | 71 | 10 | 36 | 53 | 23 || I1 | 79 | 128 | / | 73 | 15 | 15 | 22 | 37 | 19 | 33 | 26 | 26 | 8 | 67 | 10 | 48 | 26 | 11 | 3 | 2 || U2 | 343 | 232 | 73 | / | 294 | 40 | 132 | 119 | 96 | 15 | 60 | 89 | 160 | 77 | 25 | 64 | 30 | 43 | 30 | 48 || G1 | 79 | 174 | 15 | 294 | / | 41 | 69 | 47 | 39 | 11 | 43 | 59 | 99 | 4 | 17 | 17 | 5 | 7 | 13 | 19 || J | 256 | 42 | 15 | 40 | 41 | / | 4 | 20 | 17 | 7 | 27 | 15 | 11 | 6 | 5 | 18 | 16 | 1 | 3 | 1 || I2 | 59 | 120 | 22 | 132 | 69 | 4 | / | 22 | 58 | 13 | 25 | 65 | 53 | 10 | 13 | 37 | 4 | 10 | 6 | 7 || A | 218 | 122 | 37 | 119 | 47 | 20 | 22 | / | 26 | 14 | 30 | 22 | 24 | 27 | 14 | 35 | 25 | 23 | 11 | 3 || S1 | 43 | 62 | 19 | 96 | 39 | 17 | 58 | 26 | / | 15 | 15 | 42 | 46 | 23 | 12 | 11 | 5 | 10 | 24 | 18 || S2 | 61 | 83 | 33 | 15 | 11 | 7 | 13 | 14 | 15 | / | 10 | 16 | 6 | 29 |  | 2 | 9 | 71 | 3 | 3 || C2 | 137 | 153 | 26 | 60 | 43 | 27 | 25 | 30 | 15 | 10 | / | 45 | 19 | 40 | 10 | 9 | 2 | 12 | 17 | 3 || F | 44 | 100 | 26 | 89 | 59 | 15 | 65 | 22 | 42 | 16 | 45 | / | 39 | 11 | 4 | 2 | 8 | 16 | 18 | 10 || N | 28 | 105 | 8 | 160 | 99 | 11 | 53 | 24 | 46 | 6 | 19 | 39 | / |  | 19 | 8 | 3 | 1 | 11 | 14 || S3 | 61 | 55 | 67 | 77 | 4 | 6 | 10 | 27 | 23 | 29 | 40 | 11 |  | / | 8 | 4 | 41 | 120 | 1 | 3 || T | 12 | 42 | 10 | 25 | 17 | 5 | 13 | 14 | 12 |  | 10 | 4 | 19 | 8 | / | 5 | 7 | 3 | 2 | 1 || S4 | 161 | 71 | 48 | 64 | 17 | 18 | 37 | 35 | 11 | 2 | 9 | 2 | 8 | 4 | 5 | / | 4 | 2 |  | 1 || M | 38 | 10 | 26 | 30 | 5 | 16 | 4 | 25 | 5 | 9 | 2 | 8 | 3 | 41 | 7 | 4 | / | 40 |  | 3 || P | 57 | 36 | 11 | 43 | 7 | 1 | 10 | 23 | 10 | 71 | 12 | 16 | 1 | 120 | 3 | 2 | 40 | / | 3 |  || B | 6 | 53 | 3 | 30 | 13 | 3 | 6 | 11 | 24 | 3 | 17 | 18 | 11 | 1 | 2 |  |  | 3 | / |  || G2 | 4 | 23 | 2 | 48 | 19 | 1 | 7 | 3 | 18 | 3 | 3 | 10 | 14 | 3 | 1 | 1 | 3 |  |  | / |Table\\xa07. Collaborations between the top 20 countries in the field of affective computingNote: C1, China; U1, USA; I1, India; U2, UK; G1, Germany; J, Japan; I2, Italy; A, Australia; S1, Spain; C2, Canada; S2, South Korea; F, France; N, Netherlands; T, Turkey; S3, Saudi Arabia; S4, Singapore; M, Malaysia; P, Pakistan; B, Brazil; G2, Greece.### Important research institutionsThe top 10 institutions in the world by number of publications (counting all authors) are listed in Table . This study used indicators such as Citation Impact, Category Normalized Citation Impact (CNCI), and Highly Cited Papers to further evaluate the influence of various institutions in the field of affective computing. Among them, CNCI is a valuable and unbiased impact indicator that excludes the influence of publication year, subject field, and document type. A CNCI value of 1 indicates that the cited performance of a group of papers is equivalent to the global average level, a value greater than 1 indicates higher performance, and a value less than 1 indicates lower performance; a value of 2 indicates performance twice as high as the global average. The top 5 institutions according to CNCI rankings were Nanyang Technological University (5.06), Imperial College London (3.58), Tsinghua University (3.23), the Chinese Academy of Sciences (3.15), and the University of California System (2.77).| No. | Institution | Number of papers | Citation impact | Category Normalized Citation Impact | H-index | Percentage in Q1 journals | Country || --- | --- | --- | --- | --- | --- | --- | --- || 1 | Chinese Academy of Sciences | 699 | 20.97 | 3.15 | 60 | 59.87 | China || 2 | University of London | 443 | 50.26 | 2.29 | 77 | 69.45 | UK || 3 | UDICE-French Research Universities | 388 | 18.86 | 1.37 | 42 | 50.43 | France || 4 | Centre National de la Recherche Scientifique (CNRS) | 377 | 19.33 | 1.36 | 42 | 51.56 | France || 5 | University of California System | 371 | 40.83 | 2.77 | 64 | 58.72 | USA || 6 | National Institute of Technology (NIT System) | 364 | 9.68 | 1.46 | 29 | 26.43 | India || 7 | Indian Institute of Technology System (IIT System) | 360 | 13.51 | 1.99 | 36 | 44.7 | India || 8 | Nanyang Technological University | 350 | 46.35 | 5.06 | 69 | 68.99 | Singapore || 9 | Tsinghua University | 302 | 24.87 | 3.23 | 44 | 62.93 | China || 10 | Imperial College London | 300 | 41.00 | 3.58 | 49 | 70.75 | UK |Table\\xa08. Institutions with a top 10 publication in affective computingNotes: 1. Citation impact: The citation impact of a set of documents is calculated by dividing the total number of citations of the set of documents by the number of documents. Citation impact shows the average number of citations received by a document in the group. 2. Category Normalized Citation Impact (CNCI): The CNCI of a document is obtained by dividing the actual number of citations by the expected number of citations of documents of the same type, publication year, and subject. When a document is classified into multiple subject areas, the average value of the ratio of actual citations to expected citations is used. The CNCI of a country is the average of the CNCIs of the publications of that country.### Citation network analysisThis section analyzes the direct citations of all authors in the field of affective computing. To highlight the key authors, 40 authors who had published no fewer than 30 papers were selected for analysis. The results are shown in Fig. . Authors in clusters of the same color have strong correlations and inheritance in research content. Representative scholars from the 5 clusters are listed in Table .![](\\xa05. Citation network of scholars.| Scholar | Organization | Research fields || --- | --- | --- || Baoliang Lu | Shanghai Jiaotong University, China | Brain-like computing, neural networks, deep learning, emotion AI, affective brain–computer interface || Bjoern Schuller | Imperial College London, UK | Machine intelligence, signal processing, affective computing, digital health, speech recognition || Erik Cambria | Nanyang Technological University, Singapore | Affective computing, sentiment analysis, commonsense reasoning, natural language understanding || Fuji Ren | The University of Tokushima, Japan the University of Electronic Science and Technology of China, China | Natural language processing, artificial intelligence, affective computing, and emotional robots || Wenming Zheng | Southeast University, China | Multimodal affective computing, neural computation, pattern recognition, machine learning, and computer vision |Table\\xa09. Representative scholars in the citation network### Word frequency analysisWord frequency refers to the number of times a word occurs in the document being analyzed. In scientometric research, word frequency dictionaries can be established for specific subject areas to quantify the analysis of scientists’ creative activities. Word frequency analysis is the method of extracting keywords or subject words that express the core content of the articles in the literature, to study the development trends and research hotspots of the field through the frequency distribution of these words. The results of conducting frequency and co-occurrence analysis on keywords assigned to papers by authors in the field of affective computing are shown in Table .| No. | Number of occurrences | Technical keyword | Number of co-occurrences with other keywords | Time period | Proportion of occurrences within last 3 years (%) || --- | --- | --- | --- | --- | --- || 1 | 7,621 | Sentiment analysis | Machine learning [958]; Opinion mining [936]; Natural language processing [829] | 2006–2023 | 21 || 2 | 4,566 | Emotion recognition | Feature extraction [422]; Affective computing [397]; Deep learning [372] | 1997–2023 | 24 || 3 | 2,457 | Affective computing | Emotion recognition [397]; Machine learning [191]; Emotion [137] | 2000–2023 | 15 || 4 | 2,232 | Deep learning | Sentiment analysis [691]; Emotion recognition [372]; Machine learning [268] | 2012–2023 | 40 || 5 | 2,054 | Machine learning | Sentiment analysis [958]; Natural language processing [275]; Deep learning [268] | 2002–2023 | 27 || 6 | 1,816 | Facial expression recognition | Deep learning [182]; Feature extraction [150]; Face recognition [109] | 1997–2023 | 18 || 7 | 1,348 | Natural language processing | Sentiment analysis [829]; Machine learning [275]; Deep learning [209] | 2006–2023 | 30% of 1,348 || 8 | 1,214 | Feature extraction | Emotion recognition [422]; Sentiment analysis [213]; Task analysis [181] | 2003–2023 | 32 || 9 | 1,209 | Opinion mining | Sentiment analysis [936]; Natural language processing [159]; Machine learning [151] | 2006–2023 | 11 || 10 | 1,067 | Emotion | Affective computing [137]; Emotion recognition ]; Facial expression ] | 1999–2023 | 13 || 11 | 1,007 | Twitter | Sentiment analysis [770]; Machine learning [160]; Social media [145] | 2011–2023 | 18 || 12 | 975 | Speech emotion recognition | Deep learning ]; Feature extraction ]; Emotion recognition ] | 2006–2023 | 29 || 13 | 852 | Social media | Sentiment analysis [587]; Twitter [145]; Machine learning [105] | 2009–2023 | 21 || 14 | 732 | Social cognition | Schizophrenia [193]; Emotion recognition [184]; Theory of mind [179] | 2001–2023 | 16 || 15 | 657 | Text mining | Sentiment analysis [486]; Natural language processing ]; Opinion mining ] | 2006–2023 | 15 || 16 | 635 | EEG | Emotion recognition [357]; Affective computing ]; Emotion ] | 2004–2023 | 27 || 17 | 620 | Classification | Sentiment analysis [208]; Machine learning ]; Emotion recognition ] | 2003–2023 | 19 || 18 | 618 | Facial expression | Emotion recognition [175]; Emotion ]; Affective computing ] | 1998–2023 | 15 || 19 | 582 | Convolutional neural network | Deep learning [146]; Facial expression recognition ]; Emotion recognition ] | 2003–2023 | 30 || 20 | 535 | Schizophrenia | Social cognition [193]; Emotion recognition ]; Theory of mind ] | 1998–2023 | 8 || 21 | 478 | Support vector machine | Sentiment analysis [123]; Facial expression recognition ]; Emotion recognition ] | 2002–2023 | 9 || 22 | 470 | Feature selection | Sentiment analysis [119]; Emotion recognition ]; Feature extraction ] | 2001–2023 | 16 || 23 | 423 | Face recognition | Feature extraction [155]; Emotion recognition [124]; Facial expression recognition [109] | 1997–2023 | 29 || 24 | 422 | Transfer learning | Emotion recognition ]; Deep learning ]; Sentiment analysis ] | 2009–2023 | 40 || 25 | 404 | Data mining | Sentiment analysis [251]; Feature extraction ]; Machine learning ] | 2006–2023 | 22 |Table\\xa010. Frequency analysis of top 25 keywords in affective computingThe Thomson Data Analyzer was used to automatically and manually clean the keywords assigned by the authors of papers in the dataset. Subsequently, VOSviewer was used to cluster the core (high-frequency) subject words and set a certain co-occurrence frequency and co-occurrence intensity according to the size of the dataset to cluster the keywords. Combined with expert interpretation, each cluster was named and interpreted, and the topics of the journal articles were identified and analyzed. After keyword cleaning, 613 keywords appearing more than 20 times were selected as analysis objects for cluster calculation. Five clusters were obtained by clustering the core subject words with the highest co-occurrence intensity, as shown in Table  and Fig. .| No. | Research theme | Number of core subject words | Average number of citations | Average correlation strength || --- | --- | --- | --- | --- || 1 | Natural language processing techniques used for affective computing and opinion mining | 153 | 10.41 | 197.80 || 2 | Facial expression and micro-expression recognition and analysis | 134 | 15.89 | 178.77 || 3 | Affective computing studies in human–computer interaction | 121 | 18.69 | 110.38 || 4 | Applied research of affective computing in affective disorder analysis | 30 | 33.5 | 165.59 || 5 | Multimodal sentiment analysis based on deep learning | 81 | 9.8 | 260.95 |Table\\xa011. Five research themes in affective computing![](\\xa06. Five research themes in affective computing.The average number of citations of a research theme is the average number of times that a paper containing these subject words has been cited since publication, and the average correlation strength of a research theme indicates the closeness of the connection between the core subject words contained in this theme concept. The greater the correlation strength, the greater the co-occurrence intensity between the core subject words and the more concentrated the research. In contrast, relatively lower correlation is associated with more scattered research. Research on the application of affective computing in the analysis of affective disorders has the highest average citation frequency, which shows that interdisciplinary research involving affective computing and medicine, especially research on affective disorders and depression recognition, has a greater influence. The average correlation strength of multimodal sentiment analysis based on deep learning is the largest, which shows that the research on this topic is the most concentrated.## DiscussionThis paper presents a comprehensive analysis and review of systematically collected data on papers and major intellectual property rights in the field of affective computing. The results reveal that over the past 25 years, affective computing has experienced rapid growth in the number of published papers, representing a vibrant academic ecology and an interdisciplinary character with a wide range of disciplines. Additionally, scholars worldwide actively participate in a relatively close cooperation network. In particular, Chinese scholars have led the world in terms of the number of publications, scholars, and collaborative papers in this field. Among important research institutions, Tsinghua University and the Chinese Academy of Sciences stand out, with CNCI values indicating that the average number of citations of their papers was more than twice the global average. Citation network analysis showed that Chinese scholars are representative and have become essential nodes in the citation network, indicating that China is constructing a large-scale talent team for affective computing and progressing in both the quantity and quality of research. However, China also faces disadvantages in academic journals, international conferences, and other aspects, leading to weak dominance, which restricts China’s academic discourse improvement in this field. Notably, in recent years, India’s publication volume has exceeded that of the United States for the first time, revealing a robust development potential linked to its advantages in computing. Nonetheless, India still has room for growth in terms of research quality and paper impact as it lacks representative scholars in the field of affective computing.### Challenges and technology development trends#### Modeling of cultural contextsThis study found that affective computing researchers are distributed across various countries globally and have a wide range of cultural backgrounds. While emotional expression has a degree of consistency across humanity, it is considerably influenced by cultural background. Cultural norms and values determine the different emotional experiences of individuals and how others perceive these emotions. Therefore, affective computing systems developed using a single cultural group may fail in other cultural contexts. For example, Chinese, Germans, and Japanese express emotions relatively implicitly, whereas Americans, British, and Brazilians express emotions more overtly. This indicates that emotion agents must match emotion calculation rules with the cultural context. Many Western cultural standards may not necessarily apply in Eastern contexts. For example, Japanese researchers tend to develop robots that can express emotions implicitly because overly direct expressions of emotions may cause user dissatisfaction ]. Therefore, cultural characteristics must be considered in developing universal cross-cultural emotional agents for people from different cultural backgrounds. Hofstede defined culture in terms of 5 measures—power distance, identity, gender, uncertainty avoidance, and long-term orientation—which can be used to summarize the typical rules of emotional expression in different cultural contexts ]. When it is challenging to obtain culture-specific empirical affective data, it is more feasible to design affective computational models using cultural theories and rules.#### Emotion generation techniquesThe cluster analysis of topic terms in affective computing revealed 5 important core topics, including “natural language processing techniques for affective computing and opinion mining” and “facial expression and micro-expression recognition and analysis.” Current research focuses more on emotion recognition, with relatively limited attention accorded to emotion generation. Emotion recognition and generation are both essential aspects of affective computing and constitute an important technical basis for the closed loop of human–computer interaction. To enable machines to provide more anthropomorphic and natural feedback, it is crucial to focus on the following 2 research areas. (a) Generation of facial expressions. The fact that human emotions are expressed through visual (55%), voice (38%), and verbal (7%) signals is also known as the “3V rule,” which reflects the importance of human facial expressions in emotion analysis ]. Appropriate use of facial expressions by avatars and robots can enhance human–robot interaction. Thus, current research aims to build a lexicon of facial expressions that can translate communicative intent into associated expressive morphology and dynamic features to express various meanings. Meanwhile, a team of animation experts is required to achieve realistic facial rendering effects, including lighting and muscle textures. (b) Generation of emotional body movement. This requires the design of embodied agents using computer models of body expression. This area involves studying human kinematics; however, researchers have yet to determine how to characterize the organic combination of body parts, movement strength, and posture of specific emotional states.#### Fine-grained sentiment classification modelsEkman’s basic emotion theory model is a widely used classification model for emotion computation ]. However, in real life, people’s emotions often exist in a mixed state. For example, people often simultaneously express surprise and joy, sadness and pain, etc. Du et\\xa0al. ] proposed the concept of mixed emotions based on research conducted using the Facial Action Coding System (FACS). They suggested that the combination of 2 basic emotions creates mixed emotions and defined different types using scenario examples. Using a FACS-based face recognition algorithm model, microvariations in facial muscles can be analyzed to accurately discriminate between different types of mixed emotions. Martinez ] assessed whether mixed emotions can be semantically labeled correctly. The test tasks included prioritization and forced selection of mixed emotion labels, and the results showed that subjects performed consistent and accurate categorization. Mixed emotion is an essential research direction for expression-based fine-grained emotion classification. This concept extends the core idea of FACS, aiming to reveal the relationship between mixed and basic emotions. It offers a better solution to the problem of differentiation of emotions and clarifies the relationship between differentiated emotions and their original emotions, providing traceable clues and measurement possibilities for the generation, development, and change of emotions. It summarizes complex emotional changes into a logical dynamic composite form with similar configuration effects, resulting in strong interpretability, logic, and unity.#### Code of ethics and technical standardsRecording an individual’s emotional state has implications for privacy, particularly when it comes to recording video or audio. Subjects may not agree to provide researchers with authentic and naturalistic emotional data and may feel uncomfortable being monitored in daily life. For example, the results of AI emotion monitoring tools may be analyzed alongside employee performance evaluations, predictions of the risk of leaving the job, and patterns of employee–team interactions for predicting behavior. Although the use of such technology reduces employee turnover and saves costs for organizations ], employees may experience constant psychological stress, leading to burnout ]. Additionally, individuals may lose autonomy as they become more hesitant to display emotions in public, instead choosing to use a “poker face.” While there should be openness in the use of affective computing, appropriate regulation is necessary to assess potential risks involving privacy and security, and the technology should be reviewed and documented for each industry to maximize benefits while minimizing harm, risks, and costs. Ethical issues are more likely to be overlooked in computing and engineering than in psychology. The collection of individual data, particularly physiological data, should be regulated by human research ethics committees, which are best suited to managing informed consent and privacy issues.Efforts should be made to strengthen the development of international standards in the field of affective computing to form a universally accepted specification. Currently, the available standard is “Information technology—Affective computing user interface (AUI)” (standard number ISO/IEC 30150-1:2022). The first part, “Model,” was released in June 2022, and the second part, “Affective Characteristics,” is under construction. However, there is a lack of standards for data collection, data security, and personal privacy protection in the field of affective computing. Therefore, the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union (ITU) should improve relevant standards and unify them for global use.#### Cognitive neuroscience-inspired affective computingJust as CNN architectures are inspired by biological visual processing and reinforcement learning methods are inspired by behaviorist theories in psychology, impulse network models are inspired by neuroplasticity. Cognitive neuroscience has also developed theories on affective circuits ], multiple-wave models ], embodied cognition ], and other related areas, providing brain-inspired insights into the design of affective computation models. Studies on the physiological representations of different emotions offer theoretical foundations and guidelines for feature extraction in affective computing based on facial expressions, psychophysiological measurements, and neuroimaging. Further human research in the field of cognitive neuroscience will ultimately affect the development of affective computing and artificial intelligence as a whole. The cognitive process of human brain emotion processing, its neural mechanism, and its anatomical basis provide essential inspiration for the development of affective computing models. However, to ensure that machines have genuine emotions rather than just appearing to have emotions, further research in cognitive neuroscience is required. This research may involve exploring the neural basis for the generation of human consciousness, the neural mechanism for the construction of human values, and other key scientific issues. Based on this neural theoretical foundation, simulation and machine implementation are feasible options for providing machines with authentic emotions.#### Construction of large-scale multimodal datasetsThe development of affective computing is highly dependent on the construction of large-scale open datasets. Three major trends are described below. The first trend predicts that dataset sizes will continue to grow to meet the demands of deep learning algorithm training. Deep-learning models have a substantial number of parameters, and the selection of these parameters requires samples that are typically 100 times the number of parameters. A larger dataset size enables the trained model to avoid overfitting, which improves model learning. However, the challenge lies in labeling these massive datasets. Thus, it is necessary to explore active, weakly supervised, and unsupervised learning methods to label the meaningful data in large unlabeled datasets or train machines for labeling. The second trend highlights the need for the collection of multimodal data, the accumulation of richer modal information, and fine-grained alignment between different modalities. At this stage, machines differ from human beings in 2 critical aspects: First, humans exist in a multimodal social environment, as evidenced by their joint expression of intentions and emotions through language, facial expressions, speech, and actions; second, humans can switch between modalities for emotional reasoning when dealing with emotions. They can also switch between different modalities to search for clues, eliminate ambiguities, and conduct emotional reasoning through interconnections. Therefore, creating a large-scale multimodal emotion dataset can contribute to the development of human-like emotion intelligence technology and the realization of more accurate emotion recognition. The third trend focuses on collecting natural-scene data, as emotional data collected in performance or evoked mode may not accurately represent real-life scenarios. However, collecting high-quality labeled emotional-physiological data in daily life remains a challenge due to the lack of hardware collection devices that are sufficiently comfortable and resistant to interference.#### Multimodal fusion technology innovationMultimodal fusion combines information from multiple modalities using multimodal representations for sentiment classification. It can enhance the performance of sentiment-computing models by playing a complementary and disambiguating role ]. Multimodal fusion methods can be classified as model-independent or model-based. Model-independent fusion methods do not rely on a specific deep-learning method, whereas model-based fusion methods do.There are 3 categories of model-independent fusion methods: early fusion (feature-based fusion), late fusion (decision-based fusion), and hybrid fusion (combination of the 2). Early fusion integrates features immediately after they are extracted and uses multiple signals to create a single feature vector, which is then modeled using machine-learning algorithms. The larger the number of features and the greater the variation in these features, the more challenging feature-level fusion becomes and the easier it is to overfit the training data. In contrast, late fusion performs integration only after each model outputs the results (e.g., classification or regression results). It can better handle overfitting but does not allow the classifier to train on all data simultaneously. The Dempster–Shafer theory of evidence is a generalization of Bayesian theory to subjective probability. It is widely used in late fusion models because of its ability to model uncertain knowledge and combine beliefs from different sources to obtain new beliefs that take into account all available evidence. Hybrid fusion combines the outputs of earlier fusion methods and unimodal predictors. Although it is flexible, careful design is required to determine the timing, modalities, and method of fusion based on the specific application problem and research content. Researchers must select the appropriate approach at their discretion.Model-based fusion methods address the multimodal fusion problem through implementation techniques and models, using 3 common methods: multiple kernel learning (MKL), graphical models (GMs), and neural networks (NNs). As these methods easily exploit the spatial and temporal structure of the data, they are particularly suitable for time-related modeling tasks. Additionally, they allow human expert knowledge to be embedded in the model, thereby enhancing interpretability. However, their disadvantage is that they are computationally expensive and challenging to train.Research has shown that synesthesia is generated not only in the cerebral cortex but also in the subcortical limbic system, including the thalamus, amygdala, and hippocampus, which are closely related to emotional processing ]. Inspired by the multistage fusion phenomenon that integrates multisensory information in the brain, a multistage multimodal emotion fusion method can be developed. This would first involve training a unimodal model, splicing it as an implicit state with another modal feature, training the bimodal model similarly, and continuing with this process until a multimodal model is obtained. In conclusion, multimodal fusion technology effectively utilizes the synergistic complementarity of different modal information ], enhances emotional understanding and expression, and improves model robustness and performance. This represents an important direction for future research.#### Data- and knowledge-driven technological innovationIn its early stages, affective computing research relied heavily on collected data to make inferences. However, this data-driven approach is both inefficient and ineffective at the application level. For humans to understand data fully, they must activate other associated information, such as potential knowledge or common sense. The human brain can seamlessly combine this information to enable more generalized, intelligent, and frugal computation for complex problems. Therefore, affective computing requires not only big data and extensive computing power but also the integration of knowledge. Knowledge guidance and inspiration can compensate for insufficient or uneven data quality while conserving computational power. For instance, in constructing a multidisciplinary and multi-faceted emotional knowledge map, fine-grained emotional knowledge integrated through emotional commonsense associations is used to enable the modeling of hierarchical logical relationships between aspect words and emotional words. This approach facilitates the dynamic correlation, aggregation, and reasoning of domain, aspect, and emotional knowledge. It provides an optimal solution for various applications of affective computing, such as efficient real-time online sentiment analysis, emotion-injected dialogue systems, and emotion-injected story generation. These applications provide dynamic and accurate domain-adaptive sentiment knowledge.#### Group affective computingCurrent research in affective computing primarily focuses on sentiment analysis at the individual level, neglecting the potential value of group-affective computing. For instance, emotions felt by individual employees can aggregate and spread to create “collective emotions” in the workplace. These shared emotions can considerably affect the organization by offering insights into absenteeism, intra-team communication, team cohesion and performance, and organizational citizenship behavior. As such, affective computing research could expand its focus from individual to collective affect analysis and the propagation of affect across people. Furthermore, group affective computing can predict consumer behavior. EEG-based hyperscanning technology, which explores dynamic brain activity between 2 or more interacting customers and their underlying neuroemotional activities, can be used to anticipate shared consumption intentions, panic buying, and group-buying marketing effects. Although group affective computing currently lacks a well-established research methodology, it is a promising direction for future studies.#### Unique emotional carriersEmotions are ubiquitous in human political, economic, and cultural life, and the carriers of emotions are continually increasing in number, making them a popular research topic. Several areas have been identified as key carriers of emotions. (a) Political speeches: CORPS is a corpus that contains political speeches with markers indicating audience reactions such as applause, standing ovations, and boos ]. Researchers can use this information to predict emotion-evoking actions and persuasive content that may induce empathy and sympathy in audiences. (b) Music and drama: Affective computing in music and drama provides a basis for the categorized retrieval of relevant emotional carriers. Advancements in artificial intelligence-generated content (AIGC) technology have made machine-generated music possible, and affective computing can enhance the generation of music to conform to emotional classifications. (c) Oil painting: As a representative art form, oil painting allows creators to express their innermost emotions. Its charm lies not in the degree of realism but in the emotions it conveys. Combining affective computing with oil painting would enable the exploration of artificial intelligence methods for emotional expression, the integration of technology and art, and the establishment of a library of emotion-inducing materials for oil paintings, thereby providing resources for the development of affective computing disciplines.### Outlook for future applications#### Affective brain–computer interfacesAffective brain–computer interfaces (aBCIs) are primarily designed to measure emotional states through neurological measurements and to recognize and/or regulate human emotions. Currently, aBCIs are one of the main methods of realizing emotional intelligence. At this stage, the most commonly used physiological signals for emotional brain–computer interfaces are EEG signals, which map closely to an individual’s emotional state. As in motor brain–computer interfaces, the human brain plays the role of a controller for the entire system. The first step involves decoding an individual’s initial emotional state and then recognizing and understanding their emotions. Subsequently, a control strategy or system is designed to achieve the target emotion using control signals or parameters that provide feedback to the brain, thereby forming a closed-loop system.Unlike facial expressions, physiological signals such as EEG signals are difficult to disguise and provide an accurate reflection of the real emotional state of the individual. As a result, affective brain–computer interfaces play a crucial role in clinical diagnostics and therapy. Their uses include detecting workload and mental state, using neurofeedback for stress relief, aiding in the diagnosis of social anxiety and other disorders ], and enabling objective assessment and intervention in depression. Furthermore, affective brain–computer interfaces have considerable potential for military applications. They can help maximize the physiological capabilities of individual soldiers, enhance their endurance and tolerance to extreme environments, and improve their overall physical and mental fitness. These objectives are achieved by installing electroencephalography electrodes inside combat helmets to detect threats and emotional signals emitted by the brain. The signals are then converted into computer language using computer algorithms, analyzed, and confirmed by combat command. Subsequently, threat warnings and reminders about emotional regulation are sent to the affected soldiers, and signals to cooperate in combat are transmitted to surrounding soldiers. In addition, direct transcranial current stimulation, transcranial electromagnetic stimulation, and deep brain cortex stimulation can act on the brain to eliminate fatigue, reduce stress and anxiety, control pain sensation, and enhance cognitive ability. This system helps improve the situational awareness of soldiers on the battlefield, thereby improving their ability to survive.The primary obstacle to the application of affective brain–computer interfaces is their unstable performance. Cross-modal affective models that rely on heterogeneous transfer learning (HTL) may be necessary for establishing reliable and robust aBCI technology in complex real-world environments. To address the missing-modalities problem, cross-modal emotion models comprehensively analyze signals from multiple modalities and extract correlation characteristics during the training process. In the testing stage, predictions are made based on partial modal information. For example, correlating EEG signals with eye movement enables the use of eye movement alone to assess emotions in scenarios where collecting EEG signals is difficult. The HTL approach ensures that performance degradation in the absence of modalities is acceptable, thereby improving model robustness. In addition, transfer-learning techniques based on deep and generative adversarial networks can solve the problem of individual differences. These techniques enable generalization from the source domain to the target domain, thereby expanding the scope of possible applications of affective brain–computer interfaces.#### Empathic human–computer dialogueThere have been 4 waves of change in the way people interact with machines. The first wave, represented by Microsoft, involved the organic fusion of the user interface, operating system, keyboard, and mouse. This greatly reduced the difficulty of human–computer interaction and contributed to the rapid popularization of the personal computer. The second wave, represented by Google, involved the organic integration of search engine and internet technologies. This integration broke down information silos and considerably expanded the boundaries of interaction. The third wave, represented by Apple, involved the miniaturization of computing represented by the smartphone. This breakthrough removed the physical space limitations of human–computer interaction, enabling interconnectivity anytime, anywhere. Currently, we are in the fourth wave, represented by OpenAI. This wave involves the comprehensive application of a human–computer dialogue system that makes human–computer interaction more anthropomorphic and naturalized.The essence of human–computer dialogue is to make human–computer interaction more human-like. Humans exchange information through natural language and multiple senses, and human–computer interaction can imitate this process through multimodal information for joint analysis and decision-making. Human–computer dialogue involves a diverse range of signals, including speech, text, and images (such as individual facial expressions and body movements), conveying information in both the rational and perceptual dimensions. Linguistic text serves as the ontology of intent understanding, but emotional information conveyed through voice intonation, facial expressions, and body movements plays a crucial role in disambiguation, which is essential for in-depth communication between humans and machines. The use of different emotional colors to express the same sentence results in entirely different connotations. As Nobel Prize winner Simon noted, emotion recognition is crucial for the communication and understanding of information. Therefore, affective computing offers machines the ability to achieve deep contextual understanding.In advanced technology fields, research has expanded to include machine expression and action generation, referred to as “multimodal emotional expression generation.” A current focus area is the development of a “virtual human” interface that not only appears human-like but also simulates human demeanor and behavior. For instance, voice-driven facial-expression animation generation technology can create virtual humans with facial expressions and lip, head, and body movements that closely resemble those of real people. The virtual human no longer has an empty skin but appears more 3-dimensional and vivid. The personalization of human–computer interaction lays the crucial foundations for future applications in areas such as elderly companions, intelligent customer service, and mayor hotlines, revealing important prospects for practical use.#### Emotion-assisted decision-makingHuman–computer interaction involves both shallow and deep levels. At the shallow level, machines are equipped with the ability to read and speak, whereas at the deep level, they are capable of thinking and making decisions like humans. Nobel Prize winner Kahneman described human decision-making as entailing 2 processes: fast (System 1) and slow (System 2). The unconscious “System 1” relies on emotions, experience, and rapid judgments, while the conscious “System 2” relies on rational deliberation. Emotions play an important role in advanced human thinking and decision-making. The book “Descartes’ Error” emphasizes that emotions are crucial for rational decision-making and behavior ]. Numerous studies have indicated that purely rational decision-making may not always be the optimal solution for humans when dealing with problems due to the complexity of the social environment. Incorporating emotional factors into the decision-making process may help individuals identify better solutions. Therefore, inputting emotional variables can enable machines to make decisions in a more human-like manner. In building a harmonious human–machine symbiotic society, it is essential to master this high-level function, which is also an important direction in affective computing research. The modeling of machine agents has begun to incorporate patterns of emotional influence on human rational decision-making and mechanisms for deciding and interrupting behaviors based on goals , ].Emotion-assisted decision-making abilities can be applied widely across various fields of human–machine collaboration. For example, in production tool manipulation, the operator’s emotional state regarding operation specifications, safety awareness, and accurate judgment has an impact. Monitoring and early warning of negative emotions, psychological stress, fatigue, and drowsiness, etc., can help identify potential anthropogenic risks to production safety. Machines can then optimize management decisions, intervene early, and intervene intelligently to avoid major accidents. In assisted driving, negative emotions such as anger and anxiety can seriously affect the driver’s concentration and may lead to traffic accidents. Emotion-assisted decision making can be incorporated into driver monitoring systems (DMS) that use facial-expression recognition technology and wearable devices to provide real-time monitoring of the driver’s emotional state. This approach equips the vehicle with enhanced safety performance and improves the overall driving experience ].#### Affective virtual realityThe metaverse is generating considerable interest in both industrial and academic circles as the next generation of immersive, full-fledged internet. It is considered a theme park for digitized human beings, a virtual complex resulting from the development of cutting-edge technologies, and a utopia where the human body and consciousness can cross physical time and space. As a new type of future living space, the development of the metaverse cannot be limited to creating a virtual space parallel to the real world. It should exist in human life like air, enabling humans to shuttle freely between the virtual and real worlds. Affective virtual reality is crucial for constructing the metaverse because it can considerably enhance an individual’s experience of bodily ownership, sense of agency, and situational awareness. In particular, an individual’s avatar in the metaverse, which is a core element of the metaverse construction, includes voice tone, facial expressions, body movements, and gestures that richly and 3-dimensionally express the individual’s emotions and create scenes and spaces for emotional twins ]. As in movies and literature, complex and emotionally rich avatar characters engage audiences more than simple and stable characters do. This appeal creates the illusion that avatars are alive and pass the Turing test, which enhances the audience’s interest and engagement in the virtual world ]. Affective virtual reality has considerable potential for applications in virtual reality socialization, virtual reality anchors, and virtual reality marketing.### LimitationsThis bibliometric analysis has several limitations that should be acknowledged. First, the basic processing unit of information in this study is the article in its entirety, and the full content of the literature has not been systematically broken down, which may result in incomplete analysis and conclusions. Second, the assumption that the articles contain information of equal quality makes it difficult to consider the objective differences in the value of the literature. In future research, a combination of bibliometrics and content analysis could be used to enhance the reliability and accuracy of the analytical results.## ConclusionAffective computing is a rapidly developing field with broad prospects. Emerging forces such as China and India are injecting strong momentum into the field. However, the field of affective computing also faces challenges and development trends in 10 aspects, including cultural background modeling, ethical and moral norms, and multimodal integration. Affective computing has great potential for application in 4 major fields and requires the joint efforts of researchers and industry practitioners. These efforts can make affective computing beneficial to the progress of human society by building a more anthropomorphic, harmonious, and natural human–computer symbiotic social form.## Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.## Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.## References1Keltner D, Sauter D, Tracy J, Cowen A. Emotional expression: Advances in basic emotion theory. _J Nonverbal Behav_. 2019; **43**(2):133–160.2Soleymani M, Garcia D, Jou B, Schuller B, Chang S-F, Pantic M. A survey of multimodal sentiment analysis. _Image Vis Comput_. 2017; **65**:3–14.3Bach DR, Dayan P. Algorithms for survival: A comparative perspective on emotions. _Nat Rev Neurosci_. 2017; **18**:311–319.4Chen L, Zhou M, Wu M, She J, Liu Z, Dong F, Hirota K. Three-layer weighted fuzzy support vector regression for emotional intention understanding in human–robot interaction. _IEEE Trans Fuzzy Syst_. 2018; **26**(5):2524–2538.5Kahneman D. _Thinking_, _fast and slow_. Macmillan, London, UK: Farrar, Straus and Giroux; 2011.6Fanselow MS. Emotion, motivation and function. _Curr Opin Behav Sci_. 2018; **19**:105–109.7Lopes PN, Salovey P, Coté S, Beers M. Emotion regulation abilities and the quality of social interaction. _Emotion_. 2005; **5**:113–118.8Suvilehto JT, Glerean E, Dunbar RIM, Hari R, Nummenmaa L. Topography of social touching depends on emotional bonds between humans. _Proc Natl Acad Sci U S A_. 2015; **112**:13811–13816.9Picard RW. _Affective computing_. Cambridge (MA): MIT Press; 1997.10Ekman P. Are there basic emotions? _Psychol Rev_. 1992; **99**(3):550–553.11Russell JA. A circumplex model of affect. _J Pers Soc Psychol_. 1980; **39**:1161–1178.12Mehrabian A. Framework for a comprehensive description and measurement of emotional states. _Genet Soc Gen Psychol Monogr_. 1995; **121**(3):339–361.13Bakker I, Van Der Voordt T, Vink P, De Boon J. Pleasure, arousal, dominance: Mehrabian and russell revisited. _Curr Psychol_. 2014; **33**:405–421.14Pozzi FA, Fersini E, Messina E, Liu B. Chapter\\xa01—Challenges of sentiment analysis in social networks: An overview. In: Pozzi FA, Fersini E, Messina E, Liu B, editors, _Sentiment analysis in social networks_. Boston: Morgan Kaufmann; 2017. p. 1–11.15Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. Poster presented at: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies; Portland, Oregon, USA; 2011. p. 142–150.16Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing; 2013; Seattle, WA, USA. p. 1631–1642.17Blitzer J, Dredze M, Pereira F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Poster presented at: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics; 2007; Prague, Czech Republic. p. 440–447.18Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of German emotional speech. _Interspeech_. 2005; **5**:1517–1520.19McKeown G, Valstar M, Cowie R, Pantic M, Schroder M. The SEMAINE Database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE Trans Affect Comput_. 2011; **3**(1):5–17.20Xu L, Xu M, Yang D. Chinese emotional speech database for the detection of emotion variations. _J Tsinghua Univ Nat Sci._ 2009;49(S1):1413–1418 _._21Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: From unimodal analysis to multimodal fusion. _Inf Fusion_. 2017; **37**:98–125.22Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, Gao S, Sun Y, Ge W, Zhang W, et al. A systematic review on affective computing: Emotion models, databases, and recent advances. _Inf Fusion_. 2022; **83–84**:19–52.23Zhang Z, Luo P, Loy CC, Tang X. From facial expression recognition to interpersonal relation prediction. _Int J Comput Vis_. 2018; **126**:550–569.24Mollahosseini A, Hasani B, Mahoor MH. AffectNet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Trans Affect Comput_. 2019; **10**:18–31.25Li S, Deng W, Du J. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017; . Honolulu, HI. p. 2584–2593.26Li X, Pfister T, Huang X, Zhao G, Pietikäinen M. A spontaneous micro-expression database: Inducement, collection and baseline. Paper presented at: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG); 2013; Shanghai, China. p. 1–6.27Galvão F, Alarcão SM, Fonseca MJ. Predicting exact valence and arousal values from EEG. _Sensors (Basel)_. 2021; **21**(10):3414.28Shalbaf A, Bagherzadeh S, Maghsoudi A. Transfer learning with deep convolutional neural network for automated detection of schizophrenia from EEG signals. _Phys Eng Sci Med_. 2020; **43**(4):1229–1239.29Shirahama K, Grzegorzek M. Emotion recognition based on physiological sensor data using codebook approach. In: Piętka E, Badura P, Kawa J, Wieclawek W, editors. _Information technologies in medicine_. Cham: Springer International Publishing; 2016. p. 27–39.30Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I. DEAP: A database for emotion analysis\\xa0using physiological signals. _IEEE Trans Affect Comput_. 2012; **3**(1):18–31.31Duan R-N, Zhu J-Y, Lu B-L. Differential entropy feature for EEG-based emotion classification. Paper presented at: 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER); 2013; San Diego, CA, USA. p. 81–84.32Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K. Introducing WESAD, a multimodal dataset for wearable stress and affect detection. Paper presented at: Proceedings of the 20th ACM International Conference on Multimodal Interaction; 2018; Boulder, CO, USA. p. 400–408.33Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. _Comput Linguist_. 2011; **37**(2):267–307.34Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. Paper presented at: Proceedings of the International Conference on Web Search and Web Data Mining—WSDM ’08; 2008; Palo Alto, CA, USA. p. 231.35Mullen T, Collier N. Sentiment analysis using support vector machines with diverse information sources. Paper presented at: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing; 2004; Barcelona, Spain. p. 412–418.36Pak A, Paroubek P. Text representation using dependency tree subgraphs for sentiment analysis. In: Xu J, Yu G, Zhou S, Unland R, editors. _Database systems for advanced applications_. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. p. 323–332.37Deng J, Ren F. A survey of textual emotion recognition and its challenges. _IEEE Trans Affect Comput_. 2023; **14**(1):49–67.38Heaton CT, Schwartz DM. Language models as emotional classifiers for textual conversation. Paper presented at: Proceedings of the 28th ACM International Conference on Multimedia; 2020; Seattle, WA, USA. p. 2918–2926.39Mao R, Liu Q, He K, Li W, Cambria E. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. _IEEE Trans Affect Comput_. 2022; **14**(3):1743–1753.40Lee CM, Narayanan SS. Toward detecting emotions in spoken dialogs. _IEEE Trans Audio Speech Lang Process_. 2005; **13**(2):293–303.41Lugger M, Yang B. The relevance of voice quality features in speaker independent emotion recognition. Paper presented at: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing—ICASSP ’07; 2007; Honolulu, HI, USA. p. IV-17–IV–20.42Likitha MS, Gupta SRR, Hasitha K, Raju AU. Speech based human emotion recognition using MFCC.Paper presented at: 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); 2017; Chennai, India. p. 2257–2260.43Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. _Speech Commun_. 2010; **52**(7–8):613–625.44Alisamir S, Ringeval F. On the evolution of speech representations for affective computing: A brief history and critical overview. _IEEE Signal Process. Mag_. 2021; **38**(6):12–21.45Stappen L, Baird A, Schumann L, Schuller B. The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements. _IEEE Trans Affect Comput_. 2023; **14**(2):1334–1350.46Huang Z, Dong M, Mao Q, Zhan Y. Speech emotion recognition using CNN. Paper presented at: Proceedings of the 22nd ACM International Conference on Multimedia; 2014; New York, NY, USA. p. 801–804.47Neumann M, Vu NT. Improving speech emotion recognition with unsupervised representation learning on unlabeled speech. Paper presented at: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2019; Brighton, UK. p. 7390–7394.48Abdelwahab M, Busso C. Domain adversarial for acoustic emotion recognition. _IEEE/ACM Trans Audio Speech Lang Process_. 2018; **26**(12):2423–2435.49Shan C, Gong S, McOwan PW. Facial expression recognition based on Local Binary Patterns: A comprehensive study. _Image Vis Comput_. 2009; **27**(6):803–816.50Chao W-L, Ding J-J, Liu J-Z. Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection. _Signal Process_. 2015; **117**:1–10.51James W. Review of la pathologie des emotions by Ch. Féré. _Philos Rev_. 1893; **2**:333–336.52Cannon WB. The James-Lange theory of emotions: A critical examination and an alternative theory. _Am J Psychol_. 1987; **100**:567–586.53Kim M-K, Kim M, Oh E, Kim S-P. A review on the computational methods for emotional state estimation from the human EEG. _Comput Math Methods Med_. 2013; **2013**: Article e573734.54Craik A, He Y, Contreras-Vidal JL. Deep learning for electroencephalogram (EEG) classification tasks: A review. _J Neural Eng_. 2019; **16**(3): Article 031001.55Maria MA, Akhand MAH, Shimamura T. Emotion recognition from EEG with normalized mutual information and convolutional neural network. Paper presented at: 2022 12th International Conference on Electrical and Computer Engineering (ICECE); 2022; Dhaka, Bangladesh. p. 372–375.56Rahman MM, Sarkar AK, Hossain MA, Hossain MS, Islam MR, Hossain MB, Quinn JMW, Moni MA. Recognition of human emotions using EEG signals: A review. _Comput Biol Med_. 2021; **136**: Article 104696.57D’mello SK, Kory J. A review and meta-analysis of multimodal affect detection systems. _ACM Comput Surv_. 2015; **47**(3):1–36.58He Z, Li Z, Yang F, Wang L, Li J, Zhou C, Pan J. Advances in multimodal emotion recognition based on brain–computer interfaces. _Brain Sci_. 2020; **10**(10):687.59Filippini C, Perpetuini D, Cardone D, Chiarelli AM, Merla A. Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review. _Appl Sci_. 2020; **10**(8):2924.60Spezialetti M, Placidi G, Rossi S. Emotion recognition for human-robot interaction: Recent advances and future perspectives. _Front Robot AI_. 2020; **7**: Article 532279.61Peng Y, Fang Y, Xie Z, Zhou G. Topic-enhanced emotional conversation generation with attention mechanism. _Knowl Based Syst_. 2019; **163**:429–437.62Dybala P, Ptaszynski M, Rzepka R, Araki K, Sayama K. Metaphor, humor and emotion processing in human-computer interaction. _Int J Comput Linguist Res_. 2013.63Goswamy T, Singh I, Barkati A, Modi A. Adapting a language model for controlled affective text generation. Paper presented at: Proceedings of the 28th International Conference on Computational Linguistics; 2020; Barcelona, Spain. p. 2787–2801.64Lei Y, Yang S, Wang X, Xie L. MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. _IEEE/ACM Trans Audio Speech Lang Process_. 2022; **30**:853–864.65Crawford K. Time to regulate AI that interprets human emotions. _Nature_. 2021; **592**(7853):167.66Ho M-T, Mantello P, Nguyen H-KT, Vuong Q-H. Affective computing scholarship and the rise of China: A view from 25 years of bibliometric data. _Humanit Soc Sci Commun_. 2021; **8**: Article 282.67Yadegaridehkordi E, Noor NFBM, Ayub MNB, Affal HB, Hussin NB. Affective computing in education: A systematic review and future research. _Comput Educ_. 2019; **142**: Article 103649.68Wu C-H, Huang Y-M, Hwang J-P. Review of affective computing in education/learning: Trends and challenges. _Br J Educ Technol_. 2016; **47**(6):1304–1323.69Liberati G, Veit R, Kim S, Birbaumer N, von Arnim C, Jenner A, Lulé D, Ludolph AC, Raffone A, Belardinelli MO, da Rocha JD, Sitaram R. Development of a binary fMRI-BCI for Alzheimer patients: A semantic conditioning paradigm using affective unconditioned stimuli. Paper presented at: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction; 2013; Geneva, Switzerland. p. 838–842.70Yuvaraj R, Murugappan M, Mohamed Ibrahim N, Iqbal Omar M, Sundaraj K, Mohamad K, Palaniappan R, Mesquita E, Satiyan M. On the analysis of EEG power, frequency and asymmetry in Parkinson’s disease during emotion processing. _Behav Brain Funct_. 2014; **10**:12.71Baki P, Kaya H, Çiftçi E, Güleç H, Salah AA. A multimodal approach for mania level prediction in bipolar disorder. _IEEE Trans Affect Comput_. 2022; **13**(4):2119–2131.72Mohammadi-Ziabari SS, Treur J. Integrative biological, cognitive and affective modeling of a drug-therapy for a post-traumatic stress disorder. In: Fagan D, Martín-Vide C, O’Neill M, Vega-Rodríguez MA, editors. _Theory and practice of natural computing_. Cham: Springer International Publishing; 2018. p. 292–304.73Tivatansakul S, Ohkura M. Healthcare system focusing on emotional aspects using augmented reality—Implementation of breathing control application in relaxation service. Paper presented at: 2013 International Conference on Biometrics and Kansei Engineering; 2013; Tokyo, Japan. p. 218–222.74Zenonos A, Khan A, Kalogridis G, Vatsikas S, Lewis T, Sooriyabandara M. HealthyOffice: Mood recognition at work using smartphones and wearable sensors. Paper presented at: 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops); 2016; Sydney, NSW, Australia. p. 1–6.75Weziak-Bialowolska D, Bialowolski P, Lee MT, Chen Y, VanderWeele TJ, McNeely E. Psychometric properties of flourishing scales from a comprehensive well-being assessment. _Front Psychol_. 2021; **12**: Article 652209.76Pei G, Xiao Q, Pan Y, Li T, Jin J. Neural evidence of face processing in social anxiety disorder: A systematic review with meta-analysis. _Neurosci Biobehav Rev_. 2023; **152**: Article 105283.77Pei G, Li T. A literature review of EEG-based affective computing in marketing. _Front Psychol_. 2021; **12**: Article 602843.78Valle-Cruz D, Fernandez-Cortez V, López-Chau A, Sandoval-Almazán R. Does twitter affect stock market decisions? Financial sentiment analysis during pandemics: A comparative study of the H1N1 and the COVID-19 periods. _Cognit Comput_. 2022; **14**(1):372–387.79Gómez LM, Cáceres MN. Applying data mining for sentiment analysis in music. In: De la Prieta F, Vale Z, Antunes L, Pinto T, Campbell AT, Julián V, Neves AJR, Moreno MN, editors. _Trends in cyber-physical multi-agent systems_. Cham: Springer International Publishing; 2018. p. 198–205.80Yu L, Zhang W, Wang J, Yu Y. SeqGAN: Sequence generative adversarial nets with policy gradient. Paper presented at: Proceedings of the AAAI Conference on Artificial Intelligence; 2017; San Francisco, CA, USA. p. 31.81Oliveira HG. A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation. Paper presented at: Proceedings of the 10th International Conference on Natural Language Generation; 2017; Santiago de Compostela, Spain. p. 11–20.82Zhang X, Lapata M. Chinese Poetry Generation with Recurrent Neural Networks. Paper presented at: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2014; Doha, Qatar. p. 670–680.83Mao G, Liu X, Du H, Zuo J, Wang L. Way forward for alternative energy research: A bibliometric analysis during 1994–2013. _Renew Sustain Energy Rev_. 2015; **48**:276–286.84Haustein S, Larivière V. The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects. In: Welpe I, Wollersheim J, Ringelhan S, Osterloh M, editors. _Incentives and performance: Governance of research organizations_. Cham: Springer International Publishing; 2014. p. 121–139.85Hammarfelt B, Rushforth AD. Indicators as judgment devices: An empirical study of citizen bibliometrics in research evaluation. _Res Eval_. 2017; **26**(3):169–180.86Wang J, Veugelers R, Stephan P. Bias against novelty in science: A cautionary tale for users of bibliometric indicators. _Res Policy_. 2017; **46**(8):1416–1436.87Van Eck NJ, Waltman L. Software survey: VOSviewer, a computer program for bibliometric mapping. _Scientometrics_. 2010; **84**(2):523–538.88Šabanović S. Robots in society, society in robots. _Int J of Soc Robotics_. 2010; **2**:439–450.89Hofstede G. _Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations_. London, UK: Sage; 2001.90Mehrabian A. _Communication without words. Communication theory. 2nd ed._ London, UK: Routledge; 2008.91Du S, Tao Y, Martinez AM. Compound facial expressions of emotion. _Proc Natl Acad Sci U S A_. 2014; 111(15):E1454–E1462.92Martinez AM. Computational models of face perception. _Curr Dir Psychol Sci_. 2017; **26**(3):263–269.93Dragano N, Lunau T. Technostress at work and mental health: Concepts and research results. _Curr Opin Psychiatry_. 2020; **33**(4):407–413.94LeDoux J. _The emotional brain: The mysterious underpinnings of emotional life_. New York, NY, USA: Simon and Schuster; 1998.95Pessoa L, Adolphs R. Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. _Nat Rev Neurosci_. 2010; **11**(11):773–782.96Price TF, Peterson CK, Harmon-Jones E. The emotive neuroscience of embodiment. _Motiv Emot_. 2012; **36**:27–37.97Cytowic RE. _Synesthesia: A union of the senses_. Cambridge, MA, USA: MIT Press; 2002.98Guerini M, Strapparava C, Stock O. CORPS: A corpus of tagged political speeches for persuasive communication processing. _J Inf Technol Politics_. 2008; **5**(1):19–32.99Damasio AR. _Descartes’ error_. New York, NY, USA: Random House; 2006.100Scheutz M. The inherent dangers of unidirectional emotional bonds between humans and social robots. In: Lin P, Abney K, Bekey GA, editors. _Robot ethics: The ethical and social implications of robotics_. Cambridge (MA): MIT Press; 2011. p. 205.101Scheutz M, Schermerhorn P. Dynamic robot autonomy: Investigating the effects of robot decision-making in a human-robot team task. Paper presented at: Under review for the 4th ACM International Conference on Human-Robot Interaction; 2009; La Jolla, CA, USA.102Gill R, Singh J. A review of neuromarketing techniques and emotion analysis classifiers for visual-emotion mining. Paper presented at: 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART); 2020; Moradabad, India. p. 103–108.103Pei G, Li B, Li T, Xu R, Dong J, Jin J. Decoding emotional valence from EEG in immersive virtual reality. Paper presented at: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC); 2022; Chiang Mai, Thailand. p. 1469–1476.104Ochs M, Sadek D, Pelachaud C. A formal model of emotions for an empathic rational dialog agent. _Auton Agent Multi-Agent Syst_. 2012; **24**:410–440.### Information#### Published In![]( ComputingVolume 3#### CopyrightCopyright © 2024 Guanxiong Pei et\\xa0al.Exclusive licensee Zhejiang Lab. No claim to original U.S. Government Works. Distributed under a .#### Article versions#### Submission history**Received**: 15 October 2023**Accepted**: 6 December 2023**Published online**: 5 January 2024#### Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.#### Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.### Authors#### AffiliationsGuanxiongPei Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.HaiyingLi National Science Library, Chinese Academy of Sciences, Beijing, China.YandiLu Center for Psychological Sciences, Zhejiang University, Hangzhou, China.YanleiWangDe.InnoScience, Deloitte, Shanghai, China.ShizhenHuaResearch Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.TaihaoLi  Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.#### Funding Informationthe Zhejiang Provincial Natural Science Foundation of China : LQ22C090007the National Natural Science Foundation of China : T2241018the National Science and Technology Major Project of the Ministry of Science and Technology of China : 2021ZD0114303the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2023KFKT003the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2022KFKT002#### Notes*Address correspondence to: ### Metrics#### Article UsageArticle Metrics- - No data available.05001,0001,500Jan 2024Apr 2024Jul 2024Oct 2024Jan 2025Apr 2025Jul 202520,97629- - - - Total number ofdownloads and citation**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.Citation information is sourced from  service.#### Altmetrics]( more details]( up by **10** news outlets]( by **7** X users]( **1** Facebook pages]( readers on Mendeley#### Dimensions32Total citations32Recent citationsn/aField Citation Ration/aRelative Citation Ratio]( 10 citation in Web of Science- 29 citation in Crossref#### scite![]( Citations22010Citing PublicationsSupportingMentioningContrastingSee how this article has been cited at scite shows how a scientific paper has been cited by providing the context of the citation, a classification describing whether it supports, mentions, or contrasts the cited claim, and a label indicating in which section the citation was made.### Citations#### Cite as- Guanxiong Pei,- Haiying Li,- Yandi Lu,- Yanlei Wang,- Shizhen Hua,- Taihao Li.Affective Computing: Recent Advances, Challenges, and Future Trends._Intell Comput._2024;3:0076.DOI: #### Export citationSelect the format you want to export the citation of this publication.Please select one from the listRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorksDirect import01. - Jonas Fritsch,    Sticking with Affect in HCI and Design: from Interaction to Relation, Proceedings of the sixth decennial Aarhus conference: Computing X Crisis, (202-215), (2025).     02. - Yuan Zhou,    - Siamak Khatibi,Exploring Artificial Personality Grouping Through Decision Making in Feature Spaces, AI, **6**, 8, (184), (2025). 03. - FNU Rakhi,    Modeling Students’ Emotions in Computing Education: A Context-Specific Multi-Modal Approach, Proceedings of the 2025 ACM Conference on International Computing Education Research V.2, (59-60), (2025).     04. - Jialin Sun,    Research on the Design of Intelligent Voice Interaction System Based on Affective Computing, International Scientific Technical and Economic Research, (1-15), (2025).     05. - Susanne Draheim,    - Kai von Luck,    - Larissa Putzar,Working with the Gaps in Understanding: Revisiting an Approach to Affective Computing Research in the EmotionBike Project, Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments, (596-601), (2025). 06. - Ken Nee Chee,    Affectagogy Unleashed, Pedagogical Approaches to Bridging Emotional and Cognitive Learning, (277-292), (2025).     07. - Joaquim C. Reis,    - Luzia Travado,    - Michael H. Antoni,Advancing health psychology research in oncology: Biobehavioral models, stress pathways, and stress-management interventions for cancer patients, International Journal of Clinical and Health Psychology, **25**, 3, (100615), (2025). 08. - Manuel Paniagua-Gómez,    - Manuel Fernandez-Carmona,Trends and Challenges in Real-Time Stress Detection and Modulation: The Role of the IoT and Artificial Intelligence, Electronics, **14**, 13, (2581), (2025). 09. - Thuy Thi Tran,    - Thang Van Le,    - Nam Hoang Le,    - Anh Van Thi Dam,    - Thao Thi Nguyen,    - Anh Thi Tuan Nguyen,    - Hue Thi Nguyen,Emotional attachment to artificial intelligence and perceived social isolation among university students: An application of Sternberg’s triangular theory of love, Multidisciplinary Science Journal, **7**, 12, (2025662), (2025). 10. - Jan Kincl,    - Marc T. P. Adam,    - Tanja Pavleska,The Role of\\xa0Human Factors in\\xa0Cybersecurity Incident Management, HCI for Cybersecurity, Privacy and Trust, (35-53), (2025). 11. See moreLoading...### View options#### PDF formatDownload this article as a PDF file### Share#### Copy the article linkCopy LinkCopied!Copying failed.#### Share on social media        | Now Reading:- - \")- - - - - -  xrefBack.goToxrefBack.goToRequest permissionsExpand AllCollapseExpand for moreShow all referencesSHOW ALL BOOKSCopy link✓Thanks for sharing!Find any serviceA2A![]('}]\n",
      "Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 6, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "[{'title': 'Best Vibe Coding Tools 2025: Top Picks for Better Coding', 'url': 'https://azumo.com/artificial-intelligence/ai-insights/best-vibe-coding-tools', 'markdown': '- - - # Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunExplore the 10 best vibe coding tools for 2025, designed to make coding a more enjoyable and efficient experience. These tools enhance developer productivity with features like AI assistance, fast performance, intuitive design, and smooth collaboration. Learn what makes a tool have \"good vibes\" and why selecting the right coding environment is essential for long-term success. Whether you\\'re looking for minimal setup friction or an elegant interface, these tools will improve your coding journey. From Cursor IDE to GitHub Copilot, find the best fit for your workflow and elevate your development process.Written by:September 2, 2025![]( Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunLet\\'s be real for a second. You can absolutely build amazing software with basic text editors and command-line tools. But why would you want to? The days of suffering through clunky interfaces and frustrating workflows are long gone. Today\\'s coding tools aren\\'t just about functionality anymore; they\\'re about creating an experience that makes you actually excited to write code.Think about it this way: you probably spend 8+ hours a day staring at your development environment. Shouldn\\'t it feel good? Shouldn\\'t it make you more productive instead of fighting you every step of the way? That\\'s exactly what we\\'re talking about when we say \"vibe\" in coding tools.The numbers back this up, too. According to Stack Overflow\\'s 2024 Developer , 76% of developers are now using or planning to use AI coding assistants. The entire software development tools market is projected to hit . This isn\\'t just about new features anymore; it\\'s about developer happiness.At Azumo, we’ve worked with leading companies like Meta, Wolters Kluwer, and Wine Enthusiast to deliver cutting-edge AI-driven solutions, helping teams optimize their workflows and enhance business outcomes.Our deep expertise in  and our  give us a unique perspective on the best tools available in the coding space. We understand that the right tools can dramatically improve a developer\\'s experience and productivity, which is why we’re excited to share our insights on the best vibe coding tools for 2025.## What Makes a Coding Tool Have \"Good Vibes\"?Before we jump into the tools themselves, let\\'s talk about what we actually mean by \"good vibes\" in a development environment. It\\'s not just about pretty colors or trendy interfaces (though those don\\'t hurt).**Smooth Performance That Doesn\\'t Get in Your Way:** You know that feeling when you\\'re in the zone and your editor starts lagging? Total vibe killer. Great tools respond instantly to your input, handle large files without breaking a sweat, and never make you wait around while they \"think.\"**Intuitive Design That Feels Natural:** The best coding tools feel like they were designed by someone who actually codes. They put the most important features right where you expect them, use shortcuts that make sense, and don\\'t make you hunt through menus for basic functionality.**Smart Assistance Without Being Pushy:** Modern tools are getting really good at predicting what you want to do next. The good ones offer helpful suggestions and automate tedious tasks. The great ones do it without interrupting your flow or making you feel like the computer is trying to do your job for you.**Beautiful Aesthetics That Reduce Eye Strain:** Look, we\\'re not being shallow here. When you\\'re staring at code for hours, having clean typography, thoughtful color schemes, and well-organized layouts actually reduces cognitive load. Your brain doesn\\'t have to work as hard to parse what you\\'re looking at.**Strong Community and Ecosystem:** Tools with great vibes usually have passionate communities behind them. That means better documentation, more extensions, helpful tutorials, and people who actually want to help when you run into problems.**Flexibility to Match Your Workflow:** Everyone codes differently. Some people love keyboard shortcuts, others prefer clicking around. Some want everything in one window, others like to spread things across multiple monitors. The best tools adapt to how you work, not the other way around.**Minimal Setup Friction:** Nothing kills enthusiasm faster than spending three hours trying to configure a tool before you can write your first line of code. Great tools either work beautifully out of the box or make customization genuinely enjoyable.This combination of factors creates what developers often describe as \"flow state\"—that magical zone where you\\'re completely absorbed in solving problems and everything just works. That\\'s what we\\'re looking for in our tool selection.## Top 10 Best Vibe Coding Tools### 1. !If we\\'re talking about tools that completely change how coding feels, Cursor has to top the list. This isn\\'t just another editor with AI bolted on; it\\'s been built from the ground up around the idea of AI-human collaboration.What makes Cursor special is how naturally the AI assistance flows into your regular coding process. Instead of feeling like you\\'re constantly asking a chatbot for help, it\\'s more like having a really smart pair programming partner who knows exactly what you\\'re trying to build. Cursor IDE has emerged as particularly noteworthy, making waves as one of the best AI coding tools available due to its superior auto-completion features and integration capabilities.#### Key Features:- Context-aware code completion that actually understands your project- Natural language to code conversion that works surprisingly well- Built-in chat interface for explaining complex code sections- Multi-file editing with AI understanding the relationships between files- Beautiful, clean interface that stays out of your way**Why It Has Great Vibes:** Cursor feels like coding in the future. The AI suggestions are smart enough to be helpful but not so aggressive that they interrupt your thought process. Plus, the interface is clean and modern without being distracting.### 2. !VS Code basically won the editor wars, and for good reason. Visual Studio Code continues to dominate the IDE landscape, establishing itself as the cornerstone development environment regardless of programming language, whether JavaScript, Python, or C++.What\\'s amazing about VS Code isn\\'t just that it\\'s free and powerful; it\\'s that Microsoft somehow made it feel lightweight and fast despite having more features than most full IDEs. The extension ecosystem is incredible, which means you can customize it to feel exactly right for any type of project.#### Key Features:- Massive extension marketplace covering every language and framework- Integrated terminal that actually works well- Git integration that makes version control painless- IntelliSense that\\'s smart without being annoying- Cross-platform consistency**Why It Has Great Vibes:** VS Code strikes the perfect balance between power and simplicity. It\\'s familiar enough that you can jump right in, but deep enough that you\\'ll keep discovering new features months later. The community support means there\\'s always someone who\\'s solved whatever problem you\\'re facing.### 3. !GitHub Copilot isn\\'t an editor itself, but it transforms whatever editor you\\'re using. According to , GitHub Copilot stands as the second most popular AI coding tool, having been tried by 40% of developers and regularly used by 26% of respondents.What\\'s wild about Copilot is how it actually learns your coding style over time. It\\'s not just suggesting random code snippets—it\\'s suggesting code that looks like something you would write, just faster than you could type it.#### Key Features:- Real-time code suggestions as you type- Support for dozens of programming languages- Context awareness that spans multiple files- Integration with the most popular editors- Comment-to-code generation**Why It Has Great Vibes:** Copilot feels like having a coding buddy who\\'s really good at remembering syntax and common patterns. It handles the tedious stuff so you can focus on the interesting problems. Plus, when it suggests exactly what you were about to type, it\\'s genuinely satisfying.### 4. !Zed is the new kid on the block, but it\\'s making waves for all the right reasons. Built in Rust, it\\'s designed from the ground up for performance and collaboration. If you\\'re tired of editors that slow down as your projects get bigger, Zed might be exactly what you need.#### Key Features:- Lightning-fast startup and response times- Real-time collaborative editing- Beautiful, minimalist interface- Built-in AI assistance- Memory-efficient architecture**Why It Has Great Vibes:** Zed feels incredibly snappy and responsive. There\\'s something satisfying about an editor that never makes you wait. The collaborative features are particularly smooth—you can literally watch your teammates code in real-time without any of the lag or sync issues you get with other tools.### 5.  (via Command Line)!According to , the best AI developer tools in 2025 include Aider, Cursor, Zed, Claude Code, Windsurf, and GitHub Copilot, representing the cutting edge of AI-assisted development.Claude Code brings AI assistance directly to your terminal, which is perfect if you\\'re someone who lives in the command line. It\\'s particularly good at understanding complex codebases and making suggestions that actually make sense in context.#### Key Features:- Terminal-native AI assistance- Deep understanding of project structure- Excellent at refactoring and code optimization- Natural language query capabilities- Git integration for tracking changes**Why It Has Great Vibes:** If you\\'re a terminal person, Claude Code feels like having a really smart assistant who speaks your language. It doesn\\'t try to force you into a GUI—it works within the workflow you already love.### 6. !Windsurf represents the next generation of AI-integrated development environments. It\\'s designed around the principle that AI and human developers should work together seamlessly, not as separate entities.#### Key Features:- Deep AI integration across all features- Intelligent project understanding- Advanced debugging assistance- Collaborative AI agent capabilities- Modern, intuitive interface**Why It Has Great Vibes:** Windsurf feels like what development environments will look like in five years. The AI integration is so smooth that it doesn\\'t feel like you\\'re using an AI tool—it just feels like you\\'ve gotten really good at coding really fast.### 7. !Don\\'t sleep on Sublime Text. According to , Sublime Text maintains its reputation for exceptional performance, known for quick response times and the ability to handle large files with ease, featuring innovative multi-caret editing capabilities that allow developers to edit multiple lines simultaneously.While everyone else has been adding AI and collaborative features, Sublime Text has been perfecting the core editing experience. Sometimes you just want an editor that\\'s incredibly fast, looks beautiful, and never gets in your way.#### Key Features:- Incredibly fast performance, even with huge files- Multi-caret editing for complex refactoring- Clean, distraction-free interface- Powerful search and replace capabilities- Minimal memory usage**Why It Has Great Vibes:** Sublime Text feels like a precision instrument. Every interaction is crisp and immediate. It\\'s the editor equivalent of a really good mechanical keyboard: simple, reliable, and satisfying to use.### 8. !Aider is a command-line AI coding assistant that\\'s particularly good at making changes to existing codebases. If you work with large, complex projects and need AI help that actually understands the bigger picture, Aider is worth checking out.#### Key Features:- Git-aware AI assistance- Excellent at understanding large codebases- Command-line focused workflow- Automated testing integration- Smart refactoring capabilities**Why It Has Great Vibes:** Aider respects your existing workflow while making it significantly more powerful. It doesn\\'t try to change how you work—it just makes you better at what you\\'re already doing.### 9. !Replit has transformed from a simple online coding playground into a full-featured development environment. What makes it special is how quickly you can go from idea to running code, especially for collaborative projects.#### Key Features:- Zero-setup development environment- Real-time collaboration features- Built-in deployment and hosting- AI-powered code assistance- Educational-friendly features**Why It Has Great Vibes:** Replit removes all the friction from getting started. You can literally go from having an idea to sharing a working prototype in minutes. It\\'s particularly great for experimenting with new technologies or collaborating with people who might not have development environments set up.### 10. !Tabnine focuses specifically on AI-powered code completion, and it\\'s really good at it. What sets it apart is the focus on privacy and the ability to train on your own codebase to provide more relevant suggestions.#### Key Features:- Privacy-focused AI completion- Team training capabilities- Multi-language support- Integration with the most popular editors- On-premises deployment options**Why It Has Great Vibes:** Tabnine feels thoughtful and respectful of your work. The privacy focus means you can use it on sensitive projects, and the team training features help it get better at suggesting code that matches your team\\'s style and standards.## Productivity Enhancement Features That MatterBeyond the core editing experience, the tools that have the best vibes integrate seamlessly with the rest of your development workflow. According to , IDEs substantially boost developer productivity by consolidating all necessary coding tools into one cohesive workspace, providing access to features like built-in error detection and version control integration rather than requiring manual coding in simple text editors.**Version Control Integration:** The best coding tools make Git feel natural instead of scary. They show you what\\'s changed, make it easy to stage and commit files, and help you resolve merge conflicts without wanting to throw your computer out the window.**Project Management Connections:** According to , popular project and task management tools that enhance the coding experience include Jira, Trello, Asana, and Microsoft Planner, while continuous integration and deployment tools have become essential components of modern software development practices. When your editor can show you which issues you\\'re working on or automatically update task status based on your commits, it creates a much smoother workflow.**CI/CD Pipeline Integration:** Nothing kills momentum like having to switch between your editor and a web browser to check if your build passed. The best tools give you build status right in your editor and make it easy to see what went wrong when something breaks.**Real-Time Collaboration:** Whether you\\'re pair programming or just need to show someone a bug, tools that make collaboration feel natural rather than awkward create much better team experiences.## The Future of Vibe-Focused Development ToolsWe\\'re in the middle of a major shift in how development tools work. According to , product innovation in the software development tools market continues to expand rapidly, with nearly 57% of newly introduced tools now supporting low-code or no-code development approaches.AI integration is becoming table stakes, but the focus is shifting from \"AI that can code\" to \"AI that helps you code better.\" We\\'re seeing tools that understand not just syntax but intent, that can refactor entire codebases while maintaining your coding style, and that can explain complex code in plain English.The developer community is also driving innovation in ways we haven\\'t seen before. According to , which received over 49,000 responses from 177 countries, provides crucial insights into the evolving developer landscape. The survey focuses on 314 different technologies, with new emphasis on AI agent tools, LLMs, and community platforms, indicating where the industry is heading.Visual design and user experience are becoming just as important as functionality. Developers are no longer willing to use tools that feel clunky or outdated, even if they\\'re technically powerful.## Choose Your Coding VibeHere\\'s the thing about coding tools—the \"best\" one is the one that fits how you work and think. If you love keyboard shortcuts and never want to touch a mouse, Sublime Text or Vim-style editors might be your jam. If you\\'re working on a team and need seamless collaboration, Cursor or Zed could be game-changers. If you\\'re just getting started and want something that works well out of the box, VS Code is still hard to beat.The good news is that most of these tools are free or have generous trial periods, so you can actually try them out and see what clicks. Don\\'t just read reviews—actually use them for a few days on real projects. You\\'ll know pretty quickly which ones feel right.The software development industry continues to grow at an incredible pace, with the developer community expanding from 27 million to a projected  in the next decade. As this community grows, the demand for tools that not only function well but actually make coding enjoyable will only increase.Remember, you\\'re going to spend thousands of hours with whatever tools you choose. Pick ones that make those hours feel productive, creative, and maybe even fun. The best coding tool is the one that gets out of your way and lets you focus on building amazing things.Try a few of these tools this week. Set up Cursor for your next AI-assisted project, give Zed a spin if you want something blazingly fast, or explore how GitHub Copilot could speed up your current workflow. The future of coding is here, and it feels pretty good.**Ready to level up your development experience?** At , we understand that great software comes from developers who have access to the best tools and environments. Our nearshore development teams use cutting-edge tools and AI-powered workflows to build intelligent applications that drive real business results. Whether you\\'re looking to augment your existing team or build something completely new, we\\'d love to show you how the right combination of talent and tools can transform your development process.\\u200d![]( the Author:ML/AI & Backend EngineerGuillermo Germade, Data Science Expert at Azumo, specializes in building machine learning models and AI systems, focusing on consumer tech, entertainment, and big data. reCAPTCHASelect all images with **cars** Click verify once there are none left|     |     |     || --- | --- | --- || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( |Please try again.Please select all matching images.Please also check the new images.Please select around the object, or reload if there are none.Verify'}, {'title': 'Top 10 Vibe Coding Tools That Feel Like Magic in 2025', 'url': 'https://dev.to/therealmrmumba/top-10-vibe-coding-tools-that-feel-like-magic-in-2025-1md', 'markdown': 'AI is no longer just assisting developers—it’s transforming how we build software. Whether you\\'re a solo indie hacker, a startup engineer, or part of an enterprise team, there’s now an AI tool that aligns with your workflow. The best ones don’t simply automate tasks; they feel intuitive, context-aware, and remarkably efficient.* * *## Why These Tools Matter in 2025Software development has evolved. We’ve moved past the era of simple code autocomplete and template generators. Today, developers demand tools that integrate deeply with their tech stack, adapt to their workflow, and help them ship faster without compromising quality.Here are three key trends that shape this new landscape:1. **Autonomous Assistance** – Developers now rely on AI tools that can complete multi-step tasks with minimal guidance, from writing modules to fixing bugs.2. **Context-Aware Coding** – Tools that understand your full codebase, API structure, and data flow are no longer a luxury—they’re a necessity.3. **Real-Time Collaboration** – AI is becoming a collaborative partner in the development process, offering suggestions, generating code, and even reviewing pull requests in real-time.]( list focuses on tools that exemplify these traits and offer genuine improvements to the way developers work in 2025.* * *## The Top 10 Vibe Coding Tools in 2025### 1. Apidog MCP Server**The missing link between your API and AI coding assistants** bridges the gap between your backend and tools like Cursor, Cline, and Copilot Workspace. Once your OpenAPI definitions are connected, you can:- Autogenerate API logic and DTOs based on updated schema- Let AI tools access real-time API documentation for better suggestions- Streamline backend/frontend development across large teams]( especially powerful in teams that maintain frequently changing APIs or use domain-driven design.* * *### 2. Cursor]( code editor designed for pair programming with GPT-4** is built on VS Code but reimagines the editing experience with AI at the center. Unlike plugins, Cursor deeply integrates AI into every layer:- Offers context-aware autocomplete that understands your full project- Supports inline debugging and refactoring with natural language prompts- Can explain, rewrite, or extend code based on your cursor positionIt’s ideal for fast iteration, onboarding into unfamiliar codebases, or accelerating solo projects.* * *### 3. Copilot Workspace]( task-oriented Copilot that tackles full development cycles**Copilot Workspace is a major leap from GitHub Copilot. Rather than suggesting code line by line, it can:- Understand a GitHub issue and break it into subtasks- Write full PRs for those subtasks- Summarize what it did and wait for your feedbackThis makes it perfect for junior developers, product engineers, or teams with many open tickets to manage.* * *### 4. Sweep]( that turns GitHub issues into functioning pull requests** is a bot that integrates directly into your GitHub repo. You describe an issue, and it:- Scans the entire repo to understand the current architecture- Writes a PR that resolves the issue- Updates associated files and testsFor lean teams or solo founders managing multiple products, Sweep can save hours of context-switching.* * *### 5.Fine by Fine.dev]( is the AI assistant that startup CTOs have been dreaming of. This AI-powered development platform automates tasks throughout the software development lifecycle, from coding and debugging to testing and code review.What makes Fine special is its ability to work autonomously—it can create and implement code, conduct pull request reviews, generate tests, and handle common issues without constant human input. The GitHub integration and support for asynchronous work make it particularly suitable for distributed teams.* * *### 6. Continue]( with your codebase in any editor** is an open-source extension that works in VS Code and JetBrains. It allows you to:- Chat with your entire codebase- Ask for refactors, explanations, or generation tasks- Use local models or connect with remote LLMs like Claude, GPT-4, or MistralIt’s the go-to for developers who want full AI capabilities without being locked into a closed ecosystem.* * *### 7. The Windsurf Editor by Codeium]( to Keep you in flow state** is revolutionizing the IDE concept by seamlessly integrating AI as a collaborator rather than just a tool. The platform offers a unique blend of AI copilots and autonomous agents that work together with developers to enhance productivity.The Cascade feature, which provides deep contextual awareness across your codebase, has helped me understand and navigate unfamiliar codebases much faster than traditional methods. The multi-file editing and predictive actions keep me in a constant flow state, which has noticeably improved my productivity.* * *### 8. Lazy AI]( app creation with AI-powered workflows** makes building applications almost effortless. Designed for speed and simplicity, it lets users create full-stack apps without writing code. Its standout features include:- Pre-built workflows for common developer and automation tasks- One-click deployment for both frontend and backend projects- Customizable templates for bots, AI tools, dev utilities, and marketing apps- Minimal skill requirement—ideal for rapid prototyping or solo makersIt’s the perfect fit for anyone looking to bring ideas to life without the overhead of traditional development.* * *### 9. Devika**Your open-source AI software engineer** is an open-source autonomous AI agent that takes high-level instructions and breaks them down into actionable development tasks. Designed to behave like a junior developer, it combines:- Advanced planning and reasoning algorithms- Contextual keyword extraction for precision- Dynamic agent tracking and decision-making- Web browsing to supplement gaps in knowledgeDevika stands out as a community-driven alternative to commercial AI coding platforms. It’s ideal for developers looking to delegate complex coding work while retaining full control and transparency over the process.* * *### 10. Cody by Sourcegraph]( code understanding with full repo context** is designed to:- Answer questions across massive codebases- Perform intelligent refactors based on architecture- Search, navigate, and explain with precisionIts integration with Sourcegraph gives it superpowers in monorepos, enterprise codebases, and multi-language projects.### AI Coding Tools in Practice: Use Case ScenariosTo better understand how these AI coding tools can be applied in real-world settings, here are a few practical scenarios that demonstrate their potential across different developer environments:#### **Solo Developer: Boosting Productivity with Cursor + Continue**Independent developers often juggle multiple roles—from writing code to debugging and managing small deployments. Tools like **Cursor** (an AI-native code editor) combined with **Continue** (a Copilot-like extension that integrates into IDEs) offer a significant productivity boost. Together, they streamline code suggestions, automate refactoring, and provide quick answers without leaving the editor, allowing solo devs to maintain momentum and ship faster.#### **Startup Team: Automating Ticket Resolutions with Sweep + Copilot Workspace**Early-stage startups typically operate with lean engineering teams that need to move fast. Tools like **Sweep**, which auto-generates pull requests from GitHub issues, and **Copilot Workspace**, which can create and manage entire features autonomously, allow small teams to maintain high output. These tools handle boilerplate tasks and bug fixes, freeing up developers to focus on high-impact product features.#### **Enterprise: Using Cody + Tabnine for Large, Secure Codebases**For enterprises with massive codebases and stricter security requirements, **Cody** (from Sourcegraph) and **Tabnine** offer scalable AI integration. Cody excels at understanding large repositories and referencing documentation or historical code changes. Tabnine, known for its privacy-first model, ensures that proprietary code doesn’t leave company boundaries, making it ideal for secure enterprise environments.#### **Data Scientist: Speeding Up EDA with Cogram + CodeSquire**Data scientists benefit greatly from tools that can reduce time spent on repetitive analysis. **Cogram** helps generate SQL queries and Jupyter code based on natural language, while **CodeSquire** personalizes code suggestions based on the user’s unique style and workflows. These tools accelerate exploratory data analysis (EDA) and modeling, letting scientists focus more on insights and less on syntax.* * *## Final Thoughts2025 isn’t about whether you use AI as a developer—it’s about how. The tools listed above aren’t just trendy—they’re reshaping the development experience by embedding intelligence into your daily workflow. Some extend your capabilities. Others remove tedious work. The best ones do both.Choose the ones that match your stack, integrate with your workflow, and make you feel like a better developer—not just a faster one.* * *Sentry]( PromotedDropdown menu- - * * *- ]( Read More!Templates let you quickly answer FAQs or store snippets for re-use.SubmitPreview CollapseExpand]( David]( David]( David![]( of Postiz, an open-source social media scheduling tool.Running Gitroom, the best place to learn how to grow open-source tools.- EducationDidn\\'t finish high school :(- PronounsNev/Nevo- WorkOSS Chief @ Gitroom- JoinedFeb 23, 2022• Dropdown menu- - Hide- Neat how coding is getting easier, like having a little helper doing chores for you. But if AI keeps taking on bigger parts of the job, what skills should people still work on to stay sharp in tech?CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Totally! As AI handles more, staying sharp in problem-solving, system design, and critical thinking is key. Knowing why things work still sets you apart.CollapseExpand]( Sečkár]( Sečkár]( Sečkár![]( am from Czechia, born in 1988. Studied computer science at KIT VŠE, Prague. Currently working as senior Java & JS developer for Eviden.- Email- LocationPrague, Czechia- EducationKIT VŠE, Prague- PronounsHe- WorkSenior application developer @ Eviden- JoinedOct 21, 2023• Dropdown menu- - Hide- business analysis, software architecture, observability, scalability, security...to name a fewCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- And to continue on the reply of , convert abstract requirements to good software requirements is still key, AI can only hallucinate solutions that most likely be wrong or wrongly translated.Software architecture and applying best practices is also still important, AI only generate some code which might not scale very well. Or doesn\\'t fit the overal design, or worse it\\'s using deprecated functionality. So a human developer should be really keen on it and not just accept all the code. How well it even might look like. It\\'s still just a tool.CollapseExpand]( Lopez]( LopezAudrey Lopez]( JoinedMar 6, 2025• Dropdown menu- - Hide- Nice post, Emmanuel Mumba!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Thanks Lopez! Glad you enjoyed it.CollapseExpand]( Sebastian]( SebastianRalph Sebastian]( JoinedAug 10, 2023• Dropdown menu- - Hide- Nice list. By the way, Windsurf has free GPT 4.1 access for now (it will not last long), worth trying out.CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Oh nice, thanks for the heads-up! I’ll definitely check Windsurf out before the window closes.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Openai is indeed delivering free gpt 4.1 while in preview.But also o4-mini models I believe. Mini-high, Etc.CollapseExpand]( Lee]( LeeJennie Lee]( JoinedDec 18, 2024• Dropdown menu- - Hide- OK, vibe coding is getting hot this time!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Haha, for real! 🔥 Coding’s definitely having its moment, More like the vibe is leveling up every week.CollapseExpand]( the Coder]( the CoderJosh the Coder]( Coder that is Interested in AI, No Code. Learning forever!- JoinedNov 27, 2023• Dropdown menu- - Hide- OK, this is a good list. I am using Cursor but also willing to explore some alternatives. Good work Emmanuel!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Appreciate that! 🙌 Cursor’s solid, always cool to explore what else is out there too. Let me know if you find any gems.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Windsurf is really good. Since they at least providea decent deb file setup, causing less issues on my linux setup than the stupid appimages from cursor.CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- I\\'ve spent more time trying to fix our juniors vibe coded problems than I can imagine. And what\\'s worse is having to explain why. They have no idea what the code they are pumping out does. It\\'s a horrifying trend. Hopefully this doesn\\'t spread to all industries and we end up with vibe built airplanes LOLCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Haha I was already afraid this will happen 2 years ago. And now we are at this stage where agents are commonly available and used in editors.It\\'s powerful for experienced devs. But juniors should really not just using it and blindly accept it. That is not how you grow in your knowledge or carrière. Instead of you fixing their issues, they should take full responsibility of the code they put out for review.CollapseExpand]( Orion]( OrionAshe Orion]( JoinedAug 12, 2024• Dropdown menu- - Hide- Okay so as a developer in the making, at what point in my learning process will these tools stop being a hindrance to my education and start being actual real usable tools and not like Nintendo 64 cheat codes because right now, having nothing but HTML CSS and some very intermediate vanilla JavaScript at my disposal (and only as it relates to the front-end. Getting at back end soon) even just the Codium Auto-complete in VS was detracting from my ability to build a solid foundation by typing through my full lines of code. I noticed that when I had auto complete enabled, pretty much immediately anytime I was working in other environments without an autocomplete buddy by my side my syntax absolutely suffered because my monkey brain wasn\\'t getting the muscle memory of typing out the proper syntax and to be honest it was a little embarrassing in Zoom meetings with my tutors to just sit there and fumble multiple times on things like spacing or back ticks or, dare I out myself, neglected line termination in ; and }However I definitely see the value of not having to type out very obvious return statements or whatever other repetitive this.thing = thething; -esque scenarios that when you add them all up really do burn giant holes your productive work hours. I\\'m just so terrified of using it too early, leaving the safety of being an \\'aspiring engineer\\' for the reality of the job market, finally landing an interview only to realize that while I thought I was learning to code I\\'d just been surfing the vibe.CollapseExpand]( Enlightened]( EnlightenedAaron Enlightened]( Developer using AI tools for programming but not depending on them completely!- JoinedApr 18, 2025• Dropdown menu- - Hide- LOL ... I know exactly how you feel! I am at a similar junction in my programming journey and this is what I do to not feel like an imposter later on - Open a plain .txt file and just type some HTML and CSS code (along with vanilla JavaScript if you want to) into it, then save the file as \"index.html\", and then open this file in Google Chrome browser and see the result. To modify or edit any code that you feel is giving unexpected results, just use the Chrome Developer tools. This will give you confidence in YOUR coding prowess with ZERO autocomplete using any type of AI. Do this for just 15-20 minutes every day for 1 month and you will have solved the imposter syndrome to a large extent.For all the rest of your tutorials use the autocomplete feature on VSCode Editor, but try to ask ChatGPT \"why\" was some particular code snippet suggested by the AI autocomplete. This will help you in understanding the WHY of AI coding and will help make you a better programmer.BTW, I signed up on DEV just to post this comment. So, thank you for that. ;-)CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- It\\'s a tough gig for new devs, you are entering a world where tools exist that actually hinder your understanding of what\\'s going on.At the risk of showing my age, id say it\\'s akin to why the monk will make the initiate carry pales of water to then monastery before he learns the king fu. To generate an understanding of the fundamentals of movement, patience and discipline.Some people a lot older than me would say using an IDE is not true development and that real devs can code in notepad.Tools evolve and as long as they fill the purpose of making coding easier, but still allow you to learn and gain the knowledge and understanding I wouldn\\'t see a problem with using them. But as you pointed out, it can be a hindrance, and you actually realizing that and accepting it means you will succeed where many of your peers won\\'t.AI is just another tool, it\\'s great when you already have an understanding of how systems should work and just want some snippets. But the problem we are seeing is a misuse of these tools. The blind copy paste vibe coding is not creating developers it\\'s creating slop.Here\\'s my ultimate advice though, AI doesn\\'t have to be just a tool, it can be an amazing teacher. It can explain things to you in anyway you want , if you ask it to simplify and breakdown topics it will. It can explain things to you as if you were a toddler. USE THIS.Vibe code your heart out but every single time it gives you code you do not understand, ask it to explain it you. You will be amazed how quickly it goes from hindrance to game changing.CollapseExpand]( jones]( jonessam jones]( JoinedApr 16, 2025• Dropdown menu- - Hide- Very informative article. Although not at all what I expected from title \"Vibe Coding\". In industry, Vibe Coding is being used to signify low code/no code or citizen development. Basically, the same holy grail that the industry has been searching for since Oracle Forms or earlier.BUt putting the Title aside, looking forward to trying out some of your suggestions.CollapseExpand]( Mottley]( MottleyDaley Mottley]( Consultant | Full Stack Web Developer- Email- LocationBarbados- JoinedOct 8, 2024• Dropdown menu- - Hide- Nice post, Emmanuel!Have you tried ?It\\'s a pretty decent vibe coding tool. I love it... 😅Thanks for sharing 👍CollapseExpand]( Green]( GreenStuart Green]( JoinedApr 18, 2025• Dropdown menu- - Hide- Crazy times ahead. Can only imagine the downstream impact these tools will have (personalized software, rise of individual creators, etc).I also love platforms like  and  for \"vibe coding\" without the actual code. Though harder to build more advanced features than with Cursor, etc.Some comments may only be visible to logged-in visitors.  to view all comments.Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\'s .Hide child comments as wellConfirmFor further actions, you may consider blocking this person and/or Embeddable]( PromotedDropdown menu- - * * *- ]( Embed in minutes, load in milliseconds, extend infinitely. Import any chart, connect to any database, embed anywhere. Scale elegantly, monitor effortlessly, CI/CD & version control.👋 Kindness is contagiousDropdown menu- - * * *- CloseDive into this thoughtful piece, beloved in the supportive DEV Community. **Coders of every background** are invited to share and elevate our collective know-how.A sincere \"thank you\" can brighten someone\\'s day—leave your appreciation below!On DEV, **sharing knowledge smooths our journey** and tightens our community bonds. Enjoyed this? A quick thank you to the author is hugely appreciated.### !We\\'re a place where coders share, stay up-to-date and grow their careers. ![]('}]\n",
      "Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 8, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "---------- TextMessage (research_analyst) ----------\n",
      "보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 요약\n",
      "\n",
      "Executive Summary\n",
      "- 바이브 코딩(감정/환경 인식 기반 코딩 및 인터랙티브 코딩)은 AI 보조 코딩의 확장된 개념으로 주목받고 있으며, 개발자와 비개발자 간의 협업 방식과 개발 생산성에 새로운 기회를 열고 있습니다.\n",
      "- 현재 주요 트렌드는 AI 코드 생성 도구의 급속한 확산, 코드 작성 보조의 고도화, IDE/에디터의 AI 내재화, 그리고 AI 에이전트의 활용 확대입니다. 다국적 대기업-스타트업 간 경쟁 구도가 심화되고 있습니다.\n",
      "- 개발자의 미래에 대해선 “대체”보다 “보조-확대”의 방향이 주류이며, 개발자의 역할은 시스템 설계, 아키텍처, 보안, 복합 문제 해결 등 고부가 가치 영역으로 재편될 가능성이 큽니다. 다만 초입력 개발자(엔트리 레벨) 일자리의 변화 가능성과 훈련/교육의 적응이 중요한 도전 과제로 남습니다.\n",
      "- 향후 교육·기업 전략 측면에서 감정인식/Affective Computing, 윤리 및 데이터 보안, 문화적 맥락의 고려 등 새로운 연구 주제가 핵심 이슈로 부상할 전망입니다.\n",
      "\n",
      "Background & Current State\n",
      "- 바이브 코딩의 정의 및 현황\n",
      "  - 바이브 코딩은 자연어/감정 기반의 코드 작성 지원 및 대화형 인터페이스를 통해 프로그래밍을 보조하는 개념으로 확장되고 있습니다. Databricks의 레이크베이스(Lakebase) 출시 및 Agent Bricks/AI 판사 같은 기능들이 데이터-AI 워크플로우를 하나의 플랫폼에서 통합하려는 시도를 보여줍니다.\n",
      "  - 바이브 코딩은 데이터+AI 생태계의 통합, 비즈니스 프로세스 자동화, 그리고 AI 코드 생성 도구의 통합으로 기업 생산성 증대에 기여합니다.\n",
      "- 최근 개발 동향 및 주요 데이터\n",
      "  - 글로벌 개발자 커뮤니티에서 AI 도구의 채택이 빠르게 확산되고 있으며, Stack Overflow의 2025 개발자 서베이 결과에 따르면 84%의 응답자가 AI 도구를 사용 중이거나 향후 사용할 계획이며, 일부 규모의 개발자는 매일 AI 도구를 사용합니다(일부 직군에서 50% 이상이 일상적으로 사용).\n",
      "  - 다수의 기업/스타트업이 AI 코드 생성 도구에 대한 투자를 확대하고 있으며, Cursor, Windsurf, Codeium 등의 스타트업 기업가치가 수십억 달러 규모로 평가되는 사례가 있습니다.\n",
      "  - 엔트리 레벨 개발자 채용시장에선 AI 도입으로 반복 작업이 감소하는 등 일자리 구조의 변화가 나타나고 있으며, 빅테크가 자체 AI 코딩 도구를 확산시키고 있습니다.\n",
      "- 관련 데이터 및 현장 인사이트(발췌)\n",
      "  - 2025 Stack Overflow Developer Survey: AI 도구의 사용 증가, 일상 사용 비중 상승, 도구의 정확성 및 보안에 대한 우려 증가, 복잡한 작업에서의 신뢰성 이슈 지적.\n",
      "  - 데이터코어 및 스타트업 생태계 소식: Cursor(가치평가 약 9B 달러), Windsurf/Codeium(가치평가 약 3B 달러) 등, AI 코드 생성 도구의 상용화 및 투자가 활발.\n",
      "  - 감정 인식/ affective computing 연구의 확산: 교육 현장, 산업 적용, 데이터셋(DevEmo, DEAP 등) 및 국제 표준(IS/IEC 30150-1 등) 이슈가 제시됨.\n",
      "- 한계 및 주의점\n",
      "  - AI 도구의 정확도, 보안, 데이터 프라이버시 이슈는 여전히 큰 걱정거리이며, 인간의 검증이 여전히 필요합니다.\n",
      "  - 바이브 코딩의 문화적/사회적 맥락에서의 차이를 고려한 접근이 필요합니다. 특정 문화권에서의 감정 표현 차이가 시스템 설계에 영향을 줄 수 있습니다.\n",
      "\n",
      "Analysis & Insights\n",
      "- 주요 트렌드\n",
      "  - AI-보조 코딩에서 AI-주도 코딩으로의 이동이 진행 중이나, 인간 개발자의 역할은 여전히 필요합니다. 대규모 시스템 설계, 보안, 아키텍처 의사결정, 도메인 지식의 적용 등은 인간의 영역으로 남습니다.\n",
      "  - 에이전트 기반 협업의 확산: 에이전트 브릭스/AI 판사 등 기능이 도입되며, 대화형 명령으로 정보 추출, 자동 코드 작성, 품질 관리가 가능해집니다. 다수의 도구가 IDE 내 협업/배포/모니터링까지 연결됩니다.\n",
      "  - 감정인식 기반의 개발환경: 개발 학습/실무 환경에서 감정 데이터를 활용한 적응형 학습, 몰입도 향상, 학습 효과 개선에 대한 연구가 축적되고 있습니다. 다만 프라이버시 및 문화적 차이에 대한 규범이 필요합니다.\n",
      "- 다양한 관점\n",
      "  - 기술-생태계 관점: 대형 플랫폼(예: Databricks)와 스타트업 간의 경쟁+협력 구조가 형성되어 있으며, Lakebase 같은 플랫폼은 데이터+AI 워크플로우의 통합을 촉진합니다.\n",
      "  - 산업-경제 관점: AI 코드 생성 도구에 대한 투자 증가 및 스타트업의 고밸류에이션 현상은 시장의 성장 잠재력을 시사합니다. 엔트리레벨 개발자 일자리가 변동될 가능성과 대규모 기업의 자동화 도입은 고용시장에 직접적인 영향을 줄 수 있습니다.\n",
      "  - 윤리-정책 관점: 데이터 프라이버시, 보안, 문화적 맥락의 반영, 표준화의 필요성 등은 향후 연구/정책의 핵심 이슈가 됩니다.\n",
      "- 전문가 의견 및 현장 견해\n",
      "  - 전문가들은 “AI는 개발자의 보조 도구”로 자리매김하며, 인간은 설계, 아키텍처, 보안, 도메인 전문성 측면에서 주도적 역할을 유지해야 한다는 입장을 공통적으로 제시합니다.\n",
      "  -emotion-aware/감정 인식 연구는 교육 및 협업 도구의 개선에 기여할 수 있으나, 구현의 복잡성과 윤리적 쟁점으로 인해 도입에 신중해야 한다는 시각이 많습니다.\n",
      "  - AI 에이전트의 채택은 생산성 증가 및 학습 가속에 긍정적이지만, 협업의 질이나 팀 내 커뮤니케이션에 미치는 영향에 대한 재평가가 필요합니다.\n",
      "- 도전 요소\n",
      "  - 신뢰성 문제: \"Almost right\" 한 AI 결과를 디버깅하는 데 소요되는 시간과 노력이 여전히 큽니다.\n",
      "  - 데이터 관리 및 보안: 에이전트 기억(memory) 관리, 데이터 접근, 로그 보안 등은 엔터프라이즈 환경에서 핵심 이슈입니다.\n",
      "  - 문화/윤리: 서로 다른 문화적 맥락에서의 감정 표현 차이에 맞춘 UX/대화 전략 필요.\n",
      "  - 교육/재훈련: 개발자의 기술적 재교육 필요성 증가. 특히 고급 아키텍처 설계/보안/데브옵스 관점의 역량 강화가 중요합니다.\n",
      "\n",
      "Future Outlook\n",
      "- Emerging trends\n",
      "  - 에이전트-주도적 개발: IDE 내 에이전트가 코드 생성, 버그 수정, 테스트 작성까지 자동화하는 흐름이 확산될 가능성.\n",
      "  - 멀티모달 감정 인식의 확장: 텍스트/음성/비주얼/생리 신호를 결합한 감정 인식으로 개발 환경의 적응성과 협업 품질이 개선될 전망.\n",
      "  - 문화적 맥락 반영: 글로벌 확산 속에서 지역별 문화 차이를 고려한 감정 모델과 인터페이스 표준화 필요성이 증가.\n",
      "  - 교육-기업 연결 강화: affective computing/감정인식 기반 학습 도구를 활용한 실무형 교육 커리큘럼 확산.\n",
      "- Predictions\n",
      "  - 2030년대 초반까지 반복적이고 보일러플레이트 수준의 코딩은 AI 도구가 상당 부분 대체하거나 보조하는 수준에 이를 가능성 큼.\n",
      "  - 고급 개발자 역할은 시스템 설계, 보안, 아키텍처 의사결정, 도메인 지식의 적용에 더 집중될 것이고, 개발 생태계의 협업 도구는 더 통합적이고 자동화된 흐름을 제공할 것.\n",
      "  - 교육 체계는 AI 도구 사용법뿐 아니라 윤리, 보안, 문화적 맥락 이해를 포함한 AI-리터러시를 강화하는 방향으로 재편될 가능성.\n",
      "- Implications\n",
      "  - 기업: 데이터/AI 워크플로우의 통합 투자 확대, 에이전트-기반 협업 도구 도입, 보안·규정 준수 체계 강화 필요.\n",
      "  - 교육: 감정 인식·사회적 상호작용의 이해를 포함한 교육과정 설계, 재교육/직무전환 프로그램 강화.\n",
      "  - 정책/윤리: 개인정보보호, AI 투명성, 책임소재, 데이터의 편향성 문제를 다루는 국제 표준 및 기업 내부 정책의 필요성 증가.\n",
      "\n",
      "Recommendations (실무 및 정책 제안)\n",
      "- 기업 전략\n",
      "  - AI 도구 채택 로드맷 수립: 어떤 코드 작성 단계에서 어떤 도구를 언제 사용할지 명시하고, 인간 검증 프로세스를 병행.\n",
      "  - 에이전트 거버넌스 구축: 에이전트의 메모리/데이터 접근 제어, 감사 로그, 보안 규정 준수 확보.\n",
      "  - 문화/윤리 가이드라인: 감정 인식 기반 UX 설계 시 문화 간 차이를 고려한 정책 수립.\n",
      "- 교육-인재 전략\n",
      "  - AI 도구 활용 역량 강화 교육: 프레이밍된 과제에서의 도구 사용법, 리뷰 및 디버깅 전략, 보안/윤리 의식 교육 포함.\n",
      "  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계, 보안, 성능 최적화에 초점을 맞춘 훈련 강화.\n",
      "  - 감정 인식 교육의 도입: 교육 현장에서의 감정 데이터 활용 윤리와 데이터 관리 교육 포함.\n",
      "- 연구 방향\n",
      "  - 감정 인식과 코드 품질 간의 관계 연구: 감정 상태가 개발 생산성과 코드 품질에 미치는 영향 분석.\n",
      "  - 다문화/다언어 환경에서의 감정Affective 모델 비교 연구.\n",
      "  - 에이전트-협업의 효과성 분석: 팀 성과, 학습효과, 협업 품질에 대한 실증 연구.\n",
      "\n",
      "Sources\n",
      "- 데이터/동향 및 기술 현황\n",
      "  - Stack Overflow Developer Survey 2025: AI 도구 사용 현황, 일상 사용 비율, 업무 신뢰도 및 복잡도 처리에 대한 인식\n",
      "  URL 예시: https://survey.stackoverflow.co/2025/ai (발췌 요약)\n",
      "- 바이브 코딩 및 스타트업 생태계\n",
      "  - 2025년 바이브 코딩/AI 개발 동향 보고서(예시 기사/리포트): Databricks의 바이브 코딩 개념, Lakebase 출시 및 에이전트 도구\n",
      "  - Cursor, Windsurf, Codeium 등 스타트업 밸류에이션 및 투자현황\n",
      "  - 예시 기사/리포트: TokenPost, Nate News, Unite.AI, Dev.to 등(요지 요약)\n",
      "- AI 코드 생성 도구 및 비교\n",
      "  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 활용 사례\n",
      "  - Repl.it/에이전트 협업 기능, 에이전트-브릭스/AI 판사 등 기능 설명\n",
      "- 감정 인식/ affective computing 연구\n",
      "  - Real-time emotion aware applications: 사례 연구 및 GPU 병렬처리 가속\n",
      "  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\n",
      "  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing/Computing)\n",
      "  - DevEmo 데이터셋, AffecTube 등 데이터셋 및 교육 연구\n",
      "- 관련 정책 및 표준\n",
      "  - ISO/IEC 30150-1:2022 (Affective computing user interface)\n",
      "  - 감정 인식 기술의 윤리·데이터 프라이버시 이슈 관련 논의\n",
      "- AI와 개발자 직무의 미래에 관한 분석/리뷰\n",
      "  - AI가 개발자 직무에 미치는 영향에 대한 종합 리뷰 및 2025년 트렌드 요약\n",
      "  - AI 에이전트의 생산성 영향 및 협업 효과에 관한 설문/연구\n",
      "- 참고 기사 예시(한국어/영문)\n",
      "  - 2025 바이브 코딩 관련 기사 모음(예: TokenPost, 네이트 뉴스)\n",
      "  - 2025 AI 코드 생성 도구 비교 기사(EliteBrains, Dev.to, Azumo 등)\n",
      "\n",
      "마무리\n",
      "- 본 보고서는 바이브 코딩의 개념적 확장과 AI 코드 생성 도구의 확산이 소프트웨어 개발자의 미래에 미치는 영향을 다각적으로 조망했습니다. 주요 시사점은 기술의 보조적 역할 강화, 인간 고유의 설계/보안/도메인 전문성의 중요성 유지, 그리고 감정/문화적 맥락을 반영한 윤리적이고 책임 있는 도입이 필요하다는 점입니다.\n",
      "- 필요하신 경우, 위의 핵심 주제(2–3개)와 각 주제에 대한 구체적 하위 주제, 그리고 3–5개의 최신 자료 목록으로 구성된 맞춤형 FOCUSED 연구 계획서를 추가로 작성해 드리겠습니다.\n",
      "\n",
      "REPORT_COMPLETE\n",
      "---------- TextMessage (research_analyst) ----------\n",
      "제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\n",
      "\n",
      "Executive Summary\n",
      "- 정의와 맥락: 바이브 코딩은 분위기/감정 인식 기반의 코딩 방식과 인터랙티브한 개발 환경을 포괄하는 확장 개념으로 이해되며, AI 기반 코드 작성 보조와 데이터-AI 플랫폼의 통합 트렌드를 아우릅니다.\n",
      "- 현황 요약: AI 보조 코딩 도구의 채택이 급격히 확산 중이며, 대기업-스타트업 간의 생태계 경쟁이 심화하고 있습니다. 커뮤니티 기반의 협업 도구, 에이전트 기능, 그리고 플랫폼 차원의 데이터-AI 워크플로우 통합이 두드러집니다.\n",
      "- 소프트웨어 개발자의 미래: AI 보조 도구는 개발자 업무의 생산성과 속도를 높이되, 인간 개발자의 핵심 역할은 시스템 설계, 아키텍처, 보안, 도메인 전문성 등 고부가 가치 영역으로 재편될 가능성이 큽니다. 특히 초보 개발자 진입 및 직무전환 측면에서 교육 체계의 신속한 적응이 중요한 과제로 남습니다.\n",
      "- 시사점: 교육, 기업 거버넌스, 윤리/보안 표준의 정교화가 필요하고, 문화적 맥락을 반영한 감정 기반 UX의 도입과 데이터 프라이버시 관리가 중요합니다.\n",
      "\n",
      "Background & Current State\n",
      "- 현재 개발 풍경\n",
      "  - 바이브 코딩은 감정/상황 인식 기반의 코드 생성과 업무 자동화를 포함하는 포괄적 모형으로 진화 중이며, 데이터 생태계의 통합과 AI 자동화 도구의 결합이 중심 축입니다. 예를 들어 Databricks의 레이크베이스(Lakebase)와 관련 생태계, 에이전트 빌딩(Agent Bricks) 및 AI 판사(judges) 같은 기능이 데이터-AI 워크플로우의 혁신적 연결고리로 주목됩니다.\n",
      "  - 스타트업 생태계: Cursor, Windsurf, Codeium 같은 AI 코딩 도구 관련 스타트업의 가치 평가가 수십억 달러 수준에 이르는 사례가 늘고 있습니다. 이는 AI 코드 생성 시장의 성장성과 투자 열기를 시사합니다.\n",
      "- 최근 developments 및 데이터 포인트\n",
      "  - 대규모 개발자 커뮤니티의 채택 증가: Stack Overflow의 2025 개발자 서베이에서 AI 도구를 사용하는/사용 계획이 80%대에 이르는 부문이 다수였고, 일부 개발자는 매일 AI 도구를 사용합니다.\n",
      "  - AI 도구에 대한 인식과 신뢰도: 전문 개발자층에서 AI 도구의 긍정적 효과에 대한 인식은 높지만, 도구의 정확성·보안·프라이버시 이슈에 대한 우려도 여전히 큽니다.\n",
      "  - 엔트리 레벨 개발자 고용환경: AI 도입으로 반복적·보일러플레이트 작업이 감소하는 한편, 대형 IT 기업들은 자체 AI 코드 생성 솔루션을 확산시키고 있습니다.\n",
      "- 주요 통계 및 데이터 포인트(요약)\n",
      "  - Stack Overflow 개발자 설문 2025: 약 84%가 AI 도구를 사용 중이거나 곧 사용할 계획. profesionals의 경우 일부에서 매일 사용 비중이 높음.\n",
      "  - AI 도구에 대한 신뢰도: 고급 숙련자일수록 도구의 신뢰도에 대해 신중하며, 완전한 자동화를 기대하기에는 추가 검증이 필요하다는 입장 증가.\n",
      "  - 에이전트 활용: 전체 응답의 상당수가 에이전트를 작업에 도입 중이거나 도입 계획을 가지고 있으나, 에이전트 기억(memory) 관리, 보안, 관측 가능성 등의 이슈가 남아 있습니다.\n",
      "  - 바이브 코딩의 실무성 인식: 다수 응답자가 바이브 코딩을 “주류 작업의 일부”로 보지 않고, 아직은 도입 초기 또는 비주류 흐름으로 간주하는 경향이 높습니다.\n",
      "- 용어 정리\n",
      "  - 바이브 코딩: 분위기/감정 인식 기반의 코딩 또는 Chat/자연어 명령을 바탕으로 한 대화형 개발 도구를 포함하는 개념.\n",
      "  - Lakebase: 데이터+AI 워크플로우를 한 플랫폼에서 처리하도록 설계된 서버리스 OLTP 데이터베이스.\n",
      "  - 에이전트 빅스(Agent Bricks) 및 AI 판사: 대화형 명령으로 AI 에이전트를 생성/관리하고, 생성된 결과의 품질을 모니터링하는 도구.\n",
      "  - 감정 인식(Affective Computing) 연구: 교육, 인터랙션 디자인, 멀티모달 감정 인식 등을 중심으로 한 연구 분야.\n",
      "\n",
      "Analysis & Insights\n",
      "- 주요 트렌드\n",
      "  - AI-보조 코딩의 강화: 코드 작성의 보조에서 벗어나 다층적 작업(요구 분석, 코드 생성, 테스트, 문서화)을 자동화하는 방향으로 도구가 발전하고 있습니다.\n",
      "  - 에이전트-기반 협업의 확산: IDE 내에서 에이전트가 실시간으로 협업하고, 코드 품질 관리까지 참여하는 사례가 늘고 있습니다.\n",
      "  - 데이터 플랫폼의 통합: 데이터 관리와 AI 개발이 하나의 워크플로우로 결합되는 사례가 증가하고 있으며, Lakebase와 같은 플랫폼의 채택이 확산될 것으로 보입니다.\n",
      "  - 감정/문화적 맥락의 고려 필요성: 감정 인식의 UX 설계에서 문화적 차이를 반영하고, 프라이버시/윤리 이슈를 관리하는 것이 핵심 이슈로 부상합니다.\n",
      "- 다양한 관점\n",
      "  - 기술 관점: 개발 생산성 향상, 코드 품질 보장, 대규모 코드베이스의 자동화된 관리 등 긍정적 효과가 예상됩니다. 다만 복잡한 문제 해결과 설계 의사결정은 여전히 인간 주도 영역입니다.\n",
      "  - 교육 관점: AI 도구의 활용은 학습 속도와 접근성을 높이지만, 기초 프로그래밍 원리의 이해를 저해하지 않도록 교육 설계가 필요합니다.\n",
      "  - 사회/정책 관점: 프라이버시 보호, 데이터 소유권, 알고리즘의 편향성 관리, 국제 표준 정립이 중요합니다.\n",
      "- 전문가 의견 및 현장 견해\n",
      "  - 전문가 다수는 “AI는 개발자의 보조 도구”로 보며, 시스템 설계/아키텍처, 보안, 도메인 지식의 융합 능력이 앞으로의 핵심 역량이라고 봅니다.\n",
      "  - 감정 인식 분야 연구자들은 교육 현장과 산업 적용에서의 잠재력은 크지만, 구현의 복잡성과 윤리적 문제에 대한 신중한 접근이 필요하다고 지적합니다.\n",
      "  - 스타트업 및 대기업 간의 협력/경쟁 구도가 강화되며, 데이터/AI 생태계의 통합에 대한 투자와 전략적 파트너십이 중요하다고 봅니다.\n",
      "- 도전 과제\n",
      "  - 정확도와 신뢰성: AI가 생성한 코드의 품질에 대한 인간의 검증 필요성은 여전합니다.\n",
      "  - 보안/프라이버시: 에이전트 기억(memory) 관리와 데이터 접근 제어, 감사 가능성 확보가 필수입니다.\n",
      "  - 문화적 차이: 글로벌 채택에서 문화적 맥락 차이가 UX 설계에 반영되어야 합니다.\n",
      "  - 교육 체계의 적응: 재교육/직무전환 프로그램의 설계와 실행이 중요합니다.\n",
      "\n",
      "Future Outlook\n",
      "- Emerging trends\n",
      "  - AI 에이전트 주도 개발의 확대: IDE 내에서 에이전트가 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 방향으로 발전 가능.\n",
      "  - 멀티모달 감정 인식의 확장: 텍스트, 음성, 얼굴 표정, 생리 신호 등을 결합한 감정 인식이 개발 환경의 적응성 및 협업 품질을 향상시킬 전망.\n",
      "  - 문화적 맥락의 반영: 지역/문화별 감정 표현 차이를 반영한 인터페이스 및 모델 표준화 필요성이 커짐.\n",
      "  - 교육-산업 연계 강화: affective computing/감정인식 기반 학습 도구가 현장 직무 교육에 통합될 가능성 증가.\n",
      "- Predictions\n",
      "  - 향후 5–10년간 반복적이고 보일러플레이트 수준의 코딩은 AI 도구가 더욱 많이 대체하거나 보조하는 수준으로 발전할 가능성 큼.\n",
      "  - 개발자의 역할은 고급 설계, 아키텍처 의사결정, 보안/컴플라이언스, 도메인 전문성 중심으로 재편될 가능성.\n",
      "  - 교육 체계는 AI 도구의 사용법뿐 아니라 윤리/보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 재구성될 필요가 증가.\n",
      "- Implications\n",
      "  - 기업: 데이터/AI 워크플로우의 통합 확대, 에이전트 관리 거버넌스 구축, 보안·윤리 정책 강화 필요.\n",
      "  - 교육: 감정 인식 기반 교육 도입 및 재교육 프로그램 설계, 시스템적 사고/아키텍처 역량 강화 필요.\n",
      "  - 정책/표준: 데이터 프라이버시, 책임소재, 국제 표준 정립 등 정책적 규범의 정교화가 중요해질 것.\n",
      "\n",
      "Recommendations (실무-교육-정책 측면 제안)\n",
      "- 기업 전략\n",
      "  - AI 도구 채택 로드맵 수립: 어느 단계에서 어떤 도구를 도입하고, 언제 인간의 검증을 필수로 할지 명확한 프로세스 수립.\n",
      "  - 에이전트 거버넌스 및 데이터 보안: 메모리 관리, 접근 제어, 감사 로그, 보안 표준 준수 체계 확립.\n",
      "  - 문화/윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영한 정책 및 교육.\n",
      "- 교육-인재 전략\n",
      "  - AI 도구 활용 역량 중심 교육: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함한 교육과정 설계.\n",
      "  - 시스템적 사고 강화: 아키텍처 설계, 성능 최적화, 보안 등 고급 역량 강화 교육 강화.\n",
      "  - 감정 인식 연구·교육의 도입: 데이터 관리와 윤리 교육을 함께 다루는 커리큘럼 개발.\n",
      "- 연구 방향\n",
      "  - 감정 상태와 코드 품질/생산성의 관계 실증 연구.\n",
      "  - 다문화/다언어 맥락에서의 감정 모델 비교 연구.\n",
      "  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구.\n",
      "\n",
      "Sources\n",
      "- Stack Overflow Developer Survey 2025: AI 도구 사용 현황, 일상 사용 비율, 신뢰도/복잡도 처리 인식 등\n",
      "  - 예시: https://survey.stackoverflow.co/2025/ai\n",
      "- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\n",
      "  - 데이터/동향: Databricks의 바이브 코딩 컨셉, Lakebase 출시 등의 보도\n",
      "  - 스타트업 사례 및 밸류에이션: Cursor, Windsurf, Codeium 등\n",
      "  - 기사 예시: TokenPost, Nate News, Unite.AI, Dev.to 등\n",
      "- AI 코드 생성 도구 비교 및 활용 사례\n",
      "  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter, Cursor, Windsurf 등\n",
      "  - Replit의 에이전트 기능, 협업 기능 등\n",
      "- 감정 인식/Affective Computing 연구 및 데이터셋\n",
      "  - Real-time emotion aware applications(연구 사례), Towards Emotion-awareness in Programming Education(IEEE Xplore)\n",
      "  - Affective Computing: Recent Advances, Challenges, and Future Trends( Intell Comput, 2024)\n",
      "  - DevEmo, AffecTube, DEAP, SEED 등 데이터셋/연구\n",
      "- 표준 및 정책\n",
      "  - ISO/IEC 30150-1:2022, Affective computing user interface\n",
      "  - 감정 인식 기술의 윤리 및 데이터 프라이버시 논의\n",
      "- 미래 직무 및 업계 분석 리뷰\n",
      "  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈에 관한 최신 리뷰 및 설문\n",
      "  - AI가 직무에 미치는 영향에 관한 종합 연구 및 현장 인터뷰\n",
      "\n",
      "REPORT_COMPLETE\n",
      "---------- TextMessage (research_analyst) ----------\n",
      "보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\n",
      "\n",
      "Executive Summary\n",
      "- 정의 확장: 바이브 코딩은 분위기/감정 인식 기반의 코딩 및 인터랙티브한 개발 환경을 포괄하는 개념으로, AI 보조 코딩의 진화와 데이터-AI 워크플로우의 통합 흐름을 함께 다룬다.\n",
      "- 현황 요약: AI 코드 생성 도구의 급속한 확산, IDE/에디터의 AI 내재화, 에이전트 기반 협업 기능의 확대가 두드러진 현상이다. Databricks의 레이크베이스 같은 플랫폼 출시와 스타트업(Cursor, Windsurf, Codeium 등)의 밸류에이션 상승이 시장의 활황을 시사한다.\n",
      "- 소프트웨어 개발자의 미래: 개발자의 역할은 “보조 도구의 활용과 고부가 가치 영역의 설계/아키텍처/보안”으로 재편될 가능성이 크다. 엔트리 레벨 개발자 취업 환경은 AI 도입으로 변화하고 있으며, 재교육/직무 전환이 중요한 과제다.\n",
      "- 시사점: 교육 체계의 현대화, 데이터 프라이버시 및 보안 거버넌스 강화, 문화적 맥락을 반영한 UX/윤리 표준 확립이 필요하다.\n",
      "\n",
      "Background & Current State\n",
      "- 현장 landscape\n",
      "  - 바이브 코딩은 자연어 기반 코드 생성과 업무 자동화를 넘어, 데이터-플랫폼 서비스와 AI 자동화 도구를 하나의 생태계로 연결하려는 방향으로 발전하고 있다.\n",
      "  - Lakebase(데이터브릭스의 서버리스 OLTP 데이터베이스)와 Agent Bricks, AI 판사(Judges) 등의 기능이 데이터-AI 워크플로우의 혁신적 통합을 촉진한다.\n",
      "- 최근 developments\n",
      "  - 대기업-스타트업 생태계의 경쟁 구도 강화: Cursor, Windsurf, Codeium 등 AI 코드 생성 스타트업의 밸류에이션 상승 및 투자 증가.\n",
      "  - 플랫폼 중심의 협업 도구 강화: Replit, Cursor, Windsurf 등의 협업/에이전트 기능으로 팀 간 개발 흐름이 더욱 연결되고 있다.\n",
      "  - 졸업/학계 트렌드: 감정 인식(Affective Computing) 연구 및 교육 현장의 활용 사례 증가, DevEmo, DEAP 등 데이터셋과 국제 표준(IS/IEC 30150-1:2022) 이슈가 주목받고 있다.\n",
      "- 주요 통계 및 데이터\n",
      "  - Stack Overflow Developer Survey 2025: AI 도구를 사용하거나 사용할 계획인 개발자 비중이 높음(대부분의 직군에서 80%대에 이르는 추세). 일부 응답자는 매일 AI 도구를 사용.\n",
      "  - AI 코드 생성 도구 관련 시장 동향: Copilot, CodeWhisperer, Cursor, Windsurf 등의 도구가 실무에 널리 도입되며, 스타트업 투자 및 밸류에이션이 상승하고 있음.\n",
      "  - 엔트리 레벨 개발자 채용 변화: AI 도입으로 반복적 작업이 축소되면서 신규 채용 흐름에 변화가 나타남.\n",
      "- 데이터의 한계\n",
      "  - AI 도구의 정확도, 보안성, 데이터 프라이버시 이슈는 여전히 중요한 제약 요인이다. 윤리적·문화적 맥락 반영에 관한 연구와 정책 정비가 필요하다.\n",
      "\n",
      "Analysis & Insights\n",
      "- 주요 트렌드\n",
      "  - AI 보조의 고도화: 코드 작성 보조를 넘어 요구 분석, 테스트 작성, 문서화까지 포괄하는 자동화 수준으로 확장 중이다.\n",
      "  - 에이전트 기반 협업 확산: 에이전트 빅스, AI 판사 등의 도구가 코드 품질 관리 및 멀티에이전트 협업을 촉진한다.\n",
      "  - 데이터-플랫폼의 통합: Lakebase 등의 플랫폼이 데이터 관리와 AI 개발 워크플로우를 하나의 환경에서 처리하도록 진화한다.\n",
      "  - 감정 인식의 적용 확대: 교육 및 협업 도구의 학습효과 및 몰입도 개선 가능성이 있다. 다만 프라이버시와 문화적 차이에 대한 주의가 필요하다.\n",
      "- 다양한 관점\n",
      "  - 기술/기업 관점: 생산성 향상과 대규모 코드베이스의 관리 효율성 증가가 기대되나, 보안·규정 준수의 강화가 필수다.\n",
      "  - 교육 관점: AI 도구의 활용 능력과 함께 기초 원리 이해, 보안/윤리 의식 강화가 필요하다.\n",
      "  - 사회/정책 관점: 개인정보 보호, 알고리즘 편향성 관리, 국제 표준 정립 등 정책적 이슈가 부상한다.\n",
      "- 전문가 의견 요약\n",
      "  - “AI는 개발자의 보조 도구”라는 공통된 시각이 지배적이며, 시스템 설계, 아키텍처, 보안, 도메인 전문성 같은 고부가 가치 영역으로 인간의 역할이 재정의될 가능성이 크다.\n",
      "  - 감정 인식 연구는 교육과 인터랙티브 도구 개선에 기여하나, 구현의 복잡성과 윤리 문제에 대한 신중한 접근이 필요하다는 견해가 많다.\n",
      "  - 스타트업-대기업 간의 협력 및 경쟁은 기술 생태계의 건강성과 혁신 속도를 좌우한다.\n",
      "- 도전 과제\n",
      "  - AI의 불완전성 및 디버깅 부담: “Almost right” 문제가 여전히 흔하며 디버깅 시간이 길어질 수 있다.\n",
      "  - 데이터 관리 및 보안: 에이전트의 기억(memory) 관리, 로그 기록, 접근 제어 등 거버넌스가 중요하다.\n",
      "  - 문화적 맥락의 반영 필요성: 문화적 차이에 따른 감정 표현 모델의 차이가 UX에 큰 영향을 준다.\n",
      "  - 교육 재구성 필요성: AI 도구의 도입과 함께 기초 원리/아키텍처 역량의 체계적 재교육이 필요하다.\n",
      "\n",
      "Future Outlook\n",
      "- Emerging trends\n",
      "  - AI 에이전트 주도 개발의 확산: IDE 내에서의 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 흐름이 확대될 전망.\n",
      "  - 멀티모달 감정 인식의 강화: 텍스트, 음성, 표정, 생리 신호를 결합한 감정 인식으로 개발 환경의 적응성 및 협업 품질이 향상될 가능성.\n",
      "  - 문화적 맥락 반영과 표준화: 다문화 환경에서의 감정 모델 차이를 고려한 UX 설계 및 국제 표준 정립이 중요한 의제가 됨.\n",
      "  - 교육-산업 연계 강화: affective computing 기반 학습 도구의 현장 직무 학습 도구로의 확산.\n",
      "- Predictions\n",
      "  - 5–10년 내 반복적이고 boilerplate 수준의 코딩은 AI 도구에 의해 상당 부분 대체 혹은 보조될 가능성 큼.\n",
      "  - 개발자의 역할은 고급 설계/아키텍처/보안/도메인 전문성 중심으로 재편될 가능성 높음.\n",
      "  - 교육 체계는 AI 도구 활용법 외에도 윤리, 보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 진화할 가능성.\n",
      "- Implications\n",
      "  - 기업: 데이터/AI 워크플로우의 통합 투자, 에이전트 거버넌스 강화, 프라이버시 및 보안 정책의 정교화 필요.\n",
      "  - 교육: AI 활용 역량과 함께 시스템적 사고, 아키텍처 설계 역량 강화 필요.\n",
      "  - 정책/표준: 데이터 보호, 책임 소재, 공정성/투명성 등의 원칙 정립이 중요해질 것.\n",
      "\n",
      "Recommendations\n",
      "- 실무/기업\n",
      "  - AI 도구 채택 로드맵 수립: 도구 도입 시점별 목표와 인간 검증 포인트를 명확히 정의.\n",
      "  - 거버넌스 체계 강화: 메모리 관리, 접근 제어, 감사 로그, 보안 규정 준수 등을 포함한 에이전트 관리 프레임워크 구축.\n",
      "  - 문화·윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영하는 정책 마련.\n",
      "- 교육\n",
      "  - AI 도구 활용 역량 중심의 커리큘럼 강화: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함.\n",
      "  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계/성능 최적화/보안 등 영역에 집중한 재교육 프로그램 운영.\n",
      "  - 감정 인식 연구의 교육적 활용: 데이터 관리 및 윤리 교육을 포함한 커리큘럼 디자인.\n",
      "- 연구 방향\n",
      "  - 감정 상태와 코드 품질/생산성 간의 관계 실증 연구\n",
      "  - 다문화 맥락에서의 감정 모델 비교 및 적합성 평가\n",
      "  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구\n",
      "\n",
      "Sources\n",
      "- Stack Overflow Developer Survey 2025: AI 도구 사용 현황 및 개발자 인식\n",
      "  - 예시 출처: Stack Overflow, 2025 Developer Survey, AI 도구 관련 섹션\n",
      "- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\n",
      "  - Databricks의 바이브 코딩 개념, Lakebase 출시 관련 보도\n",
      "  - Cursor, Windsurf, Codeium 등의 스타트업 가치평가 및 투자 현황\n",
      "- AI 코드 생성 도구 및 비교\n",
      "  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 사례\n",
      "  - Replit의 에이전트 협업 기능, 협업 환경 소개\n",
      "- 감정 인식/Affective Computing 연구\n",
      "  - Real-time emotion aware applications: GPU 병렬처리 가속 사례\n",
      "  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\n",
      "  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing, 2024)\n",
      "  - DevEmo, AffecTube, DEAP 등 데이터셋 및 연구\n",
      "- 표준 및 정책\n",
      "  - ISO/IEC 30150-1:2022 (Affective computing user interface)\n",
      "  - 프라이버시 및 데이터 보안 관련 논의 및 윤리 가이드라인\n",
      "- 미래 직무 및 업계 분석\n",
      "  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈 관련 최신 리뷰 및 기사\n",
      "- 대표 기사 예시(한국어/영문)\n",
      "  - TokenPost, Nate News 등 바이브 코딩 관련 기사\n",
      "  - Dev.to/엘리트브레인즈(EliteBrains) 등의 AI 코드 생성 도구 비교 및 전망 기사\n",
      "\n",
      "REPORT_COMPLETE\n",
      "---------- ToolCallRequestEvent (quality_reviewer) ----------\n",
      "[FunctionCall(id='call_NhPPT9nX4cBHtxpqXt6dlNSP', arguments='{\"content\":\"보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\\\\n\\\\nExecutive Summary\\\\n- 정의 확장: 바이브 코딩은 분위기/감정 인식 기반의 코딩 및 인터랙티브한 개발 환경을 포괄하는 개념으로 이해되며, AI 기반 코드 작성 보조와 데이터-AI 워크플로우의 통합 트렌드를 아우릅니다.\\\\n- 현황 요약: AI 코드 생성 도구의 급속한 확산, IDE/에디터의 AI 내재화, 에이전트 기반 협업 기능의 확대가 두드러진 현상이다. Databricks의 레이크베이스 같은 플랫폼 출시와 스타트업(Cursor, Windsurf, Codeium 등)의 밸류에이션 상승이 시장의 활황을 시사한다.\\\\n- 소프트웨어 개발자의 미래: 개발자의 역할은 “보조 도구의 활용과 고부가 가치 영역의 설계/아키텍처/보안”으로 재편될 가능성이 크다. 엔트리 레벨 개발자 취업 환경은 AI 도입으로 변화하고 있으며, 재교육/직무 전환이 중요한 과제다.\\\\n- 시사점: 교육 체계의 현대화, 데이터 프라이버시 및 보안 거버넌스 강화, 문화적 맥락을 반영한 UX/윤리 표준 확립이 필요합니다.\\\\n\\\\nBackground & Current State\\\\n- 현장 landscape\\\\n  - 바이브 코딩은 자연어 기반 코드 생성과 업무 자동화를 넘어, 데이터-플랫폼 서비스와 AI 자동화 도구를 하나의 생태계로 연결하려는 방향으로 발전하고 있다.\\\\n  - Lakebase(데이터브릭스의 서버리스 OLTP 데이터베이스)와 Agent Bricks, AI 판사(Judges) 같은 기능이 데이터-AI 워크플로우의 혁신적 연결고리로 주목됩니다.\\\\n- 최근 developments 및 데이터 포인트\\\\n  - 대기업-스타트업 생태계의 경쟁 구도 강화: Cursor, Windsurf, Codeium 등 AI 코드 생성 스타트업의 밸류에이션 상승 및 투자 증가.\\\\n  - 플랫폼 중심의 협업 도구 강화: Replit, Cursor, Windsurf 등의 협업/에이전트 기능으로 팀 간 개발 흐름이 더욱 연결되고 있다.\\\\n  - 졸업/학계 트렌드: 감정 인식(Affective Computing) 연구 및 교육 현장의 활용 사례 증가, DevEmo, DEAP 등 데이터셋과 국제 표준(IS/IEC 30150-1:2022) 이슈가 주목받고 있다.\\\\n- 주요 통계 및 데이터\\\\n  - Stack Overflow Developer Survey 2025: AI 도구를 사용하거나 사용할 계획인 개발자 비중이 높음(대부분의 직군에서 80%대에 이르는 추세). 일부 응답자는 매일 AI 도구를 사용.\\\\n  - AI 도구에 대한 인식 및 신뢰도: 전문가층에서 AI 도구의 긍정적 효과에 대한 인식은 높지만, 도구의 정확성·보안·프라이버시 이슈에 대한 우려도 여전히 큽니다.\\\\n  - 엔트리 레벨 개발자 채용 변화: AI 도입으로 반복적 작업이 감소하는 한편, 대형 IT 기업들은 자체 AI 코드 생성 솔루션을 확산시키고 있습니다.\\\\n- 데이터의 한계\\\\n  - AI 도구의 정확도, 보안성, 데이터 프라이버시 이슈는 여전히 중요한 제약 요인이다. 윤리적·문화적 맥락 반영에 관한 연구와 정책 정비가 필요하다.\\\\n\\\\nAnalysis & Insights\\\\n- 주요 트렌드\\\\n  - AI 보조의 고도화: 코드 작성 보조를 넘어 요구 분석, 테스트 작성, 문서화까지 포괄하는 자동화 수준으로 확장 중이다.\\\\n  - 에이전트 기반 협업 확산: 에이전트 빅스, AI 판사 등의 도구가 코드 품질 관리 및 멀티에이전트 협업을 촉진한다.\\\\n  - 데이터-플랫폼의 통합: Lakebase 등의 플랫폼이 데이터 관리와 AI 개발 워크플로우를 하나의 환경에서 처리하도록 진화한다.\\\\n  - 감정 인식의 적용 확대: 교육 및 협업 도구의 학습효과 및 몰입도 개선 가능성이 있다. 다만 프라이버시와 문화적 차이에 대한 주의가 필요하다.\\\\n- 다양한 관점\\\\n  - 기술/기업 관점: 생산성 향상과 대규모 코드베이스의 관리 효율성 증가가 기대되나, 보안·규정 준수의 강화가 필수다.\\\\n  - 교육 관점: AI 도구의 활용 능력과 함께 기초 원리 이해, 보안/윤리 의식 강화가 필요하다.\\\\n  - 사회/정책 관점: 개인정보 보호, 알고리즘 편향성 관리, 국제 표준 정립 등 정책적 이슈가 부상한다.\\\\n- 전문가 의견 요약\\\\n  - “AI는 개발자의 보조 도구”라는 공통된 시각이 지배적이며, 시스템 설계, 아키텍처, 보안, 도메인 전문성 같은 고부가 가치 영역으로 인간의 역할이 재정의될 가능성이 크다.\\\\n  - 감정 인식 연구는 교육과 인터랙티브 도구 개선에 기여하나, 구현의 복잡성과 윤리 문제에 대한 신중한 접근이 필요하다는 견해가 많다.\\\\n  - 스타트업-대기업 간의 협력 및 경쟁은 기술 생태계의 건강성과 혁신 속도를 좌우한다.\\\\n- 도전 과제\\\\n  - AI의 불완전성 및 디버깅 부담: “Almost right” 문제가 여전히 흔하며 디버깅 시간이 길어질 수 있다.\\\\n  - 데이터 관리 및 보안: 에이전트의 기억(memory) 관리, 로그 기록, 접근 제어 등 거버넌스가 중요하다.\\\\n  - 문화적 맥락의 반영 필요성: 문화적 차이에 따른 감정 표현 모델의 차이가 UX에 큰 영향을 준다.\\\\n  - 교육 재구성 필요성: AI 도구의 도입과 함께 기초 원리/아키텍처 역량의 체계적 재교육이 필요하다.\\\\n\\\\nFuture Outlook\\\\n- Emerging trends\\\\n  - AI 에이전트 주도 개발의 확산: IDE 내에서의 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 흐름이 확대될 전망.\\\\n  - 멀티모달 감정 인식의 강화: 텍스트, 음성, 표정, 생리 신호를 결합한 감정 인식으로 개발 환경의 적응성 및 협업 품질이 향상될 가능성.\\\\n  - 문화적 맥락 반영과 표준화: 다문화 환경에서의 감정 모델 차이를 고려한 UX 설계 및 국제 표준 정립이 중요한 의제가 됨.\\\\n  - 교육-산업 연계 강화: affective computing 기반 학습 도구의 현장 직무 학습 도구로의 확산.\\\\n- Predictions\\\\n  - 5–10년 내 반복적이고 boilerplate 수준의 코딩은 AI 도구에 의해 상당 부분 대체 혹은 보조될 가능성 큼.\\\\n  - 개발자의 역할은 고급 설계/아키텍처/보안/도메인 전문성 중심으로 재편될 가능성 높음.\\\\n  - 교육 체계는 AI 도구 활용법 외에도 윤리, 보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 진화할 가능성.\\\\n- Implications\\\\n  - 기업: 데이터/AI 워크플로우의 통합 투자, 에이전트 거버넌스 강화, 프라이버시 및 보안 정책의 정교화 필요.\\\\n  - 교육: 감정 인식 기반 교육 도입 및 재교육 프로그램 설계, 시스템적 사고/아키텍처 역량 강화 필요.\\\\n  - 정책/표준: 데이터 보호, 책임 소재, 공정성/투명성 등의 원칙 정립이 중요해질 것.\\\\n\\\\nRecommendations\\\\n- 실무/기업\\\\n  - AI 도구 채택 로드맵 수립: 어느 단계에서 어떤 도구를 도입하고, 언제 인간의 검증을 필수로 할지 명확한 프로세스 수립.\\\\n  - 거버넌스 체계 강화: 메모리 관리, 접근 제어, 감사 로그, 보안 규정 준수 등을 포함한 에이전트 관리 프레임워크 구축.\\\\n  - 문화·윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영하는 정책 마련.\\\\n- 교육\\\\n  - AI 도구 활용 역량 중심의 커리큘럼 강화: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함.\\\\n  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계/성능 최적화/보안 등 영역에 집중한 재교육 프로그램 운영.\\\\n  - 감정 인식 연구의 교육적 활용: 데이터 관리 및 윤리 교육을 포함한 커리큘럼 디자인.\\\\n- 연구 방향\\\\n  - 감정 상태와 코드 품질/생산성 간의 관계 실증 연구\\\\n  - 다문화 맥락에서의 감정 모델 비교 및 적합성 평가\\\\n  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구\\\\n\\\\nSources\\\\n- Stack Overflow Developer Survey 2025: AI 도구 사용 현황 및 개발자 인식\\\\n  - 예시 출처: Stack Overflow, 2025 Developer Survey, AI 도구 관련 섹션\\\\n- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\\\\n  - Databricks의 바이브 코딩 개념, Lakebase 출시 관련 보도\\\\n  - Cursor, Windsurf, Codeium 등의 스타트업 가치평가 및 투자 현황\\\\n- AI 코드 생성 도구 및 비교\\\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 사례\\\\n  - Replit의 에이전트 협업 기능, 협업 환경 소개\\\\n- 감정 인식/Affective Computing 연구\\\\n  - Real-time emotion aware applications: GPU 병렬처리 가속 사례\\\\n  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\\\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing, 2024)\\\\n  - DevEmo, AffecTube, DEAP 등 데이터셋 및 연구\\\\n- 표준 및 정책\\\\n  - ISO/IEC 30150-1:2022 (Affective computing user interface)\\\\n  - 프라이버시 및 데이터 보안 관련 논의 및 윤리 가이드라인\\\\n- 미래 직무 및 업계 분석\\\\n  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈에 관한 최신 리뷰 및 기사\\\\n- 대표 기사 예시(한국어/영문)\\\\n  - TokenPost, Nate News 등 바이브 코딩 관련 기사\\\\n  - Dev.to/엘리트브레인즈(EliteBrains) 등의 AI 코드 생성 도구 비교 및 전망 기사\\\\n\\\\nREPORT_COMPLETE\"}', name='save_report_to_md')]\n",
      "---------- ToolCallExecutionEvent (quality_reviewer) ----------\n",
      "[FunctionExecutionResult(content='report.md', name='save_report_to_md', call_id='call_NhPPT9nX4cBHtxpqXt6dlNSP', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (quality_reviewer) ----------\n",
      "report.md\n",
      "---------- TextMessage (quality_reviewer) ----------\n",
      "The research is complete. The report has been saved to report.md. Please review the report and let me know if you approve it or need additional research.\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "APPROVED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='a8b57596-d866-46ae-b3d8-2506d6558437', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 31, 38, 958442, tzinfo=datetime.timezone.utc), content='바이브 코딩과, 소프트웨어 개발자의 미래에 대해서 조사해', type='TextMessage'), TextMessage(id='8140f2b7-c2dd-4f4d-b1d1-cd147763a6f9', source='research_planner', models_usage=RequestUsage(prompt_tokens=129, completion_tokens=4405), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 32, 13, 52811, tzinfo=datetime.timezone.utc), content='좋습니다. 다만 “바이브 코딩”이 어떤 개념을 가리키는지 정확히 파악해야 계획을 정확하게 맞출 수 있습니다.\\n\\n확인 질문\\n1) 바이브 코딩이 특정 기술/도구/프레임워크의 이름인가요, 아니면 일반적인 개념으로서의 분위기(바이브) 기반 코딩이나 감정/환경 인식과 연계된 코딩 방식인가요?\\n2) 연구의 주된 초점은 학술 연구(논문 중심), 업계 트렌드(실무 영향), 또는 교육/직무 전망 중 어느 쪽에 맞추길 원하나요?\\n3) 산출물 형식은 요약 보고서, 비교 분석 표, 권고안 등의 어떤 형식을 선호하시나요?\\n4) 최근 2–3년의 최신 내용까지 포함하는 것이 좋나요, 아니면 더 넓은 시계열을 다루어도 되나요?\\n\\n확인되면, 위의 방향에 맞춰 바로 FOCUSED 연구 계획을 드리겠습니다. 아래는 한 가지 가정 하에 빠르게 예시 구조를 보여드리는 형태이며, 실제 해석이 확정되면 그에 맞춰 세부를 조정하겠습니다.\\n\\n예시(가정: 바이브 코딩을 분위기/감정 기반 코딩이나 인터랙티브 코딩이라는 일반 개념으로 해석)\\n- Core Topics (2–3개)\\n  1) 바이브 코딩의 정의와 현황: 용어 정의, 현재 사용 사례, 도구/플랫폼 현황\\n  2) 실용성 및 도입 사례: 산업별 적용 사례, 장점·한계, 도입 장애 요인\\n  3) 소프트웨어 개발자의 미래: AI 보조 coding의 진화, 스킬 체인 변화, 고용 시장과 교육의 적응\\n\\n- Search Queries (각 코어 토픽당 3–5개 예시)\\n  1) 최신 개발 동향/뉴스\\n     - \"바이브 코딩 최신 동향\" / \"vibe coding latest developments\"\\n     - \"감정 기반 코딩 도구 신제품\" / \"emotion-based coding tools 2024-2025\"\\n  2) 주요 통계나 데이터\\n     - \"바이브 코딩 도입 현황\" / \"adoption rate of mood-based coding techniques\"\\n     - \"AI 보조 개발 도구 사용률 통계\" / \"AI-assisted coding market size\"\\n  3) 전문가 분석/연구\\n     - \"전문가 분석 vibe coding\" / \"studies on emotion-aware programming\"\\n     - \"사례 연구 mood-based development\" / \"case studies of ambient/interactive coding\"\\n  4) 미래 전망\\n     - \"바이브 코딩의 미래 전망\" / \"future of mood-based programming\"\\n     - \"AI와 개발자의 역할 변화 예측\" / \"impact of AI on software developer skills\"\\n\\n참고로, 위 예시는 “바이브 코딩”을 일반적인 분위기/감정 기반 코딩으로 해석한 가정 하의 설계안입니다. 실제 용어 정의에 맞춰 코어 토픽과 질의를 정확히 재구성해 드리겠습니다. 원하시는 해석을 알려주시면, 그에 맞춰 2–3개의 핵심 주제와 3–5개의 검색 쿼리로 바로 맞춤형 FOCUSED 계획을 작성해 드리겠습니다.', type='TextMessage'), TextMessage(id='a15ab99d-5b5b-44a5-8eb2-67b1651d1fb8', source='research_planner', models_usage=RequestUsage(prompt_tokens=881, completion_tokens=2435), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 32, 33, 17881, tzinfo=datetime.timezone.utc), content='확인된 가정: 본 계획은 “바이브 코딩”을 분위기/감정 기반의 코딩 방식 혹은 인터랙티브한 분위기 인식과 연계된 개발 도구들로 해석한 것입니다. 다른 정의가 있다면 알려주시면 그에 맞춰 재구성하겠습니다.\\n\\nFOCUSED 연구 계획: 바이브 코딩과 소프트웨어 개발자의 미래\\n\\n1) Core Topics (2–3개)\\n- 바이브 코딩의 정의와 현황\\n  - 용어 정의, 현재 사용 사례, 도구/플랫폼의 현황 및 비교\\n- 실용성/도입 사례와 한계\\n  - 산업별 적용 사례, 생산성/품질에 주는 영향, 도입 장애 요인\\n- 소프트웨어 개발자의 미래 변화\\n  - AI 보조 코딩의 진화, 개발자 스킬 체인 변화, 교육 및 고용시장 영향\\n\\n2) Search Queries (4개; 각 항목은 최신 동향, 통계, 전문가 분석, 미래 전망을 포괄)\\n- 최신 개발 동향 및 뉴스\\n  - \"emotion-based coding tools 2024 2025 latest developments\" (감정 기반 코딩 도구 2024–2025 최신 동향)\\n  - \"바이브 코딩 최신 동향\" (한국어 자료 포함)\\n- 주요 통계나 데이터\\n  - \"adoption rate of emotion-based coding tools\" / \"감정 기반 코딩 도구 채택 현황\"\\n  - \"AI-assisted coding usage statistics 2023-2025\" / \"AI 보조 코딩 사용 통계 2023–2025\"\\n- 전문가 분석이나 연구\\n  - \"studies on emotion-aware programming\" / \"감정 인식 프로그래밍 연구\"\\n  - \"case studies mood-based development\" / \"기분 기반 개발 사례 연구\"\\n- 미래 전망\\n  - \"future of mood-based programming\" / \"바이브 코딩의 미래 전망\"\\n  - \"impact of AI on software developer skills\" / \"AI가 소프트웨어 개발자 스킬에 미치는 영향\"\\n\\n추가 안내\\n- 위 계획은 빠르게 시작할 수 있도록 4개 질의로 구성했습니다. 필요 시 1–2개 주제를 더 세분화하거나, 특정 산업(예: 금융, 헬스케어)에 맞춘 사례 연구를 추가로 넣어 맞춤화해 드리겠습니다.\\n- 용어 정의가 확정되면, 핵심 주제의 세부 하위 주제 목록과 각 주제별 구체적 자료 목록(저널, 컨퍼런스, 보고서)을 함께 제시해 드리겠습니다.', type='TextMessage'), ToolCallRequestEvent(id='d644712a-9409-49d7-a6de-94e56a97076c', source='research_agent', models_usage=RequestUsage(prompt_tokens=1644, completion_tokens=822), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 32, 47, 653190, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_MDdhX9RZh3sRp9e9V2HGdIZp', arguments='{\"query\": \"emotion-based coding tools 2024 2025 latest developments\"}', name='web_search_tool'), FunctionCall(id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', arguments='{\"query\": \"바이브 코딩 최신 동향\"}', name='web_search_tool'), FunctionCall(id='call_4PAMAyFwVRDo2ioyrSmVFaI2', arguments='{\"query\": \"emotion-aware programming studies\"}', name='web_search_tool'), FunctionCall(id='call_MZOCIL7HSdei6STfRI8fQssA', arguments='{\"query\": \"AI-assisted coding usage statistics 2023-2025\"}', name='web_search_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(id='a69564cd-2b38-4b90-b563-7535f7c99824', source='research_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 32, 57, 222112, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='Unexpected error during search: Status code 502. Server returned non-JSON response: \\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>502 Server Error</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Server Error</h1>\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\n<h2></h2>\\n</body></html>\\n - Full response status: 502', name='web_search_tool', call_id='call_MDdhX9RZh3sRp9e9V2HGdIZp', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'2025년 바이브 코딩 AI 개발 동향 보고서\\', \\'url\\': \\'https://seo.goover.ai/report/202506/go-public-report-ko-e086645e-079a-44f8-ae63-13f0358d262d-0-0.html\\', \\'markdown\\': \"![]( 2025년 바이브 코딩 AI 개발 동향 보고서일반 리포트2025년 06월 16일!## 목차1. 요약2. 서론3. 데이터브릭스의 통합 AI 플랫폼 전략4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황5. 최고의 AI 코드 생성 도구 비교 분석6. 미래 전망 및 시사점7. 결론## 1. 요약- 2025년 바이브 코딩 AI 개발 동향 리포트에서는 인공지능 기술의 발전에 따라 변화하고 있는 소프트웨어 개발 환경을 다룹니다. 특히, \\'바이브 코딩\\'이라는 새로운 개념을 중심으로 데이터브릭스의 통합 플랫폼 전략과 스타트업 경쟁 구도를 분석합니다. 이 보고서의 핵심 발견은, 데이터브릭스가 제시하는 레이크베이스 플랫폼과 AI 자동화 도구가 기업들이 데이터와 AI를 효과적으로 통합하고 활용할 수 있는 환경을 제공한다는 것입니다. 바이브 코딩 스타트업인 Cursor, Windsurf, Codeium 등의 사례와 90억 달러 이상의 기업 가치를 통해 AI 코드 생성 시장의 동향이 더욱 뚜렷하게 드러나며, 개발자 역할의 변화와 더불어 AI 도구의 중요성이 지속적으로 증가하고 있음을 보여줍니다.## 2. 서론- AI 기술이 급속히 발전함에 따라 소프트웨어 개발의 패러다임이 전환되고 있습니다. 이러한 변화의 중심에 자리한 \\'바이브 코딩\\'은 개발 지식이 부족한 사용자들도 손쉽게 코드를 작성할 수 있는 기회를 제공합니다. 비록 이 기술이 새로운 가능성을 창출하지만, 동시에 기존의 개발자 일자리에 위협이 되고 있습니다. 본 리포트는 바이브 코딩을 통해 신생 기업과 대규모 플랫폼 간의 경쟁 구도, 그리고 AI 코드 생성 도구의 진화에 대해 심층적으로 분석하고자 합니다. 현재 시장에서 주목받고 있는 데이터브릭스의 레이크베이스 플랫폼 전략을 시작으로, 여러 스타트업 사례와 AI 코드 생성 도구의 특징을 살펴보며, 미래의 개발 환경이 어떤 방향으로 나아갈지를 예측합니다.## 3. 데이터브릭스의 통합 AI 플랫폼 전략- 인공지능의 세계가 진화하는 과정에서 데이터브릭스는 두 가지 측면에서 주목할 만한 변화를 이끌고 있습니다. 첫째, 인공지능의 사용 범위를 확장하는 \\'바이브 코딩(vibe coding)\\' 개념의 도입과 둘째, 이를 바탕으로 한 새로운 플랫폼인 레이크베이스(Lakebase)의 출시입니다. 데이터브릭스는 근본적으로 기업의 데이터 생태계를 통합하고, AI 중심의 혁신을 통해 기업들이 데이터를 더 효과적으로 활용할 수 있도록 새로운 방향성을 제시하고 있습니다.- ### 3-1. ‘바이브 코딩’ 개념과 Databricks의 레이크베이스(Lakebase) 출시- ‘바이브 코딩’은 데이터브릭스가 최근 발표한 혁신적인 개념으로, 자연어 기반의 AI 코드 생성과 업무 자동화를 통합한 것을 의미합니다. 사용자가 원하는 작업을 자연어로 설명하면, 시스템은 내장된 데이터와 AI 모델을 활용하여 적절한 코드를 자동으로 작성합니다. 이 접근 방식은 개발자들이 직접 코드를 입력하는 대신 시스템과 대화하며 작업을 수행할 수 있도록 돕습니다. 이는 기존의 코드 중심 개발 방식을 넘어서는 중요한 이정표로서, AI의 역할을 더 효율적이고 인간 친화적으로 만들어 줍니다.- 데이터브릭스는 데이터+AI 서밋 2025에서 새로운 플랫폼인 레이크베이스를 공식 출시했습니다. 레이크베이스는 서버리스 OLTP(온라인 트랜잭션 처리) 데이터베이스로, PostgreSQL과 호환되며 오브젝트 스토리지와의 통합 기능을 갖추고 있습니다. 이 플랫폼은 데이터 처리의 효율성과 확장성을 모두 만족시키며, 기업들이 데이터 분석, 트랜잭션 처리, AI 개발을 하나의 플랫폼 내에서 유기적으로 연결될 수 있는 환경을 제공합니다. 이는 기업의 데이터 생태계를 통합하는 데 있어 중대한 시점입니다.- ### 3-2. Agent Bricks와 AI 판사 기능 설명- 에이전트 브릭스(Agent Bricks)는 데이터브릭스의 또 다른 혁신으로, 자연어 명령을 통해 정보 추출, 지식 지원, 멀티에이전트 감시 등의 역할을 수행하는 AI 에이전트를 자동으로 생성할 수 있는 도구입니다. 이 시스템은 사용자 오류를 최소화하고, 정보의 질을 높이는 방향으로 설계되었습니다.- 특히 주목할 만한 것은 AI 판사(judges) 기능입니다. 이 기능은 AI 에이전트의 성능을 모니터링하고 평가하는 데 사용되며, 특정 기준에 따라 생성된 결과의 질을 점검하고 개선하는 역할을 합니다. 이를 통해 에이전트들은 학습하고 발전할 수 있으며, 최적의 결과를 얻기 위해 코드 수정 과정을 연구하게 됩니다. 데이터브릭스는 이러한 기능들을 통해 정보 활용의 효율성을 극대화하고, AI의 잠재력을 더욱 확장하고자 합니다.- ### 3-3. 플랫폼 통합으로 달성하는 데이터·AI 워크플로우 혁신- 데이터브릭스의 통합 AI 플랫폼 전략은 단순한 기술 통합을 넘어 데이터와 AI가 밀접하게 연결된 생태계를 형성하는 데 기여하고 있습니다. BI(비즈니스 인텔리전스) 툴과 머신러닝 모델을 통합함으로써 기업들은 데이터 기반 의사결정을 신속히 내릴 수 있는 능력을 갖추게 됩니다. 이는 데이터 흐름을 자동화하고, 실시간으로 분석을 가능하게 하여 비즈니스의 민첩성을 높이는 중요한 단계입니다.- 임무별 에이전트와 AI 분석 툴 간의 협업은 기업 내에서 심층적인 데이터 분석을 수행할 수 있도록 하며, 사용자들은 실시간으로 인사이트를 확보하게 됩니다. 이러한 혁신은 기존의 데이터 분석, 데이터 관리 및 AI 모델에 대한 접근 방식을 바꾸고 있으며, 기업들이 더욱 빠르게 변화하는 시장에서 경쟁력을 유지할 수 있도록 돕습니다. 데이터브릭스는 AI 자동화와 데이터 관리의 통합을 통해 이 전환을 가속화하고 있으며, 이는 향후 기업의 데이터와 AI 활용 방식에 지대한 영향을 미칠 것입니다.## 4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황- AI 기술의 급속한 발전은 소프트웨어 개발 생태계에 혁신적인 변화를 불러오고 있습니다. 특히, 바이브 코딩(vibe coding)이라는 개념이 등장하면서 일반 사용자도 손쉽게 코드 작성이 가능해졌습니다. 이는 프로그래밍 지식을 가진 개발자들에게만 국한되지 않고, 폭넓은 사용자에게 소프트웨어 개발의 기회를 제공합니다. 이러한 변화는 단순히 기술적인 것에 그치지 않고 투자 시장에서도 큰 물결을 일으키고 있습니다. 이번 섹션에서는 바이브 코딩 스타트업의 경쟁 구도와 최근 투자 현황에 대해 분석하겠습니다.- 2025년 현재, 바이브 코딩 시장에서 주목받고 있는 스타트업은 Cursor, Windsurf, Codeium 등입니다. 특히, Cursor는 최근 90억 달러의 기업 가치에 도달하며 10억 달러 규모의 투자를 유치했습니다. 이들은 \\'AI 코드 생성\\'을 통해 프로그래밍 언어에 대한 깊은 이해 없이도 소프트웨어를 개발하는 것을 가능하게 하여, 이를 통해 많은 사용자가 개발에 참여할 수 있도록 하고 있습니다. Windsurf와 Codeium 역시 30억 달러 가치를 인정받으며, 오픈AI의 인수 협상 대상으로 떠오르는 등 바이브 코딩 시장에서 활발한 활동을 보이고 있습니다.- ### 4-1. Cursor·Windsurf·Codeium 등 주요 스타트업 사례- Cursor는 인공지능을 활용한 코드 생성 플랫폼을 제공하며, 사용자가 쉽게 코드 스니펫을 생성하고 통합할 수 있도록 돕습니다. 또한, 이 플랫폼은 커뮤니티와의 협업 기능을 강화하여 다양한 사용자가 서로의 코드 작업을 공유할 수 있게 하며, 이는 소프트웨어 개발이 전통적인 개발 흐름을 넘어선 새로운 차원으로 나아가게 합니다. Windsurf는 Codeium 개발을 통해 비개발자들도 코드 작성에 참여할 수 있는 환경을 조성하고 있으며, 이를 통해 소프트웨어 개발의 인력 부족 문제를 해결하고 있습니다.- 또한, 이들 스타트업들은 단순한 코드 작성 도구를 넘어, 사용자 경험을 최적화하고 개발자 커뮤니티와의 연결성을 증대시키기 위한 방향으로 발전하고 있습니다. 이들은 AI 코딩 도구의 수요와 더불어, 사용자 친화적인 인터페이스를 제공함으로써 모바일 기기 및 다양한 플랫폼에서의 접근성을 높이고 있습니다.- ### 4-2. AI 코드 생성 스타트업의 밸류에이션 상승 배경- AI 코드 생성 스타트업들의 밸류에이션 상승은 몇 가지 주요 요인에 기인합니다. 첫째, 기업들이 AI 도입을 통해 업무 생산성을 극대화하고 싶어하는 경향이 있습니다. 소프트웨어 개발 분야에서 AI의 활용은 코딩을 자동화하고, 반복적인 업무를 감소시켜 기업들이 원하는 시간 안에 프로젝트를 완수하도록 돕고 있습니다. 이에 따라 AI 코드 생성 스타트업은 높은 수익 성장 잠재력을 지닌 것으로 평가받고 있습니다.- 둘째, AI 기술에 대한 투자자들의 관심이 급증하면서 스타트업들이 쉽게 자금을 조달할 수 있는 환경이 조성되었습니다. 특정 투자자들은 AI가 가져올 수 있는 혁신성과 시장의 미래 가능성을 높게 평가하고 있으며, 이는 스타트업 가치의 상승으로 이어지고 있습니다.- ### 4-3. 엔트리 레벨 개발자 일자리 변화 및 빅테크 진출 현황- AI의 도입은 엔트리 레벨 개발자 일자리에 큰 변화를 일으키고 있습니다. 최근 통계에 따르면, AI가 반복적이고 기본적인 코딩 작업을 대체하면서 1년 미만 경력의 개발자 신규 채용이 24% 감소한 것으로 나타났습니다. 이는 기술 발전이 개발자들의 일자리 환경에 어떠한 영향을 끼치고 있는지를 잘 보여줍니다.- 더불어, 구글, 아마존, 마이크로소프트(MS)와 같은 빅테크 기업들은 자체적인 AI 코드 생성 제품을 출시하며 이 시장에 진입하고 있습니다. 이러한 방식으로 이들 기업은 인건비를 절감하고 효율성을 극대화하고 있습니다. 예를 들어, MS는 올해 6000명 감원을 통해 개발자 역할의 변화가 불가피함을 강조하며, 자동화 기술이 고용에 미치는 영향을 조명하고 있습니다.## 5. 최고의 AI 코드 생성 도구 비교 분석- AI 코드 생성 도구는 현대 소프트웨어 개발의 필수 요소로 자리 잡았습니다. 코드 생성의 혁신이 이루어짐에 따라, 개발자들은 반복적인 작업에서 벗어나 더 창의적이고 전략적인 업무에 집중할 시간을 확보할 수 있게 되었습니다. 이러한 도구들은 단순히 코드를 생성하는 것을 넘어서, 개발자의 작업 흐름을 혁신하고, 팀 협업을 강화하며, 코드 품질을 향상시키는 역할을 합니다. 따라서 다양한 AI 코드 생성 도구들의 기능, 장점과 단점을 분석하는 것은 이 시대의 개발 환경을 이해하는 데 필요합니다.- AI 코드 생성기는 이제 단순한 도우미가 아니라, 실제로 개발과정의 핵심적인 파트너로 성장하고 있습니다. 특정 도구의 선택은 프로젝트의 성공 여부를 크게 좌우할 수 있으며, 여러 도구 중 어떤 것이 가장 적합한지는 사용자의 요구와 팀의 구조에 따라 달라질 수 있습니다. 따라서 이러한 도구들을 비교하고 분석하는 것은 매우 중요한 작업입니다.- ### 5-1. 10가지 주요 AI 코드 생성기 기능·장단점 요약- 다양한 AI 코드 생성기 중에서 10개의 주요 도구를 선정하여 그 기능과 장단점을 요약하였습니다. 각 도구는 특정 기능에 특화되어 있으며, 사용 용도에 따라 효과적인 선택이 이루어져야 합니다. 예를 들어, GitHub Copilot은 강력한 코드 자동 완성 기능과 통합된 학습 기능을 통해 개인 개발자와 팀 모두에게 유용하나, 보안상의 이유로 민감한 프로젝트에는 적합하지 않을 수 있습니다.- 반면에, Replit은 사용자 친화적인 인터페이스와 팀 협업 기능을 제공하여 초보자와 경력 개발자 모두에게 적합합니다. 그러나 상대적으로 많은 기능이 통합되어 인프라 관리에 추가적인 학습이 필요할 수 있습니다. 각 도구의 인상적인 기능은 다음과 같습니다. 1) GitHub Copilot: 예측 코드 생성, 다국어 기능, 지속적인 학습. 2) Replit: 실시간 협업, 객체 저장소, 에이전트 기능. 3) Cursor: 스마트 자동 완성 및 상황 인식. 4) Lovable.dev: 풀스택 개발 환경, 연속적 호스팅, GitHub 통합.- 각 도구의 장단점을 제대로 이해하고 활용한다면, 기업의 코드 작성 프로세스를 획기적으로 개선할 수 있습니다.- ### 5-2. Replit 등 대표 플랫폼의 에이전트·배포·협업 기능- Replit은 클라우드 기반 환경에서 에이전트를 활용하여 애플리케이션 개발을 지원하는 대표적인 플랫폼입니다. 에이전트 v2는 사용자의 대화 내용을 바탕으로 자연어로 명령을 수행하고, 코드를 작성하는 과정에서 기계 학습을 통해 더 나은 결과를 생성합니다. 사용자는 오히려 더 많은 시간을 절약하고 창의적인 작업에 집중할 수 있습니다.- 또한, Replit의 협업 기능은 팀 프로그래밍에 최적화되어 있습니다. 팀원들은 실시간으로 함께 코드 작성, 디버깅 및 배포 작업을 수행할 수 있어 작업 효율성을 높입니다. 이러한 협업 기능은 특히 소규모 팀과 스타트업에서 효과적으로 나타납니다. 하지만 클라우드 기반 서비스이기 때문에 인터넷 연결이 필수적이며, 민감한 데이터를 처리하는 데 있어 일부 사용자에게 불안감을 줄 수 있습니다.- 편리함과 안정성만을 찾는 사용자를 위해 Windsurf와 같은 로컬 IDE 기반의 코드 편집기 또한 주목받고 있습니다. 이는 클라우드와의 연결이 필요 없으며, 오프라인 상태에서도 높은 성능을 발휘합니다. 각 플랫폼의 특성을 고려하여 조합하면 더 효과적인 개발 환경을 구축할 수 있습니다.- ### 5-3. 도구별 활용 시나리오 및 성능 비교- AI 코드 생성 도구들은 각각의 기능과 특성에 따라 다양한 활용 시나리오가 존재합니다. 예를 들어, GitHub Copilot은 대규모 오픈소스 프로젝트에서 일반적으로 사용되며, 누구에게나 접근이 용이한 코드 제안을 통해 신속한 개발을 도와줍니다. 와 같은 대규모 솔루션에서는 보안이 필수적이므로 지극히 주의가 필요합니다.- 반면 Replit은 시작 단계의 스타트업이나 부트캠프 등에서 인기를 끌고 있으며, 초보자들이 스크립트를 작성하고 간단한 애플리케이션을 만들기에 적합합니다. 이러한 교육적인 측면도 고려해야 합니다. Windsurf는 보다 고급 코드 편집 및 오프라인 환경이 필요한 사용자에게 적합하여 프리랜서 개발자들이나 개인 프로젝트를 추진하는 이들에게 이상적입니다.- 기본적으로 각 도구의 성능은 사용자의 요구와 프로젝트의 특성에 따라 매우 다르게 나타납니다. 예를 들어, 고급 사용자는 지연 시간과 성능을 중시할 수 있지만, 초보자는 사용자 경험과 접근성에 중점을 두는 경향이 있습니다. 그러므로 이러한 도구를 선택할 때에는 반드시 사용자의 요구와 환경을 충분히 고려해야 합니다.## 6. 미래 전망 및 시사점- AI의 발전은 단순한 기술 혁신을 넘어, 인류의 생활 방식과 일하는 방식을 근본적으로 변화시키고 있습니다. 특히 바이브 코딩의 확산은 이러한 변화의 중심에서 새로운 가능성을 열어주고 있습니다. 이 과정을 통해 개발자, 기업, 그리고 사회 전체가 어떻게 적응할 것인지에 대한 논의는 앞으로의 방향성을 잡는 데 중요합니다. 따라서 AI의 자동화가 가져올 개발자 역할의 변화와 플랫폼과 스타트업 간의 연계, 기술의 상호 발전 방향에 대한 통찰이 필요합니다.- ### 6-1. 주요 기술의 상호 보완적 발전 방향- AI 코드 생성기와 자연어 프로세싱(NLP) 기술의 발전은 서로를 보완하며 지속적으로 진화하고 있습니다. 예를 들어, Replit과 같은 플랫폼은 비 프로그래머도 소프트웨어 개발에 참여할 수 있도록 도와주며, 이는 결국 개발 생태계의 다변화를 가져옵니다. 이처럼 다양한 도구가 상호 연결됨으로써, 사람들이 코드를 작성하는 방식을 극 대화하는 기회를 제공합니다. 또한, AI 기반 플랫폼은 사용자가 입력한 자연어 명령을 바탕으로 즉각적인 피드백과 결과를 제공하여, 개발 속도를 높이고 품질을 개선합니다. 이는 코드 작성에서의 인간의 역량을 한 단계 끌어올리는 데 기여하고 있으며, 향후 이러한 기술의 발전 방향은 더욱 협업적이고 직관적인 개발 환경으로 이어질 것입니다. 따라서 이러한 혁신이 어떻게 산업 전체에 파급효과를 미칠 것인가에 대한 연구가 필요합니다.## 7. 결론- 이번 보고서를 통해 바이브 코딩 그리고 AI 기술이 소프트웨어 개발의 혁신을 이끌고 있음을 확인할 수 있었습니다. 데이터브릭스의 통합 플랫폼 전략이 기업들이 데이터 활용을 극대화하는 데 기여하는 동시에, 스타트업들이 생겨나는 환경은 기회이자 도전과제로 작용합니다. 특히, AI 코드 생성 도구들은 단순한 코드 생성기를 넘어, 개발자들의 업무 방식을 혁신하며 이들의 지속적인 활용이 요구됩니다. 앞으로의 연구는 이러한 기술들이 기업과 개인의 개발 환경에 미칠 장기적인 영향을 심화하여 분석하고, 변화하는 시장 환경에 대한 비즈니스 전략을 재정립하는 데 초점을 맞춰야 하겠습니다. AI 기술의 발전은 결국 인간의 일하는 방식을 변화시키고 있으며, 이에 따라 새로운 접근과 대처가 필요합니다.## 용어집- 바이브 코딩: 개발 지식이 부족한 사용자도 손쉽게 코드를 작성할 수 있도록 돕는 인공지능 기반의 코드 생성 시스템을 의미.- 데이터브릭스: 기업의 데이터 생태계를 통합하고 AI 중심의 혁신을 통해 데이터를 효과적으로 활용할 수 있도록 지원하는 플랫폼.- 레이크베이스(Lakebase): 데이터브릭스에서 출시한 서버리스 OLTP 데이터베이스로, PostgreSQL과 호환되며 데이터 처리의 효율성과 확장성을 제공.- AI 자동화: 인공지능 기술을 활용하여 반복적이고 기본적인 작업을 자동으로 수행하도록 하는 프로세스.- 에이전트 브릭스(Agent Bricks): 데이터브릭스의 도구로, 자연어 명령을 통해 AI 에이전트를 자동으로 생성 및 관리하는 기능을 가지고 있음.- AI 판사(judges): AI 에이전트의 생성 결과를 평가하고 개선하는 기능으로, 품질을 모니터링하는 역할을 함.- BI(비즈니스 인텔리전스): 기업이 데이터를 분석하고 제공하여 비즈니스 의사결정을 지원하는 기술과 시스템을 통칭.- 스타트업: 신생 기업으로, 주로 혁신적인 제품이나 서비스를 바탕으로 빠른 성장을 목표로 하는 비즈니스 모델.- 코드 생성기: 인공지능 기술을 활용하여 소프트웨어 개발자가 아닌 사용자가 코드 작성을 가능하게 도움.- AI 코드 생성: 인공지능이 자동으로 소프트웨어 코드를 작성하는 기술.- 머신러닝: 인공지능의 한 분야로, 컴퓨터가 데이터를 학습하고 예측할 수 있도록 하는 알고리즘 및 기술.- 서버리스: 서버 관리 없이 클라우드에서 애플리케이션을 실행할 수 있는 아키텍처 모델.- 클라우드 기반: 인터넷을 통해 클라우드 서비스 제공업체가 제공하는 리소스를 활용하는 방식.출처 문서- **데이터브릭스, \\'바이브 코딩\\'으로 기업 AI 생태계 통합 선언 - TokenPost** - **AI \\'바이브 코딩\\' 스타트업, 몸값 고공행진…수익성은 글쎄 : 네이트 뉴스** - **바이브 코딩을 위한 최고의 AI 코드 생성기 10가지 (2025년 XNUMX월) - Unite.AI** \"}, {\\'title\\': \\'개발자는 결국 대체될까? 바이브코딩 트렌드 리포트\\', \\'url\\': \\'https://eopla.net/magazines/32364\\', \\'markdown\\': \\'![]( 결국 대체될까? 바이브코딩 트렌드 리포트2025. 07. 23![]( 이 글은  에서 발행되었습니다.>> 가장 전문적인 바이브코딩 인사이트를 매주 받아보세요.>> [[구독하러 가기]]( 이미지]( 최근 가장 핫하고 변화도 빠른 분야입니다.매일 새로운 서비스가 등장하고 이를 소개하는 컨텐츠도 많은데요,그중\\\\xa0**팀차원에서, 실무수준으로 운영가능한 코드** 를 작성하는 관점의 컨텐츠는 거의 없습니다.이번 레터에서는 바이브코딩 컨설팅업체를 운영하며 느낀 바이브코딩의 현 위치와,\\\\xa0**제대로 사용하기위한 준비사항** 들을 소개해보겠습니다.## **바이브코딩이란?**바이브코딩은 쉽게 말해\\\\xa0AI가 대신 코딩하는 방법론입니다.어려운 개발언어 공부할 필요 없이 \"계산기 만들어줘\"처럼 요청하면 되는거죠.아티클을 더 읽고 싶다면?이오플래닛에 가입해주세요.로그인 후 모든 아티클을 무료로 볼 수 있어요.]( 7]( 뉴스레터]( 어썸데브 · CEO 댓글 5]( studio약 2달 전]( 뉴스레터 님의 아티클이 EO 뉴스레터에 실렸습니다. 이번 주 이오레터를 확인하세요!>>> ]( 0]( l 가능성에 반응하는 사람들약 2달 전]( 쉽게 이해할 수 있어 잘 읽었습니다. 한 두개 프로잭트에 활용해 봐야 겠어요. 고맙습미다.]( 0](\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0개발자약 2달 전]( 실력에서 나온 통찰력 있는 내용이라 배우고 갑니다.다음글도 기대가 되네요~]( 0]( 코리아\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Manager약 2달 전]( 맥을 주사 바늘처럼 정교하게 찔러주셔서많은 영감을 얻고 갑니다. 향후 글 기대 됩니다.]( 1]( 뉴스레터어썸데브\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0CEO약 2달 전]( 정말 감사합니다! 앞으로도 좋은 컨텐츠 남기겠습니다. :)]( 0추천 아티클최바울  페오펫]( 약 3년 전]( 약 3년 전디오니소스  디오니소스\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Owner]( 1년 이상 전]( Product Owner  1년 이상 전[# 사업전략# 마인드셋# 트렌드# 기타‘챗GPT의 아버지’ 샘 알트만에 대해 알아보자 [아티클 한 눈에 보기]0.샘 알트만이라는 독특한 캐릭터1.\"이해 받지 못하는 것이 강점이다\"2.CEO는 어떻게 시간을 써야 하는가3.실리콘밸리 요다의 3가지 깨달음4.핵융합, 기본소득, 규제, 정치적 올바름\\\\xa0\\\\xa02022년 여름부터 연일 헤드라인을 장식하는 사람이 ...](  eo\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0에디터]( 2년 이상 전]( 에디터  2년 이상 전]( 뉴스레터]( 어썸데브 · CEO ]( 7![](\\'}]', name='web_search_tool', call_id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'Real time emotion aware applications: A case study employing ...\\', \\'url\\': \\'https://www.sciencedirect.com/science/article/abs/pii/S0169260712000831\\', \\'markdown\\': \\' - - - - Other access optionsSearch ScienceDirect## Article preview- - - - - ]( \"Go to Computer Methods and Programs in Biomedicine on ScienceDirect\")## , July 2012, Pages 16-27]( Real time emotion aware applications: A case study employing emotion evocative pictures and neuro-physiological sensing enhanced by Graphic Processor UnitsAuthor links open overlay panel , , , Show moreAdd to MendeleyCite ## AbstractIn this paper the feasibility of adopting  Units towards real-time emotion aware computing is investigated for boosting the time  employed in such applications. The proposed methodology was employed in analysis of encephalographic and electrodermal data gathered when participants passively viewed emotional evocative stimuli. The  effectiveness when processing electroencephalographic and electrodermal recordings is demonstrated by comparing the  of chaos/complexity analysis through  (multi-channel correlation dimension/D2) and  (computation of  level/SCL) into various popular programming environments. Apart from the beneficial role of parallel programming, the adoption of special design techniques regarding  may further enhance the time minimization which approximates a factor of 30 in comparison with ANSI C language (single-core sequential execution). Therefore, the use of  parallel capabilities offers a reliable and  for real-time sensing the user\\\\\\'s affective state.## IntroductionEmotion aware computing was for a large period a neglected topic in the scientific community [1]. However, recent neuroscience findings have highlighted the critical role of emotions in a variety of cognitive functions like decision making [2], memory [3] and perception [4]. These arguments demonstrated the significance of emotional intelligence [5] not only when interacting with other people but also between human and machines [6]. Therefore, motivated research efforts investigate how to provide computers with abilities to recognize the user\\\\\\'s emotional state and to naturally adapt to it [7]. The importance of emotion aware computing is desirable only in cases where the user should interact with the machine in order to achieve high performance during the task procedure that should be accomplished [8]. So, providing the machine with the capability to robustly sense the users’ negative feelings [9] (frustration, anger, stress, anxiety, disappointment, etc.) the appropriate feedback may be given to neutralize their mood [10] and to encourage them to improve their performance in several applications like tests controlled through computer [11], virtual gaming [12] or remote monitoring of elderly or disabled people [13], [14]. Initial research attempts have demonstrated that the core element of a successful affective computing system is its ability to emulate the ways that are employed in the communication between human beings [15]. The pioneering work of MIT group led to the introduction of the “Affective Computing” term and to the establishment of a framework that could be adopted for a successful human–computer interaction (HCI) system [16], while also dealing with the challenges that have to be faced and the expectations created by potential applications [8].Previous research attempts have adopted communicative ways like facial expressions [17] and posture recognition [18]. However, several limitations occur since these modalities are highly dependent from the users’ personality [19] and their culture, resulting thus in enhanced inter-subject variability. Robust emotion recognition assumes the utilization of exaggerated expressions that are unlikely to be elicited in real-life situations [20]. Moreover, the use of cameras produces huge amount of data, while also communicates irrelevant information (e.g. subject\\\\\\'s identity) which the user may be unwilling to reveal [7]. Since the aforementioned methodologies are based on the recognition of externally expressed emotions, some innermost may not be easily recognized [8]. Such feelings are not easily communicated even among human beings and may be better recognized by neuro-physiological sensing [7]. Data fusion [21] from both the central and the autonomic nervous system may create discrete emotional patterns for a wide range of emotions [22], which are poorly distinguishable otherwise. However, special care should be given to the experimental methodology used for emotion elicitation.So, a key issue towards the achievement of a robust emotion aware computerized system is the establishment of a framework that is in close connection with the modern emotional theory assuring thus the reliable emotion elicitation. Recent trends regard emotions as behavioral attitudes related with evolutionary processes aiming to assure the human\\\\\\'s survival and perpetuation [23], [24], [25]. Therefore, each situation may be judged as either a pleasant or an unpleasant one. Its importance modulates the activation level needed in order to confront the stimulus appeared. Erotic or life-threatening situations require higher activation degree than melancholic or relaxing occasions. Adopting this notion, a bi-directional model was proposed. According to this approach, emotional processing is governed by two motivational systems which are the appetitive approach dealing with the pleasant situations and the defensive one activated in case of life-threatening occasions. The activation of the aforementioned systems is described through the valence dimension, while the activation degree is represented by the arousal dimension. So, these affective variables form a 2D emotional space.The International Affective Picture System (IAPS) collection adopts the aforementioned emotional model and provides a variety of affective visual stimuli as well as their normative ratings for both the arousal and valence dimension [26]. The use of this picture collection with simultaneous neurophysiological recordings demonstrated the facilitated encoding of emotional stimuli [27]. The combination of central nervous (event-related potentials/ERPs) and autonomic (electrodermal) activity revealed a significant correlation between skin conductance responses (SCRs) and the arousal ratings of the IAPS stimuli [23]. Moreover, late ERPs were more positive for emotional pictures [28], while their time course was influenced by the valence dimension [29]. A recent study investigated whether emotional processing is affected by the subject\\\\\\'s gender. Early (N100) and mid (N200) ERPs were significantly greater for female participants during passive viewing of unpleasant pictures [30].The bi-directional emotion model and the aforementioned neuroscience findings have not been widely adopted until now in the field of emotion aware computing. Relying on these notions, a Mahalanobis distance-based classification scheme was proposed for discriminating emotional instances selected from the IAPS collection. The output of the recognition sub-system was then used by an avatar which emulated the user\\\\\\'s affective state by adapting its face and voice characteristics [14]. However, there was need for further improvement of the classification accuracy by applying data mining (decision trees) and pattern recognition (Support Vector Machines) techniques [31]. Towards the achievement of a reliable emotion-aware application, extended feature fusion from different neuro-physiological modalities was proposed as well as a close connection with the theoretical emotional framework and the independency of the two emotional variables. Moreover, gender specific classifiers were proposed according to [32] in order to further enhance the method\\\\\\'s robustness which reached 81.3% for 4 emotional categories.Despite the adequate classification accuracy that was demonstrated by these research efforts, there are several open issues that should be further investigated prior to the introduction of real world emotion aware applications. The proposed discrimination framework was developed for research purposes. It is oriented towards the achievement of the optimal result employing time-consuming computations that reduce its applicability. Moreover, it has been developed as an isolated application under controlled lab environments which may differ from generic real-life applications. So, an integrative approach should be adopted for linking the emotion methodology with the acquisition subsystem as well as with the avatar behavior-generation routines. Then, the proposed system would be able to gather short segments of neuro-physiological data which are processed within fractions of seconds. The user\\\\\\'s affective state is recognized and serves as an input to the avatar which adapts its behavior either to mirror or to neutralize the user\\\\\\'s affective state.The current study investigates the feasibility of the Graphics Processing Unit (GPU) for the fast processing of neuro-physiological data. Short segments from both the central (ERPs) and the autonomic (SCRs) nervous system serve as an input to the system. These data are parallel processed during the feature extraction stage by algorithmic procedures that were re-designed in order to provide the optimal solution regarding the memory management. So, the aim of this paper is to demonstrate that the adoption of parallel processing may be greatly beneficial for the development of real-time emotion aware applications. Therefore, it is not focused to the extensive description of the parallelization techniques adopted. Moreover, it highlights some significant issues like time consumption on data transfer between host and device that should be taken into consideration during the system design in order to further minimize the execution time. So, the work\\\\\\'s contribution lays on the introduction of a framework for the adoption of parallel programming for real time emotion-aware applications.So, the remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture as well as with special programming techniques adopted for the proper parallelization of an algorithm. Then, a brief description of the parallelized algorithms is performed. Within Section 3 results of the algorithms’ implementation and the execution time are presented in Section 4. Finally, the discussion of this paper appears in Section 4.## Section snippets## The NVIDIA GPU architecture – CUDAThe voracious market demand for realtime and high definition 3D graphics led to the introduction of highly parallel, multithreaded, manycore processor Graphic Processor Unit (GPU). Characterized by high memory bandwidth and astounding computational horsepower, the GPU (Fig. 1) serves the demanding requirements of the modern designs and implementations. Its main difference with CPU is that it facilitates compute-intensive and parallel computation. Stemming from the graphics rendering demands, it## ResultsThe features (D2 complexity and SCL values) obtained from the parallel processing of electroencephalographic and autonomic data were analyzed in order to highlight differences among the various emotional states. Each emotional state is characterized by two independent variables (valence and arousal degree).Regarding the multi-channel D2 correlation dimension algorithm, the analysis was performed for each participant and for each one of the four emotional categories. As depicted in Fig. 9 (left## DiscussionThe current work aims to highlight the significant acceleration that may be achieved to emotion aware computing in case of adopting parallel programming on GPU. So, the detailed description of the parallelization techniques are beyond the paper\\\\\\'s scope and may be found in [33], [36]. These recent code execution techniques are exploited in boosting complex and time-consuming computations, such as nonlinear dynamic analysis or processing of dense data arrays. Selected results are included in## ConclusionA novel parallel-programming approach based on the CUDA architecture was proposed in order to accelerate the processing of neurophysiological recordings requiring complex computations. It aims to facilitate the already proposed emotion discrimination methodologies with the computing solution needed in order to perform real-time classification. To this end, the importance of this work towards an integrative approach of providing the machines with the capabilities to adapt their behavior## Conflict of interestThe authors do not report any conflict of interest.Special issue articlesRecommended articles- K. Sergerie _et al._### ### Neuroscience and Biobehavioral Reviews(2008)P. Vuilleumier _et al._### ### Nature Neuroscience(2003)- R.W. Picard _et al._### ### IEEE Transactions on Pattern Analysis and Machine Intelligence(2001)- J. Klein _et al._### ### Interacting with Computers(2002)- F. Nasoz _et al._### - C.A. Frantzidis _et al._### - P.J. Lang _et al._### (1997)- C. Amrhein _et al._### ### International Journal of Psychophysiology(2004)- C. Lithari _et al._### ### Brain Topography(2010)- G. Noaje _et al._- K.C. Berridge _et al._### Affective neuroscience of pleasure: reward in humans and animals### Psychopharmacology(2008)A. Bechara _et al._### Emotion, decision making and the orbitofrontal cortex### Cerebral Cortex(2000)M. Pantic _et al._### Toward an affect-sensitive multimodal human–computer interaction### Proceedings of the IEEE(2003)E. Hudlicka### To feel or not to feel: the role of affect in human–computer interaction### International Journal of Human–Computer Studies(2003)R.W. Picard### Affective computing: challenges### International Journal of Human–Computer Studies(2003)B. Kort _et al._### An affective module for an intelligent tutoring systemE. Hudlicka### Affective computing for game designC.A. Frantzidis _et al._### Description and future trends of ICT solutions offered towards independent living: the case of LLM projectP.D. Bamidis _et al._### An integrated approach to emotion recognition for advanced emotional intelligenceB. Reeves _et al._### The Media Equation(1996)R.W. Picard### Affective Computing(1997)View more references- ### 2019, Information FusionCitation Excerpt :Mehmood et al. utilized the late positive potential (LPP)-based feature extraction method and used two classifiers (SVM an KNN) for emotion detection from EEG [44]. Konstantinidis et al. utilized K-nearest neighbors (KNN) classifiers to detect six basic emotions from a three-channel forehead EEG combined with GSR [45]. Recently, Alarcao et al. [10] presented an analysis which shows that 59% of works in the literature used SVM classifiers to detect emotions, 8% of works used different variations of SVM like adaptive SVM etc.; 14% of the works used the kNN, 6.3% of the works used Linear Discriminant Analysis (LDA) and 3.17% of the works used the Naive Bayes (NB) classifier.Show abstractRecently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.- ### 2017, Computer Methods and Programs in BiomedicineShow abstractUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.- ### 2017, Frontiers in NeuroroboticsShow abstractUsing machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features. However, the conventional subject-specific classifiers may induce additional burdens to each subject for preparing multiple-session EEG data as training sets. To this end, we developed a new EEG feature selection approach, transfer recursive feature elimination (T-RFE), to determine a set of the most robust EEG indicators with stable geometrical distribution across a group of training subjects and a specific testing subject. A validating set is introduced to independently determine the optimal hyper-parameter and the feature ranking of the T-RFE model aiming at controlling the overfitting. The effectiveness of the T-RFE algorithm for such cross-subject emotion classification paradigm has been validated by DEAP database. With a linear least square support vector machine classifier implemented, the performance of the T-RFE is compared against several conventional feature selection schemes and the statistical significant improvement has been found. The classification rate and _F_-score achieve 0.7867, 0.7526, 0.7875, and 0.8077 for arousal and valence dimensions, respectively, and outperform several recent reported works on the same database. In the end, the T-RFE based classifier is compared against two subject-generic classifiers in the literature. The investigation of the computational time for all classifiers indicates the accuracy improvement of the T-RFE is at the cost of the longer training time.- ### 2014, Plos OneShow abstractInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method\\\\\\'s practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.- ### 2014, IEEE Transactions on Affective Computing- ### 2013, Computational and Mathematical Methods in MedicineCopyright © 2012 Elsevier Ireland Ltd. All rights reserved.The Identity Selector: Persistence Service\\'}, {\\'title\\': \\'Towards Emotion-awareness in Programming Education with ...\\', \\'url\\': \\'https://ieeexplore.ieee.org/document/9842520/\\', \\'markdown\\': \"Loading [MathJax]/extensions/TeX/ieee_stixext.jsTowards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation | IEEE Conference Publication | IEEE XploreSkip to Main Content# Towards Emotion-awareness in Programming Education with Behavior-based Emotion EstimationPublisher: IEEECite ThisPDF; ; ; ; ; All AuthorsSign In or Purchase1Cites inPaper239FullText Views- AlertsAdd to Citation Alerts* * *- Download PDF- Download References- Request Permissions- Save to- Alerts## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learnin...Show More## Metadata## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learning to program, rich emotions are experienced which can provide valuable feedback to their teachers. To accurately model stu-dents\\' emotions, this paper adopts the well-recognized model of emotions during complex learning that involves four states: engaged, confused, frustrated, and bored. To perform continuous estimation of students\\' emotions in a non-intrusive manner, this paper proposes to track students\\' programming behavior and estimate their corresponding emotional states. Compare to the existing approaches on acquiring the students\\' emotional states with self-reports or bio-sensors, the proposed approach is more feasible in conducting real-world, and large-scale studies for not requiring extensive human interventions or additional devices. Evaluated using data collected from a real-world course project, the proposed approach is showed to be promising for achieving an estimation accuracy of 72.06 % for the above four emotional states. As an enabling technology, the proposed is potentially useful in supporting many applications and improve the quality of programming education in computer science.**Published in:** **Date of Conference:** 27 June 2022 - 01 July 2022**Date Added to IEEE _Xplore_:** 10 August 2022**ISBN Information:****Print on Demand(PoD) ISSN:** 0730-3157**DOI:** Publisher: IEEE**Conference Location:** Los Alamitos, CA, USA## Funding Agency:![]( Contents* * *### I. IntroductionBoth learning and computer programming are high cog-nitively demanding tasks which involve the experience of complex emotions during the process [1]–[3]. Studies from the areas of education [3], psychology [2], and software engineering [1], [4], [5] have more than once shown the significance of one\\'s emotions in affecting the performance and productivity of the learners and developers, respectively. Bringing the above researches together, this paper studies the problem of understanding the students\\' emotions during the task of learning to program.Sign in to Continue Reading## Authors## Figures## References## Citations## Keywords## Metrics## Footnotes]( Like This2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)Published: 20162024 International Conference on Inventive Computation Technologies (ICICT)Published: 2024Show More**References is not available for this document.**### IEEE Account- - ### Purchase Details- - - ### Profile Information- - - ### Need Help?- **US & Canada:** +1 800 678 4333- **Worldwide:** +1 732 981 0060- - - - - - - - - A not-for-profit organization, IEEE is the world\\'s largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.The Identity Selector: Persistence Service\"}]', name='web_search_tool', call_id='call_4PAMAyFwVRDo2ioyrSmVFaI2', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'AI | 2025 Stack Overflow Developer Survey\\', \\'url\\': \\'https://survey.stackoverflow.co/2025/ai\\', \\'markdown\\': \\'### 84% of respondents are using or planning to use AI tools in their development process, an increase over last year (76%). This year we can see 51% of professional developers use AI tools daily.Do you currently use AI tools in your development process?     All RespondentsYes, I use AI tools daily47.1%Yes, I use AI tools weekly17.7%Yes, I use AI tools monthly or infrequently13.7%No, but I plan to soon5.3%No, and I don\\\\\\'t plan to16.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,662(68.7%)Professional DevelopersYes, I use AI tools daily50.6%Yes, I use AI tools weekly17.4%Yes, I use AI tools monthly or infrequently12.8%No, but I plan to soon4.6%No, and I don\\\\\\'t plan to14.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,004(53%)Learning to CodeYes, I use AI tools daily39.5%Yes, I use AI tools weekly18.7%Yes, I use AI tools monthly or infrequently15.1%No, but I plan to soon7.2%No, and I don\\\\\\'t plan to19.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,843(5.8%)Early Career DevsYes, I use AI tools daily55.5%Yes, I use AI tools weekly18.1%Yes, I use AI tools monthly or infrequently11.5%No, but I plan to soon2.5%No, and I don\\\\\\'t plan to12.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,360(13%)Early career defined as 1 - 5 years work experienceMid Career DevsYes, I use AI tools daily52.8%Yes, I use AI tools weekly16.8%Yes, I use AI tools monthly or infrequently13.5%No, but I plan to soon3.7%No, and I don\\\\\\'t plan to13.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,997(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsYes, I use AI tools daily47.3%Yes, I use AI tools weekly17.2%Yes, I use AI tools monthly or infrequently13%No, but I plan to soon6%No, and I don\\\\\\'t plan to16.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 13,001(26.5%)Experienced dev defined as 10+ years work experience### Conversely to usage, positive sentiment for AI tools has decreased in 2025: 70%+ in 2023 and 2024 to just 60% this year. Professionals show a higher overall favorable sentiment (61%) than those learning to code (53%).How favorable is your stance on using AI tools as part of your development workflow?     All RespondentsVery favorable22.9%Favorable36.8%Indifferent17.6%Unsure2.3%Unfavorable10.8%Very unfavorable9.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,412(68.2%)Professional DevelopersVery favorable23.5%Favorable37.7%Indifferent17.4%Unsure1.8%Unfavorable10.6%Very unfavorable9.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,814(52.7%)Learning to CodeVery favorable19.3%Favorable33.5%Indifferent16.6%Unsure4.3%Unfavorable13.6%Very unfavorable12.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,812(5.7%)Early Career DevsVery favorable22.8%Favorable40.3%Indifferent17%Unsure1.3%Unfavorable10.3%Very unfavorable8.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,293(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery favorable23.8%Favorable38.9%Indifferent16.2%Unsure1.5%Unfavorable11%Very unfavorable8.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,957(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery favorable23.9%Favorable36%Indifferent18.1%Unsure2.1%Unfavorable10.3%Very unfavorable9.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,941(26.4%)Experienced devs defined as 10+ years work experience### More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report \"highly trusting\" the output. Experienced developers are the most cautious, with the lowest \"highly trust\" rate (2.6%) and the highest \"highly distrust\" rate (20%), indicating a widespread need for human verification for those in roles with accountability.How much do you trust the accuracy of the output from AI tools as part of your development workflow?     All RespondentsHighly trust3.1%Somewhat trust29.6%Somewhat distrust26.1%Highly distrust19.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,244(67.8%)Professional DevelopersHighly trust2.7%Somewhat trust29.6%Somewhat distrust26.3%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,701(52.4%)Learning to CodeHighly trust6.1%Somewhat trust31.3%Somewhat distrust24.2%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,781(5.7%)Early Career DevsHighly trust3%Somewhat trust31.1%Somewhat distrust25.7%Highly distrust17.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,254(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsHighly trust2.8%Somewhat trust30.3%Somewhat distrust26.1%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,931(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsHighly trust2.5%Somewhat trust28.6%Somewhat distrust26.7%Highly distrust20.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,908(26.3%)Experienced devs defined as 10+ years work experience### In 2024, 35% of professional developers already believed that AI tools struggled with complex tasks. This year, that number has dropped to 29% among professional developers and is consistent amongst experience levels. Complex tasks carry too much risk to spend extra time proving out the efficacy of AI tools.How well do the AI tools you use in your development workflow handle complex tasks?     All RespondentsVery well at handling complex tasks4.4%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.1%Bad at handling complex tasks22%Very poor at handling complex tasks17.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know16.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,230(67.8%)Professional DevelopersVery well at handling complex tasks3.9%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.2%Bad at handling complex tasks22.8%Very poor at handling complex tasks18.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know15.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,695(52.4%)Learning to CodeVery well at handling complex tasks7.9%Good, but not great at handling complex tasks25.8%Neither good or bad at handling complex tasks12.4%Bad at handling complex tasks19%Very poor at handling complex tasks16.3%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,779(5.7%)Early Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks28.1%Neither good or bad at handling complex tasks13.4%Bad at handling complex tasks23.6%Very poor at handling complex tasks19.2%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know11.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,258(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks25.4%Neither good or bad at handling complex tasks13.8%Bad at handling complex tasks23.9%Very poor at handling complex tasks19.5%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know13.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,922(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery well at handling complex tasks3.6%Good, but not great at handling complex tasks23.5%Neither good or bad at handling complex tasks14.9%Bad at handling complex tasks22.1%Very poor at handling complex tasks17.9%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,901(26.3%)Experienced dev career defined as 10+ years work experience### Developers show the most resistance to using AI for high-responsibility, systemic tasks like Deployment and monitoring (76% don\\\\\\'t plan to) and Project planning (69% don\\\\\\'t plan to).Which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently Mostly AISearch for answers54.1%Generating content or synthetic data35.8%Learning new concepts or technologies33.1%Documenting code30.8%Creating or maintaining documentation24.8%Learning about a codebase20.8%Debugging or fixing code20.7%Testing code17.9%Writing code16.9%Predictive analytics11%Project planning10.8%Committing and reviewing code10.2%Deployment and monitoring6.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,202(22.9%)Currently Partially AISearch for answers55.8%Generating content or synthetic data28.6%Learning new concepts or technologies47.4%Documenting code30.3%Creating or maintaining documentation27.3%Learning about a codebase32.7%Debugging or fixing code47.1%Testing code27.5%Writing code59%Predictive analytics12.7%Project planning17.1%Committing and reviewing code22.6%Deployment and monitoring10.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,991(42.8%)Plan to Partially Use AISearch for answers24%Generating content or synthetic data28%Learning new concepts or technologies27.9%Documenting code30.5%Creating or maintaining documentation32.5%Learning about a codebase34.9%Debugging or fixing code30.9%Testing code34.7%Writing code32.4%Predictive analytics25%Project planning24.8%Committing and reviewing code31.4%Deployment and monitoring25%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,518(45.9%)Plan to Mostly Use AISearch for answers17.2%Generating content or synthetic data28.9%Learning new concepts or technologies15.7%Documenting code28.6%Creating or maintaining documentation31.8%Learning about a codebase23.1%Debugging or fixing code14.8%Testing code25.8%Writing code12.4%Predictive analytics23%Project planning14.3%Committing and reviewing code16.3%Deployment and monitoring15.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,790(26.1%)Don\\\\\\'t Plan to Use AI for This TaskSearch for answers19.6%Generating content or synthetic data38.2%Learning new concepts or technologies32.3%Documenting code38.5%Creating or maintaining documentation39.6%Learning about a codebase39.4%Debugging or fixing code36.4%Testing code44.1%Writing code28.9%Predictive analytics65.6%Project planning69.2%Committing and reviewing code58.7%Deployment and monitoring75.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,349(51.7%)### Respondents who said they are currently using mostly AI tools to complete tasks in the development workflow are highly satisfied with and frequently using AI to search for answers or learn new concepts; respondents plan to mostly use AI in the future for documentation and testing tasks and are slightly less satisfied with the tools they are using now.How favorable is your stance on using AI tools as part of your development workflow and which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently mostly AINumber of responses6,053685Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents5.255.35.355.45.455.55.555.65.65%5%10%15%20%25%30%35%40%45%50%55Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,184(22.8%)Currently partially AINumber of responses12,3822,194Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.74.754.84.854.94.9555.055.15.155.25.25%10%15%20%25%30%35%40%45%50%55%60Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,980(42.8%)Plan to partially use AINumber of responses7,8585,400Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents3.73.83.944.14.24.34.44.54.64.74.84.9%24%25%26%27%28%29%30%31%32%33%34%35Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,500(45.9%)Plan to mostly use AINumber of responses4,0561,588Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.64.654.74.754.84.854.94.9555.055.15.155.2%12%14%16%18%20%22%24%26%28%30%32Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,777(26.1%)Don\\\\\\'t plan to use AI for this taskNumber of responses19,2114,953Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents2.42.62.833.23.43.63.84%20%25%30%35%40%45%50%55%60%65%70%75%80Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,332(51.7%)### The biggest single frustration, cited by 66% of developers, is dealing with \"AI solutions that are almost right, but not quite,\" which often leads to the second-biggest frustration: \"Debugging AI-generated code is more time-consuming\" (45%)When using AI tools, which of the following problems or frustrations have you encountered? Select all that apply.All RespondentsAI solutions that are almost right, but not quite66%Debugging AI-generated code is more time-consuming45.2%I don’t use AI tools regularly23.5%I’ve become less confident in my own problem-solving20%It’s hard to understand how or why the code works16.3%Other (write in):11.6%I haven’t encountered any problems4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,476(64.2%)### In a future with advanced AI, the #1 reason developers would still ask a person for help is \"When I don’t trust AI’s answers\" (75%). This positions human developers as the ultimate arbiters of quality and correctness.In the future, if AI can do most coding tasks, in which situations would you still want to ask another person for help? Select all that apply.All RespondentsWhen I don’t trust AI’s answers75.3%When I have ethical or security concerns about code61.7%When I want to fully understand something61.3%When I want to learn best practices58.1%When I’m stuck and can’t explain the problem54.6%When I need help fixing complex or unfamiliar code49.8%When I want to compare different solutions44.1%When I need quick help troubleshooting27.5%Other6.1%I don’t think I’ll need help from people anymore4.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 29,163(59.5%)### Most respondents are not vibe coding (72%), and an additional 5% are emphatic it not being part of their development workflow.In your own words, is \"vibe coding\" part of your professional development work? For this question, we define vibe coding according to the , the process of generating software from LLM prompts.     All RespondentsYes, emphatically0.4%Yes11.9%Yes, somewhat2.8%I have tried it2.1%Not sure1.2%No72.2%No, emphatically5.3%Uncategorized4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,564(54.2%)18-24 years oldYes, emphatically0.3%Yes11.6%Yes, somewhat3.2%I have tried it2.4%Not sure1.2%No72.8%No, emphatically5.1%Uncategorized3.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 4,212(8.6%)25-34 years oldYes, emphatically0.4%Yes11.8%Yes, somewhat3.2%I have tried it1.6%Not sure1.3%No72.3%No, emphatically5.7%Uncategorized3.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,526(17.4%)35-44 years oldYes, emphatically0.5%Yes12%Yes, somewhat2.8%I have tried it2.2%Not sure1.1%No72%No, emphatically5.4%Uncategorized4.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 7,607(15.5%)45-54 years oldYes, emphatically0.5%Yes12.7%Yes, somewhat2.5%I have tried it1.9%Not sure1.3%No71.3%No, emphatically5.2%Uncategorized4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,838(7.8%)55-64 years oldYes, emphatically0.8%Yes11.4%Yes, somewhat2%I have tried it3.1%Not sure1.5%No71.3%No, emphatically4.6%Uncategorized5.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 1,657(3.4%)### AI agents are not yet mainstream. A majority of developers (52%) either don\\\\\\'t use agents or stick to simpler AI tools, and a significant portion (38%) have no plans to adopt them.Are you using AI agents in your work (development or otherwise)? AI agents refer to autonomous software entities that can operate with minimal to no direct human intervention using artificial intelligence techniques.    All RespondentsYes, I use AI agents at work daily14.1%Yes, I use AI agents at work weekly9%Yes, I use AI agents at work monthly or infrequently7.8%No, but I plan to17.4%No, I use AI exclusively in copilot/autocomplete mode13.8%No, and I don\\\\\\'t plan to37.9%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,877(65%)Professional DevelopersYes, I use AI agents at work daily14.9%Yes, I use AI agents at work weekly9.2%Yes, I use AI agents at work monthly or infrequently7.7%No, but I plan to17.2%No, I use AI exclusively in copilot/autocomplete mode14.2%No, and I don\\\\\\'t plan to36.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 24,752(50.5%)Learning to CodeYes, I use AI agents at work daily13.2%Yes, I use AI agents at work weekly7.8%Yes, I use AI agents at work monthly or infrequently7.4%No, but I plan to15.6%No, I use AI exclusively in copilot/autocomplete mode12.1%No, and I don\\\\\\'t plan to44.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,610(5.3%)Professional AI UsersYes, I use AI agents at work daily17.5%Yes, I use AI agents at work weekly10.8%Yes, I use AI agents at work monthly or infrequently8.9%No, but I plan to18.6%No, I use AI exclusively in copilot/autocomplete mode16.3%No, and I don\\\\\\'t plan to27.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,892(42.6%)Learning AI UsersYes, I use AI agents at work daily16.5%Yes, I use AI agents at work weekly9.6%Yes, I use AI agents at work monthly or infrequently8.7%No, but I plan to16.9%No, I use AI exclusively in copilot/autocomplete mode14.7%No, and I don\\\\\\'t plan to33.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,019(4.1%)### 52% of developers agree that AI tools and/or AI agents have had a positive effect on their productivity.Have AI tools or AI agents changed how you complete development work in the past year?All RespondentsYes, to a great extent16.3%Yes, somewhat35.3%Not at all or minimally41.4%No, but my development work has significantly changed due to non-AI factors2.6%No, but my development work has changed somewhat due to non-AI factors4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,636(64.5%)### If you happen to be using AI agents at work and you are a software developer, chances are high that you are using agents for software development (84%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.Industry PurposeSoftware engineering83.5%Data and analytics24.9%IT operations18%Business process automation17.6%Decision intelligence11.3%Customer service support11.2%Marketing8.6%Cybersecurity7.4%Robotics3.9%Other2.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,301(25.1%)### TL;DR: Agents used outside of work are mostly used for language processing tasks (49%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.General PurposeLanguage processing49%Integration with external agents and APIs38.3%MCP servers34.4%Agent/multi-agent orchestration28.1%Vector databases for AI applications24.1%Multi-platform search enablement19.4%Personalized agent creation18.3%Other3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,797(11.8%)### The most recognized impacts are personal efficiency gains, and not team-wide impact. Approximately 70% of agent users agree that agents have reduced the time spent on specific development tasks, and 69% agree they have increased productivity. Only 17% of users agree that agents have improved collaboration within their team, making it the lowest-rated impact by a wide margin.To what extent do you agree with the following statements regarding the impact of AI agents on your work as a developer?All Respondents27.3%35.9%21.3%8.2%7.3%AI agents have accelerated my learning about new technologies or codebases.29.3%34.9%22.4%7%6.4%AI agents have helped me automate repetitive tasks.17.1%31.9%25.3%14.2%11.5%AI agents have helped me solve complex problems more effectively.6.6%10.7%40.5%20%22.2%AI agents have improved collaboration within my team.12.2%25.3%32.4%17.1%13.1%AI agents have improved the quality of my code.27.7%41%20.4%6%4.9%AI agents have increased my productivity.29.3%40.8%17.8%6.9%5.1%AI agents have reduced the time spent on specific development tasks.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,823(26.2%)### Is it a learning curve, or is the tech not there yet? 87% of all respondents agree they are concerned about the accuracy, and 81% agree they have concerns about the security and privacy of data.To what extent do you agree with the following statements regarding AI agents?All Respondents57.1%29.8%9.7%2.3%1.1%I am concerned about the accuracy of the information provided by AI agents.56.1%25.3%11.7%4.7%2.2%I have concerns about the security and privacy of data when using AI agents.16.5%29.7%37.3%12.6%3.9%Integrating AI agents with my existing tools and workflows can be difficult.15.5%27.9%31.8%17.8%6.9%It takes significant time and effort to learn how to use AI agents effectively.13.8%14.4%30.6%15%26.2%My company\\\\\\'s IT and/or InfoSec teams have strict rules that do not allow me to use AI agent tools or platforms25.4%27.9%31.8%10.3%4.6%The cost of using certain AI agent platforms is a barrier.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 28,930(59%)### When it comes to data management for agents, traditional, developer-friendly tools like Redis (43%) are being repurposed for AI, alongside emerging vector-native databases like ChromaDB (20%) and pgvector (18%).You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent memory or data management in the past year?All RespondentsRedis42.9%GitHub MCP Server42.8%supabase20.9%ChromaDB19.7%pgvector17.9%Neo4j12.3%Pinecone11.2%Qdrant8.2%Milvus5.2%Fireproof5%LangMem4.8%Weaviate4.5%LanceDB4.4%mem04%Zep2.8%Letta2.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,398(6.9%)### The agent orchestration space is currently led by open-source tools. Among developers building agents, Ollama (51%) and LangChain (33%) are the most-used frameworks.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent orchestration or agent frameworks in the past year?All RespondentsOllama51.1%LangChain32.9%LangGraph16.2%Vertex AI15.1%Amazon Bedrock Agents14.5%OpenRouter13.4%Llama Index13.3%AutoGen (Microsoft)12%Zapier11.8%CrewAI7.5%Semantic Kernel6%IBM watsonx.ai5.7%Haystack4.4%Smolagents3.7%Agno3.4%phidata2.1%Smol-AGI1.9%Martian1.7%lyzr1.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,758(7.7%)### Developers are primarily adapting their existing, traditional monitoring tools for this new task, rather than adopting new, AI-native solutions. The most used tools for AI agent observability are staples of the DevOps and application monitoring world: Grafana + Prometheus are used by 43% of agent developers, and Sentry is used by 32%.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent observability, monitoring or security in the past year?All RespondentsGrafana + Prometheus43%Sentry31.8%Snyk18.2%New Relic13%LangSmith12.5%Honeycomb8.8%Langfuse8.8%Wiz6.9%Galileo6.2%Adversarial Robustness Toolbox (ART)5.5%Protect AI5%Vectra AI4.4%arize3.7%helicone3.2%Metero2.7%opik2.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,689(5.5%)### ChatGPT (82%) and GitHub Copilot (68%) are the clear market leaders, serving as the primary entry point for most developers using out-of-the-box AI assistance.You indicated you use or develop AI agents as part of your development work. Have you used any of the following out-of-the-box agents, copilots or assistants?All RespondentsChatGPT81.7%GitHub Copilot67.9%Google Gemini47.4%Claude Code40.8%Microsoft Copilot31.3%Perplexity16.2%v0.dev9.1%Bolt.new6.5%Lovable.dev5.7%AgentGPT5%Tabnine5%Replit5%Auto-GPT4.7%Amazon Codewhisperer3.9%Blackbox AI3.5%Roo code (Roo-Cline)3.4%Cody3%Devin AI2.7%Glean (Enterprise Agents)1.3%OpenHands (formerly OpenDevin)1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,323(17%) \\'}, {\\'title\\': \\'AI-Generated Code Stats 2025: How Much Is Written by AI?\\', \\'url\\': \\'https://www.elitebrains.com/blog/aI-generated-code-statistics-2025\\', \\'markdown\\': \"26 min read08/27/2025# AI-Generated Code Stats 2025: How Much Is Written by AI?#### AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development.Hey there, fellow tech enthusiasts! One thing is crystal clear as we look to the future: artificial intelligence (AI) is rapidly changing how we write code. From building entire functions to completing code snippets in milliseconds, AI is helping developers work faster and smarter. But the big question on everyone’s mind is: **How much of that code is AI-generated?** And more importantly, should developers or  worry about their jobs as AI evolves?As the founder of EliteBrains, where we focus on helping developers stay sharp with coding tests and skills development, I want to break down the AI-generated code stats for 2025, explore the tools that are making this happen, and give you a better understanding of what the future holds for software developers in this new world of AI-assisted coding.As AI takes over more of the repetitive coding tasks, the real question becomes how we measure human skill alongside machine-generated output. That’s exactly where **EliteBrains helps.**Put AI to the test and see which of your developers are truly qualified.\\\\xa0Mix and match AI-powered coding tests from 120+ programming languages, choose the right seniority level, and instantly spot who rises above the rest.Create your\\\\xa0**FREE AI coding tests today** **( only\\\\xa03 free slots available)**## **What Is AI-Generated Code?**AI-generated code is programming written or completed by artificial intelligence tools from natural language prompts, existing context or prior code patterns. In 2025, it typically means an AI assistant drafted the solution or produced most of the final lines that a developer then reviews and refines.## **What Percentage of \\\\xa0Code Is AI-Generated in 2025?**AI has been in the software development game for a while now, but its role in coding is reaching new heights in 2025. It’s not just about suggesting code anymore. AI tools are writing **massive** amounts of code, optimizing development, and helping developers be more efficient than ever before.Here’s the shocking truth: . That number alone shows just how integrated AI tools like **GitHub Copilot** have become in developers’ workflows. Whether it’s generating entire functions or suggesting small snippets of code, AI is now deeply embedded into the way we code.## **How Much Code Has AI Written So Far?**But just how much code is AI generating? Let’s break it down.![]( **How Does AI Code Generation Affect Developer Jobs and the Economy?**With AI generating such a massive amount of code, one big question looms: What does this mean for job security and the economy?![]( **How Many Lines of Code Have Been Generated by AI?**Buckle up, because here’s where it gets mind-blowing. As of 2024, **256 billion lines of code have already been generated by AI**. Think about that for a second—**256 billion lines**. That’s an insane amount of code, and we’re just getting started. From simple scripts to full-fledged software systems, AI is helping to write an overwhelming amount of code across all industries.This number is so massive that it\\'s hard to grasp. Consider this: the **total lines of code** written by humans in the last decade likely pales in comparison to this explosion of AI-generated code. And we’re still in the early stages of integrating AI tools into the development process. The future will see even more code churned out by AI as the tools become smarter, faster, and more efficient.## **How Is Google Using AI to Write Code in 2025?**Let’s look at how AI is playing a huge role in tech giants like Google. Google, known for its cutting-edge AI innovations, has been using AI to power some of its most important software. According to Sundar Pichai, Google’s CEO, **over a quarter of all new code generated by Google is written by AI**. This includes everything from Google Cloud services to Android apps. AI is also playing a significant role in developing machine learning models that power a large chunk of Google’s products.Pichai’s announcement isn’t just a marketing stunt—it’s a reflection of how deeply integrated AI has become in Google’s development process. Whether it\\'s AI-driven infrastructure management or software development for consumer-facing products, Google is leading the way in showing just how powerful AI can be for developers.Let’s dive deeper into why Google has taken this approach. With over 20,000 engineers working on various products, relying on AI to handle repetitive, time-consuming tasks such as code generation allows the company to save countless hours. This, in turn, frees up developers to focus on more complex tasks that require problem-solving skills, creativity, and innovation.## **Will AI Replace Software Developers in 2025?**As AI continues to make its mark on software development, many developers are wondering if their jobs are at risk. Should they be worried? The short answer is **no**but let’s unpack that.AI isn’t here to replace developers. Far from it. AI tools are designed to assist developers, not eliminate them. These tools automate repetitive and mundane tasks, such as writing boilerplate code or generating simple functions. But when it comes to the creative, high-level work of designing software, making complex decisions, and debugging, developers are still very much in the driver’s seat.For example, imagine a scenario where a developer is working on building a web application. AI can generate the basic structure of the code such as setting up routing, creating a simple login form, or connecting to a database but it cannot make decisions about the best UI layout for user experience or develop the logic that handles complex business rules. That’s where the developer’s creativity and problem-solving skills come into play.AI is an assistant, not a replacement. It helps developers write code faster, but it doesn’t have the creativity, problem-solving skills, or domain knowledge that human developers bring to the table. In fact, AI is actually helping developers become more productive, allowing them to focus on more important aspects of the development process.The key takeaway here is that AI is making software development more efficient, but it still needs a human touch for the final product to succeed.### **How Much Code Is AI-Generated on GitHub in 2025?**GitHub is one of the largest code repositories in the world, and it has seen a massive increase in AI-generated code in recent years. That’s not a small chunk—it’s a significant portion of the code that powers everything from web applications to mobile apps.This doesn’t mean that developers are being replaced. Instead, it shows that developers are using AI to streamline their workflows and focus on more complex tasks. **GitHub Copilot** is perhaps the most popular AI tool used for code generation on the platform. It suggests entire lines of code and can even write entire functions based on developer input. It’s designed to help developers write code faster, with fewer bugs, and more efficiently.By integrating Copilot into their workflow, developers save time and avoid getting bogged down by menial tasks, ultimately improving both speed and quality.## **Which Programming Languages and Frameworks Does AI Write Best?**One of the most exciting things about AI in software development is its ability to generate code across a wide range of programming languages and frameworks. Here’s a breakdown of the most popular ones:#### **Python:**Python has become one of the most popular programming languages, especially in fields like data science, machine learning, and web development. AI tools like GitHub Copilot are particularly skilled at generating Python code for these use cases, whether you’re building machine learning models or working with web frameworks like Django and Flask.#### **JavaScript:**JavaScript is the backbone of web development, and AI tools are excellent at generating code for JavaScript frameworks like React, Angular, and Vue.js. AI can generate entire components, hooks, and functions in JavaScript, saving developers a ton of time and effort.#### **Java:**Java remains one of the most widely used programming languages in enterprise software development. AI can help developers generate Java code for popular frameworks like Spring and Hibernate. AI can also generate code for building microservices and APIs, speeding up development time.#### **C and C++:**AI tools are also capable of generating code in low-level programming languages like C and C++, which are crucial for systems programming and embedded systems. AI is particularly helpful in optimizing low-level code for performance and efficiency.#### **Ruby, Swift, PHP, and Go:**AI is also being used to generate code in languages like Ruby, Swift (for iOS development), PHP, and Go. These languages are frequently used for web development, mobile apps, and cloud applications, and AI is helping developers write faster and more efficient code across these platforms.## **What Are the Best AI Code Generation Tools in 2025?**Now, let’s take a look at the tools that are making this all happen. These tools are transforming the way developers work, making coding faster and more efficient:#### **GitHub Copilot:**GitHub Copilot is arguably the most well-known AI tool for developers. Powered by OpenAI’s Codex, Copilot offers suggestions for entire lines or blocks of code as developers work. It supports a wide range of languages and frameworks, including Python, JavaScript, Java, and more. Copilot is designed to help developers write code faster and with fewer errors.Source: #### **Amazon CodeWhisperer:**Amazon’s CodeWhisperer is another powerful AI tool designed to assist developers in generating code for cloud-based applications. It integrates seamlessly with Amazon Web Services (AWS), allowing developers to write code for serverless applications, APIs, and more. CodeWhisperer is especially useful for developers working in the AWS ecosystem.#### **Tabnine:**Tabnine is an AI-powered code completion tool that integrates with popular code editors like VS Code, JetBrains, and more. It offers context-aware code suggestions to help developers write code more quickly and efficiently. Tabnine works with over 40 programming languages, making it a versatile tool for developers.#### **Replit Ghostwriter:**Replit’s Ghostwriter is another AI tool designed for real-time collaborative coding. It provides developers with AI-generated code suggestions and helps speed up the development process. Whether you’re working on a solo project or collaborating with others, Ghostwriter helps you write code faster and more efficiently.**Should Developers Be Worried About AI in Coding?**So, let’s get back to the big question: should developers worry about their jobs in an AI-driven world? The answer is no, but there’s a catch.While AI is indeed transforming the way we write code, it’s not replacing developers. Instead, it’s empowering them to be more productive and efficient. Developers are still needed for creative problem-solving, high-level system design, and debugging. AI tools can generate code, but they can’t design systems, make architectural decisions, or think creatively about solutions. That’s where developers come in.The same idea holds true across industries. AI can build websites or generate campaigns, but real growth still comes from people who understand the field. Veterinary practices, for example, often lean on expert guidance like this , just as tech companies depend on developers for the creativity and insight that AI cannot replace.What’s really changing is the developer’s role. Instead of getting bogged down with repetitive tasks like boilerplate code, they now have the chance to focus on more interesting, high-value work. Those who embrace AI as a tool rather than view it as a threat will find their jobs not just safe but more important than ever.### **How Can Developers Stay Ahead of AI in Coding with EliteBrains?**At , we understand the importance of staying ahead of the curve. As AI continues to transform the software development industry, we’re here to help developers stay competitive. With our **coding tests**, developers can hone their skills, stay updated on new trends, and ensure they’re prepared for the future of software development.Our platform allows developers to:- **Practice with AI-assisted tools** and sharpen their skills- **Test their knowledge** in real-world coding environments- Stay up to date with **emerging trends in AI and software development**- **Learn how to integrate AI** into their development workflow to increase efficiencyAt EliteBrains, we’re committed to helping developers succeed in an AI-powered world, ensuring they remain competitive and continue to thrive.### **Conclusion: AI and Developers—A Winning Partnership**The rise of AI in software development is reshaping the way we write code, but it’s not here to replace developers. Instead, it’s here to make their jobs easier and more efficient. By embracing AI tools like GitHub Copilot, Amazon CodeWhisperer, and Tabnine, developers can work smarter, not harder.AI is an assistant, not a replacement. The key for developers is to adapt and use these tools to their advantage. The future of software development is collaborative—AI working alongside developers to create better, faster, and more innovative software.With **EliteBrains**, developers can stay ahead of the curve, test their skills, and learn how to leverage AI to boost their productivity. The future of coding is bright, and it’s full of exciting opportunities for those who are willing to embrace the change.01. 02. 03. 04. 05. 06. 07. 08. 09. 10. #### Our clients say!With senior resources from EliteBrains, we are able to drive our digital harmonization agenda: Migration of different reporting platforms to QlikSense analytics and Harmonization of different digital detailing systems into Veeva CLM.!Jiri Matousek, Head of CRM at Hexal AG!EliteBrains provided us with a top PHP developer who has been working for us for more than 4 years. Jan became indispensable part of our team within few weeks.!Dave Etchells, CEO at Imaging-resource.com!Each time we are in a need of new software developer, we just contact the guys from EliteBrains. Their network is huge so they provide us with demanded sources very quickly.!Martin Svach, CEO at Starkys club#### Useful articles]( Do Resumes Still Matter for Backend Developers in 2025? Here’s How to Build One That Gets You Hired Faster 02. 09. 2025 . 5 min read]( AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development. 27. 08. 2025 . 11 min readSign up now to see more tipsSubscribecancel anytime#### Hiring?We picked the best developers for you#### Looking for a job?## This website uses cookiesWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Accept allManage preferencesChat Widget\"}]', name='web_search_tool', call_id='call_MZOCIL7HSdei6STfRI8fQssA', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(id='9865acc9-bf2f-47d6-a79d-1e7f0d1c2c8d', source='research_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 32, 57, 223142, tzinfo=datetime.timezone.utc), content='Unexpected error during search: Status code 502. Server returned non-JSON response: \\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>502 Server Error</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Server Error</h1>\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\n<h2></h2>\\n</body></html>\\n - Full response status: 502\\n[{\\'title\\': \\'2025년 바이브 코딩 AI 개발 동향 보고서\\', \\'url\\': \\'https://seo.goover.ai/report/202506/go-public-report-ko-e086645e-079a-44f8-ae63-13f0358d262d-0-0.html\\', \\'markdown\\': \"![]( 2025년 바이브 코딩 AI 개발 동향 보고서일반 리포트2025년 06월 16일!## 목차1. 요약2. 서론3. 데이터브릭스의 통합 AI 플랫폼 전략4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황5. 최고의 AI 코드 생성 도구 비교 분석6. 미래 전망 및 시사점7. 결론## 1. 요약- 2025년 바이브 코딩 AI 개발 동향 리포트에서는 인공지능 기술의 발전에 따라 변화하고 있는 소프트웨어 개발 환경을 다룹니다. 특히, \\'바이브 코딩\\'이라는 새로운 개념을 중심으로 데이터브릭스의 통합 플랫폼 전략과 스타트업 경쟁 구도를 분석합니다. 이 보고서의 핵심 발견은, 데이터브릭스가 제시하는 레이크베이스 플랫폼과 AI 자동화 도구가 기업들이 데이터와 AI를 효과적으로 통합하고 활용할 수 있는 환경을 제공한다는 것입니다. 바이브 코딩 스타트업인 Cursor, Windsurf, Codeium 등의 사례와 90억 달러 이상의 기업 가치를 통해 AI 코드 생성 시장의 동향이 더욱 뚜렷하게 드러나며, 개발자 역할의 변화와 더불어 AI 도구의 중요성이 지속적으로 증가하고 있음을 보여줍니다.## 2. 서론- AI 기술이 급속히 발전함에 따라 소프트웨어 개발의 패러다임이 전환되고 있습니다. 이러한 변화의 중심에 자리한 \\'바이브 코딩\\'은 개발 지식이 부족한 사용자들도 손쉽게 코드를 작성할 수 있는 기회를 제공합니다. 비록 이 기술이 새로운 가능성을 창출하지만, 동시에 기존의 개발자 일자리에 위협이 되고 있습니다. 본 리포트는 바이브 코딩을 통해 신생 기업과 대규모 플랫폼 간의 경쟁 구도, 그리고 AI 코드 생성 도구의 진화에 대해 심층적으로 분석하고자 합니다. 현재 시장에서 주목받고 있는 데이터브릭스의 레이크베이스 플랫폼 전략을 시작으로, 여러 스타트업 사례와 AI 코드 생성 도구의 특징을 살펴보며, 미래의 개발 환경이 어떤 방향으로 나아갈지를 예측합니다.## 3. 데이터브릭스의 통합 AI 플랫폼 전략- 인공지능의 세계가 진화하는 과정에서 데이터브릭스는 두 가지 측면에서 주목할 만한 변화를 이끌고 있습니다. 첫째, 인공지능의 사용 범위를 확장하는 \\'바이브 코딩(vibe coding)\\' 개념의 도입과 둘째, 이를 바탕으로 한 새로운 플랫폼인 레이크베이스(Lakebase)의 출시입니다. 데이터브릭스는 근본적으로 기업의 데이터 생태계를 통합하고, AI 중심의 혁신을 통해 기업들이 데이터를 더 효과적으로 활용할 수 있도록 새로운 방향성을 제시하고 있습니다.- ### 3-1. ‘바이브 코딩’ 개념과 Databricks의 레이크베이스(Lakebase) 출시- ‘바이브 코딩’은 데이터브릭스가 최근 발표한 혁신적인 개념으로, 자연어 기반의 AI 코드 생성과 업무 자동화를 통합한 것을 의미합니다. 사용자가 원하는 작업을 자연어로 설명하면, 시스템은 내장된 데이터와 AI 모델을 활용하여 적절한 코드를 자동으로 작성합니다. 이 접근 방식은 개발자들이 직접 코드를 입력하는 대신 시스템과 대화하며 작업을 수행할 수 있도록 돕습니다. 이는 기존의 코드 중심 개발 방식을 넘어서는 중요한 이정표로서, AI의 역할을 더 효율적이고 인간 친화적으로 만들어 줍니다.- 데이터브릭스는 데이터+AI 서밋 2025에서 새로운 플랫폼인 레이크베이스를 공식 출시했습니다. 레이크베이스는 서버리스 OLTP(온라인 트랜잭션 처리) 데이터베이스로, PostgreSQL과 호환되며 오브젝트 스토리지와의 통합 기능을 갖추고 있습니다. 이 플랫폼은 데이터 처리의 효율성과 확장성을 모두 만족시키며, 기업들이 데이터 분석, 트랜잭션 처리, AI 개발을 하나의 플랫폼 내에서 유기적으로 연결될 수 있는 환경을 제공합니다. 이는 기업의 데이터 생태계를 통합하는 데 있어 중대한 시점입니다.- ### 3-2. Agent Bricks와 AI 판사 기능 설명- 에이전트 브릭스(Agent Bricks)는 데이터브릭스의 또 다른 혁신으로, 자연어 명령을 통해 정보 추출, 지식 지원, 멀티에이전트 감시 등의 역할을 수행하는 AI 에이전트를 자동으로 생성할 수 있는 도구입니다. 이 시스템은 사용자 오류를 최소화하고, 정보의 질을 높이는 방향으로 설계되었습니다.- 특히 주목할 만한 것은 AI 판사(judges) 기능입니다. 이 기능은 AI 에이전트의 성능을 모니터링하고 평가하는 데 사용되며, 특정 기준에 따라 생성된 결과의 질을 점검하고 개선하는 역할을 합니다. 이를 통해 에이전트들은 학습하고 발전할 수 있으며, 최적의 결과를 얻기 위해 코드 수정 과정을 연구하게 됩니다. 데이터브릭스는 이러한 기능들을 통해 정보 활용의 효율성을 극대화하고, AI의 잠재력을 더욱 확장하고자 합니다.- ### 3-3. 플랫폼 통합으로 달성하는 데이터·AI 워크플로우 혁신- 데이터브릭스의 통합 AI 플랫폼 전략은 단순한 기술 통합을 넘어 데이터와 AI가 밀접하게 연결된 생태계를 형성하는 데 기여하고 있습니다. BI(비즈니스 인텔리전스) 툴과 머신러닝 모델을 통합함으로써 기업들은 데이터 기반 의사결정을 신속히 내릴 수 있는 능력을 갖추게 됩니다. 이는 데이터 흐름을 자동화하고, 실시간으로 분석을 가능하게 하여 비즈니스의 민첩성을 높이는 중요한 단계입니다.- 임무별 에이전트와 AI 분석 툴 간의 협업은 기업 내에서 심층적인 데이터 분석을 수행할 수 있도록 하며, 사용자들은 실시간으로 인사이트를 확보하게 됩니다. 이러한 혁신은 기존의 데이터 분석, 데이터 관리 및 AI 모델에 대한 접근 방식을 바꾸고 있으며, 기업들이 더욱 빠르게 변화하는 시장에서 경쟁력을 유지할 수 있도록 돕습니다. 데이터브릭스는 AI 자동화와 데이터 관리의 통합을 통해 이 전환을 가속화하고 있으며, 이는 향후 기업의 데이터와 AI 활용 방식에 지대한 영향을 미칠 것입니다.## 4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황- AI 기술의 급속한 발전은 소프트웨어 개발 생태계에 혁신적인 변화를 불러오고 있습니다. 특히, 바이브 코딩(vibe coding)이라는 개념이 등장하면서 일반 사용자도 손쉽게 코드 작성이 가능해졌습니다. 이는 프로그래밍 지식을 가진 개발자들에게만 국한되지 않고, 폭넓은 사용자에게 소프트웨어 개발의 기회를 제공합니다. 이러한 변화는 단순히 기술적인 것에 그치지 않고 투자 시장에서도 큰 물결을 일으키고 있습니다. 이번 섹션에서는 바이브 코딩 스타트업의 경쟁 구도와 최근 투자 현황에 대해 분석하겠습니다.- 2025년 현재, 바이브 코딩 시장에서 주목받고 있는 스타트업은 Cursor, Windsurf, Codeium 등입니다. 특히, Cursor는 최근 90억 달러의 기업 가치에 도달하며 10억 달러 규모의 투자를 유치했습니다. 이들은 \\'AI 코드 생성\\'을 통해 프로그래밍 언어에 대한 깊은 이해 없이도 소프트웨어를 개발하는 것을 가능하게 하여, 이를 통해 많은 사용자가 개발에 참여할 수 있도록 하고 있습니다. Windsurf와 Codeium 역시 30억 달러 가치를 인정받으며, 오픈AI의 인수 협상 대상으로 떠오르는 등 바이브 코딩 시장에서 활발한 활동을 보이고 있습니다.- ### 4-1. Cursor·Windsurf·Codeium 등 주요 스타트업 사례- Cursor는 인공지능을 활용한 코드 생성 플랫폼을 제공하며, 사용자가 쉽게 코드 스니펫을 생성하고 통합할 수 있도록 돕습니다. 또한, 이 플랫폼은 커뮤니티와의 협업 기능을 강화하여 다양한 사용자가 서로의 코드 작업을 공유할 수 있게 하며, 이는 소프트웨어 개발이 전통적인 개발 흐름을 넘어선 새로운 차원으로 나아가게 합니다. Windsurf는 Codeium 개발을 통해 비개발자들도 코드 작성에 참여할 수 있는 환경을 조성하고 있으며, 이를 통해 소프트웨어 개발의 인력 부족 문제를 해결하고 있습니다.- 또한, 이들 스타트업들은 단순한 코드 작성 도구를 넘어, 사용자 경험을 최적화하고 개발자 커뮤니티와의 연결성을 증대시키기 위한 방향으로 발전하고 있습니다. 이들은 AI 코딩 도구의 수요와 더불어, 사용자 친화적인 인터페이스를 제공함으로써 모바일 기기 및 다양한 플랫폼에서의 접근성을 높이고 있습니다.- ### 4-2. AI 코드 생성 스타트업의 밸류에이션 상승 배경- AI 코드 생성 스타트업들의 밸류에이션 상승은 몇 가지 주요 요인에 기인합니다. 첫째, 기업들이 AI 도입을 통해 업무 생산성을 극대화하고 싶어하는 경향이 있습니다. 소프트웨어 개발 분야에서 AI의 활용은 코딩을 자동화하고, 반복적인 업무를 감소시켜 기업들이 원하는 시간 안에 프로젝트를 완수하도록 돕고 있습니다. 이에 따라 AI 코드 생성 스타트업은 높은 수익 성장 잠재력을 지닌 것으로 평가받고 있습니다.- 둘째, AI 기술에 대한 투자자들의 관심이 급증하면서 스타트업들이 쉽게 자금을 조달할 수 있는 환경이 조성되었습니다. 특정 투자자들은 AI가 가져올 수 있는 혁신성과 시장의 미래 가능성을 높게 평가하고 있으며, 이는 스타트업 가치의 상승으로 이어지고 있습니다.- ### 4-3. 엔트리 레벨 개발자 일자리 변화 및 빅테크 진출 현황- AI의 도입은 엔트리 레벨 개발자 일자리에 큰 변화를 일으키고 있습니다. 최근 통계에 따르면, AI가 반복적이고 기본적인 코딩 작업을 대체하면서 1년 미만 경력의 개발자 신규 채용이 24% 감소한 것으로 나타났습니다. 이는 기술 발전이 개발자들의 일자리 환경에 어떠한 영향을 끼치고 있는지를 잘 보여줍니다.- 더불어, 구글, 아마존, 마이크로소프트(MS)와 같은 빅테크 기업들은 자체적인 AI 코드 생성 제품을 출시하며 이 시장에 진입하고 있습니다. 이러한 방식으로 이들 기업은 인건비를 절감하고 효율성을 극대화하고 있습니다. 예를 들어, MS는 올해 6000명 감원을 통해 개발자 역할의 변화가 불가피함을 강조하며, 자동화 기술이 고용에 미치는 영향을 조명하고 있습니다.## 5. 최고의 AI 코드 생성 도구 비교 분석- AI 코드 생성 도구는 현대 소프트웨어 개발의 필수 요소로 자리 잡았습니다. 코드 생성의 혁신이 이루어짐에 따라, 개발자들은 반복적인 작업에서 벗어나 더 창의적이고 전략적인 업무에 집중할 시간을 확보할 수 있게 되었습니다. 이러한 도구들은 단순히 코드를 생성하는 것을 넘어서, 개발자의 작업 흐름을 혁신하고, 팀 협업을 강화하며, 코드 품질을 향상시키는 역할을 합니다. 따라서 다양한 AI 코드 생성 도구들의 기능, 장점과 단점을 분석하는 것은 이 시대의 개발 환경을 이해하는 데 필요합니다.- AI 코드 생성기는 이제 단순한 도우미가 아니라, 실제로 개발과정의 핵심적인 파트너로 성장하고 있습니다. 특정 도구의 선택은 프로젝트의 성공 여부를 크게 좌우할 수 있으며, 여러 도구 중 어떤 것이 가장 적합한지는 사용자의 요구와 팀의 구조에 따라 달라질 수 있습니다. 따라서 이러한 도구들을 비교하고 분석하는 것은 매우 중요한 작업입니다.- ### 5-1. 10가지 주요 AI 코드 생성기 기능·장단점 요약- 다양한 AI 코드 생성기 중에서 10개의 주요 도구를 선정하여 그 기능과 장단점을 요약하였습니다. 각 도구는 특정 기능에 특화되어 있으며, 사용 용도에 따라 효과적인 선택이 이루어져야 합니다. 예를 들어, GitHub Copilot은 강력한 코드 자동 완성 기능과 통합된 학습 기능을 통해 개인 개발자와 팀 모두에게 유용하나, 보안상의 이유로 민감한 프로젝트에는 적합하지 않을 수 있습니다.- 반면에, Replit은 사용자 친화적인 인터페이스와 팀 협업 기능을 제공하여 초보자와 경력 개발자 모두에게 적합합니다. 그러나 상대적으로 많은 기능이 통합되어 인프라 관리에 추가적인 학습이 필요할 수 있습니다. 각 도구의 인상적인 기능은 다음과 같습니다. 1) GitHub Copilot: 예측 코드 생성, 다국어 기능, 지속적인 학습. 2) Replit: 실시간 협업, 객체 저장소, 에이전트 기능. 3) Cursor: 스마트 자동 완성 및 상황 인식. 4) Lovable.dev: 풀스택 개발 환경, 연속적 호스팅, GitHub 통합.- 각 도구의 장단점을 제대로 이해하고 활용한다면, 기업의 코드 작성 프로세스를 획기적으로 개선할 수 있습니다.- ### 5-2. Replit 등 대표 플랫폼의 에이전트·배포·협업 기능- Replit은 클라우드 기반 환경에서 에이전트를 활용하여 애플리케이션 개발을 지원하는 대표적인 플랫폼입니다. 에이전트 v2는 사용자의 대화 내용을 바탕으로 자연어로 명령을 수행하고, 코드를 작성하는 과정에서 기계 학습을 통해 더 나은 결과를 생성합니다. 사용자는 오히려 더 많은 시간을 절약하고 창의적인 작업에 집중할 수 있습니다.- 또한, Replit의 협업 기능은 팀 프로그래밍에 최적화되어 있습니다. 팀원들은 실시간으로 함께 코드 작성, 디버깅 및 배포 작업을 수행할 수 있어 작업 효율성을 높입니다. 이러한 협업 기능은 특히 소규모 팀과 스타트업에서 효과적으로 나타납니다. 하지만 클라우드 기반 서비스이기 때문에 인터넷 연결이 필수적이며, 민감한 데이터를 처리하는 데 있어 일부 사용자에게 불안감을 줄 수 있습니다.- 편리함과 안정성만을 찾는 사용자를 위해 Windsurf와 같은 로컬 IDE 기반의 코드 편집기 또한 주목받고 있습니다. 이는 클라우드와의 연결이 필요 없으며, 오프라인 상태에서도 높은 성능을 발휘합니다. 각 플랫폼의 특성을 고려하여 조합하면 더 효과적인 개발 환경을 구축할 수 있습니다.- ### 5-3. 도구별 활용 시나리오 및 성능 비교- AI 코드 생성 도구들은 각각의 기능과 특성에 따라 다양한 활용 시나리오가 존재합니다. 예를 들어, GitHub Copilot은 대규모 오픈소스 프로젝트에서 일반적으로 사용되며, 누구에게나 접근이 용이한 코드 제안을 통해 신속한 개발을 도와줍니다. 와 같은 대규모 솔루션에서는 보안이 필수적이므로 지극히 주의가 필요합니다.- 반면 Replit은 시작 단계의 스타트업이나 부트캠프 등에서 인기를 끌고 있으며, 초보자들이 스크립트를 작성하고 간단한 애플리케이션을 만들기에 적합합니다. 이러한 교육적인 측면도 고려해야 합니다. Windsurf는 보다 고급 코드 편집 및 오프라인 환경이 필요한 사용자에게 적합하여 프리랜서 개발자들이나 개인 프로젝트를 추진하는 이들에게 이상적입니다.- 기본적으로 각 도구의 성능은 사용자의 요구와 프로젝트의 특성에 따라 매우 다르게 나타납니다. 예를 들어, 고급 사용자는 지연 시간과 성능을 중시할 수 있지만, 초보자는 사용자 경험과 접근성에 중점을 두는 경향이 있습니다. 그러므로 이러한 도구를 선택할 때에는 반드시 사용자의 요구와 환경을 충분히 고려해야 합니다.## 6. 미래 전망 및 시사점- AI의 발전은 단순한 기술 혁신을 넘어, 인류의 생활 방식과 일하는 방식을 근본적으로 변화시키고 있습니다. 특히 바이브 코딩의 확산은 이러한 변화의 중심에서 새로운 가능성을 열어주고 있습니다. 이 과정을 통해 개발자, 기업, 그리고 사회 전체가 어떻게 적응할 것인지에 대한 논의는 앞으로의 방향성을 잡는 데 중요합니다. 따라서 AI의 자동화가 가져올 개발자 역할의 변화와 플랫폼과 스타트업 간의 연계, 기술의 상호 발전 방향에 대한 통찰이 필요합니다.- ### 6-1. 주요 기술의 상호 보완적 발전 방향- AI 코드 생성기와 자연어 프로세싱(NLP) 기술의 발전은 서로를 보완하며 지속적으로 진화하고 있습니다. 예를 들어, Replit과 같은 플랫폼은 비 프로그래머도 소프트웨어 개발에 참여할 수 있도록 도와주며, 이는 결국 개발 생태계의 다변화를 가져옵니다. 이처럼 다양한 도구가 상호 연결됨으로써, 사람들이 코드를 작성하는 방식을 극 대화하는 기회를 제공합니다. 또한, AI 기반 플랫폼은 사용자가 입력한 자연어 명령을 바탕으로 즉각적인 피드백과 결과를 제공하여, 개발 속도를 높이고 품질을 개선합니다. 이는 코드 작성에서의 인간의 역량을 한 단계 끌어올리는 데 기여하고 있으며, 향후 이러한 기술의 발전 방향은 더욱 협업적이고 직관적인 개발 환경으로 이어질 것입니다. 따라서 이러한 혁신이 어떻게 산업 전체에 파급효과를 미칠 것인가에 대한 연구가 필요합니다.## 7. 결론- 이번 보고서를 통해 바이브 코딩 그리고 AI 기술이 소프트웨어 개발의 혁신을 이끌고 있음을 확인할 수 있었습니다. 데이터브릭스의 통합 플랫폼 전략이 기업들이 데이터 활용을 극대화하는 데 기여하는 동시에, 스타트업들이 생겨나는 환경은 기회이자 도전과제로 작용합니다. 특히, AI 코드 생성 도구들은 단순한 코드 생성기를 넘어, 개발자들의 업무 방식을 혁신하며 이들의 지속적인 활용이 요구됩니다. 앞으로의 연구는 이러한 기술들이 기업과 개인의 개발 환경에 미칠 장기적인 영향을 심화하여 분석하고, 변화하는 시장 환경에 대한 비즈니스 전략을 재정립하는 데 초점을 맞춰야 하겠습니다. AI 기술의 발전은 결국 인간의 일하는 방식을 변화시키고 있으며, 이에 따라 새로운 접근과 대처가 필요합니다.## 용어집- 바이브 코딩: 개발 지식이 부족한 사용자도 손쉽게 코드를 작성할 수 있도록 돕는 인공지능 기반의 코드 생성 시스템을 의미.- 데이터브릭스: 기업의 데이터 생태계를 통합하고 AI 중심의 혁신을 통해 데이터를 효과적으로 활용할 수 있도록 지원하는 플랫폼.- 레이크베이스(Lakebase): 데이터브릭스에서 출시한 서버리스 OLTP 데이터베이스로, PostgreSQL과 호환되며 데이터 처리의 효율성과 확장성을 제공.- AI 자동화: 인공지능 기술을 활용하여 반복적이고 기본적인 작업을 자동으로 수행하도록 하는 프로세스.- 에이전트 브릭스(Agent Bricks): 데이터브릭스의 도구로, 자연어 명령을 통해 AI 에이전트를 자동으로 생성 및 관리하는 기능을 가지고 있음.- AI 판사(judges): AI 에이전트의 생성 결과를 평가하고 개선하는 기능으로, 품질을 모니터링하는 역할을 함.- BI(비즈니스 인텔리전스): 기업이 데이터를 분석하고 제공하여 비즈니스 의사결정을 지원하는 기술과 시스템을 통칭.- 스타트업: 신생 기업으로, 주로 혁신적인 제품이나 서비스를 바탕으로 빠른 성장을 목표로 하는 비즈니스 모델.- 코드 생성기: 인공지능 기술을 활용하여 소프트웨어 개발자가 아닌 사용자가 코드 작성을 가능하게 도움.- AI 코드 생성: 인공지능이 자동으로 소프트웨어 코드를 작성하는 기술.- 머신러닝: 인공지능의 한 분야로, 컴퓨터가 데이터를 학습하고 예측할 수 있도록 하는 알고리즘 및 기술.- 서버리스: 서버 관리 없이 클라우드에서 애플리케이션을 실행할 수 있는 아키텍처 모델.- 클라우드 기반: 인터넷을 통해 클라우드 서비스 제공업체가 제공하는 리소스를 활용하는 방식.출처 문서- **데이터브릭스, \\'바이브 코딩\\'으로 기업 AI 생태계 통합 선언 - TokenPost** - **AI \\'바이브 코딩\\' 스타트업, 몸값 고공행진…수익성은 글쎄 : 네이트 뉴스** - **바이브 코딩을 위한 최고의 AI 코드 생성기 10가지 (2025년 XNUMX월) - Unite.AI** \"}, {\\'title\\': \\'개발자는 결국 대체될까? 바이브코딩 트렌드 리포트\\', \\'url\\': \\'https://eopla.net/magazines/32364\\', \\'markdown\\': \\'![]( 결국 대체될까? 바이브코딩 트렌드 리포트2025. 07. 23![]( 이 글은  에서 발행되었습니다.>> 가장 전문적인 바이브코딩 인사이트를 매주 받아보세요.>> [[구독하러 가기]]( 이미지]( 최근 가장 핫하고 변화도 빠른 분야입니다.매일 새로운 서비스가 등장하고 이를 소개하는 컨텐츠도 많은데요,그중\\\\xa0**팀차원에서, 실무수준으로 운영가능한 코드** 를 작성하는 관점의 컨텐츠는 거의 없습니다.이번 레터에서는 바이브코딩 컨설팅업체를 운영하며 느낀 바이브코딩의 현 위치와,\\\\xa0**제대로 사용하기위한 준비사항** 들을 소개해보겠습니다.## **바이브코딩이란?**바이브코딩은 쉽게 말해\\\\xa0AI가 대신 코딩하는 방법론입니다.어려운 개발언어 공부할 필요 없이 \"계산기 만들어줘\"처럼 요청하면 되는거죠.아티클을 더 읽고 싶다면?이오플래닛에 가입해주세요.로그인 후 모든 아티클을 무료로 볼 수 있어요.]( 7]( 뉴스레터]( 어썸데브 · CEO 댓글 5]( studio약 2달 전]( 뉴스레터 님의 아티클이 EO 뉴스레터에 실렸습니다. 이번 주 이오레터를 확인하세요!>>> ]( 0]( l 가능성에 반응하는 사람들약 2달 전]( 쉽게 이해할 수 있어 잘 읽었습니다. 한 두개 프로잭트에 활용해 봐야 겠어요. 고맙습미다.]( 0](\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0개발자약 2달 전]( 실력에서 나온 통찰력 있는 내용이라 배우고 갑니다.다음글도 기대가 되네요~]( 0]( 코리아\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Manager약 2달 전]( 맥을 주사 바늘처럼 정교하게 찔러주셔서많은 영감을 얻고 갑니다. 향후 글 기대 됩니다.]( 1]( 뉴스레터어썸데브\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0CEO약 2달 전]( 정말 감사합니다! 앞으로도 좋은 컨텐츠 남기겠습니다. :)]( 0추천 아티클최바울  페오펫]( 약 3년 전]( 약 3년 전디오니소스  디오니소스\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Owner]( 1년 이상 전]( Product Owner  1년 이상 전[# 사업전략# 마인드셋# 트렌드# 기타‘챗GPT의 아버지’ 샘 알트만에 대해 알아보자 [아티클 한 눈에 보기]0.샘 알트만이라는 독특한 캐릭터1.\"이해 받지 못하는 것이 강점이다\"2.CEO는 어떻게 시간을 써야 하는가3.실리콘밸리 요다의 3가지 깨달음4.핵융합, 기본소득, 규제, 정치적 올바름\\\\xa0\\\\xa02022년 여름부터 연일 헤드라인을 장식하는 사람이 ...](  eo\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0에디터]( 2년 이상 전]( 에디터  2년 이상 전]( 뉴스레터]( 어썸데브 · CEO ]( 7![](\\'}]\\n[{\\'title\\': \\'Real time emotion aware applications: A case study employing ...\\', \\'url\\': \\'https://www.sciencedirect.com/science/article/abs/pii/S0169260712000831\\', \\'markdown\\': \\' - - - - Other access optionsSearch ScienceDirect## Article preview- - - - - ]( \"Go to Computer Methods and Programs in Biomedicine on ScienceDirect\")## , July 2012, Pages 16-27]( Real time emotion aware applications: A case study employing emotion evocative pictures and neuro-physiological sensing enhanced by Graphic Processor UnitsAuthor links open overlay panel , , , Show moreAdd to MendeleyCite ## AbstractIn this paper the feasibility of adopting  Units towards real-time emotion aware computing is investigated for boosting the time  employed in such applications. The proposed methodology was employed in analysis of encephalographic and electrodermal data gathered when participants passively viewed emotional evocative stimuli. The  effectiveness when processing electroencephalographic and electrodermal recordings is demonstrated by comparing the  of chaos/complexity analysis through  (multi-channel correlation dimension/D2) and  (computation of  level/SCL) into various popular programming environments. Apart from the beneficial role of parallel programming, the adoption of special design techniques regarding  may further enhance the time minimization which approximates a factor of 30 in comparison with ANSI C language (single-core sequential execution). Therefore, the use of  parallel capabilities offers a reliable and  for real-time sensing the user\\\\\\'s affective state.## IntroductionEmotion aware computing was for a large period a neglected topic in the scientific community [1]. However, recent neuroscience findings have highlighted the critical role of emotions in a variety of cognitive functions like decision making [2], memory [3] and perception [4]. These arguments demonstrated the significance of emotional intelligence [5] not only when interacting with other people but also between human and machines [6]. Therefore, motivated research efforts investigate how to provide computers with abilities to recognize the user\\\\\\'s emotional state and to naturally adapt to it [7]. The importance of emotion aware computing is desirable only in cases where the user should interact with the machine in order to achieve high performance during the task procedure that should be accomplished [8]. So, providing the machine with the capability to robustly sense the users’ negative feelings [9] (frustration, anger, stress, anxiety, disappointment, etc.) the appropriate feedback may be given to neutralize their mood [10] and to encourage them to improve their performance in several applications like tests controlled through computer [11], virtual gaming [12] or remote monitoring of elderly or disabled people [13], [14]. Initial research attempts have demonstrated that the core element of a successful affective computing system is its ability to emulate the ways that are employed in the communication between human beings [15]. The pioneering work of MIT group led to the introduction of the “Affective Computing” term and to the establishment of a framework that could be adopted for a successful human–computer interaction (HCI) system [16], while also dealing with the challenges that have to be faced and the expectations created by potential applications [8].Previous research attempts have adopted communicative ways like facial expressions [17] and posture recognition [18]. However, several limitations occur since these modalities are highly dependent from the users’ personality [19] and their culture, resulting thus in enhanced inter-subject variability. Robust emotion recognition assumes the utilization of exaggerated expressions that are unlikely to be elicited in real-life situations [20]. Moreover, the use of cameras produces huge amount of data, while also communicates irrelevant information (e.g. subject\\\\\\'s identity) which the user may be unwilling to reveal [7]. Since the aforementioned methodologies are based on the recognition of externally expressed emotions, some innermost may not be easily recognized [8]. Such feelings are not easily communicated even among human beings and may be better recognized by neuro-physiological sensing [7]. Data fusion [21] from both the central and the autonomic nervous system may create discrete emotional patterns for a wide range of emotions [22], which are poorly distinguishable otherwise. However, special care should be given to the experimental methodology used for emotion elicitation.So, a key issue towards the achievement of a robust emotion aware computerized system is the establishment of a framework that is in close connection with the modern emotional theory assuring thus the reliable emotion elicitation. Recent trends regard emotions as behavioral attitudes related with evolutionary processes aiming to assure the human\\\\\\'s survival and perpetuation [23], [24], [25]. Therefore, each situation may be judged as either a pleasant or an unpleasant one. Its importance modulates the activation level needed in order to confront the stimulus appeared. Erotic or life-threatening situations require higher activation degree than melancholic or relaxing occasions. Adopting this notion, a bi-directional model was proposed. According to this approach, emotional processing is governed by two motivational systems which are the appetitive approach dealing with the pleasant situations and the defensive one activated in case of life-threatening occasions. The activation of the aforementioned systems is described through the valence dimension, while the activation degree is represented by the arousal dimension. So, these affective variables form a 2D emotional space.The International Affective Picture System (IAPS) collection adopts the aforementioned emotional model and provides a variety of affective visual stimuli as well as their normative ratings for both the arousal and valence dimension [26]. The use of this picture collection with simultaneous neurophysiological recordings demonstrated the facilitated encoding of emotional stimuli [27]. The combination of central nervous (event-related potentials/ERPs) and autonomic (electrodermal) activity revealed a significant correlation between skin conductance responses (SCRs) and the arousal ratings of the IAPS stimuli [23]. Moreover, late ERPs were more positive for emotional pictures [28], while their time course was influenced by the valence dimension [29]. A recent study investigated whether emotional processing is affected by the subject\\\\\\'s gender. Early (N100) and mid (N200) ERPs were significantly greater for female participants during passive viewing of unpleasant pictures [30].The bi-directional emotion model and the aforementioned neuroscience findings have not been widely adopted until now in the field of emotion aware computing. Relying on these notions, a Mahalanobis distance-based classification scheme was proposed for discriminating emotional instances selected from the IAPS collection. The output of the recognition sub-system was then used by an avatar which emulated the user\\\\\\'s affective state by adapting its face and voice characteristics [14]. However, there was need for further improvement of the classification accuracy by applying data mining (decision trees) and pattern recognition (Support Vector Machines) techniques [31]. Towards the achievement of a reliable emotion-aware application, extended feature fusion from different neuro-physiological modalities was proposed as well as a close connection with the theoretical emotional framework and the independency of the two emotional variables. Moreover, gender specific classifiers were proposed according to [32] in order to further enhance the method\\\\\\'s robustness which reached 81.3% for 4 emotional categories.Despite the adequate classification accuracy that was demonstrated by these research efforts, there are several open issues that should be further investigated prior to the introduction of real world emotion aware applications. The proposed discrimination framework was developed for research purposes. It is oriented towards the achievement of the optimal result employing time-consuming computations that reduce its applicability. Moreover, it has been developed as an isolated application under controlled lab environments which may differ from generic real-life applications. So, an integrative approach should be adopted for linking the emotion methodology with the acquisition subsystem as well as with the avatar behavior-generation routines. Then, the proposed system would be able to gather short segments of neuro-physiological data which are processed within fractions of seconds. The user\\\\\\'s affective state is recognized and serves as an input to the avatar which adapts its behavior either to mirror or to neutralize the user\\\\\\'s affective state.The current study investigates the feasibility of the Graphics Processing Unit (GPU) for the fast processing of neuro-physiological data. Short segments from both the central (ERPs) and the autonomic (SCRs) nervous system serve as an input to the system. These data are parallel processed during the feature extraction stage by algorithmic procedures that were re-designed in order to provide the optimal solution regarding the memory management. So, the aim of this paper is to demonstrate that the adoption of parallel processing may be greatly beneficial for the development of real-time emotion aware applications. Therefore, it is not focused to the extensive description of the parallelization techniques adopted. Moreover, it highlights some significant issues like time consumption on data transfer between host and device that should be taken into consideration during the system design in order to further minimize the execution time. So, the work\\\\\\'s contribution lays on the introduction of a framework for the adoption of parallel programming for real time emotion-aware applications.So, the remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture as well as with special programming techniques adopted for the proper parallelization of an algorithm. Then, a brief description of the parallelized algorithms is performed. Within Section 3 results of the algorithms’ implementation and the execution time are presented in Section 4. Finally, the discussion of this paper appears in Section 4.## Section snippets## The NVIDIA GPU architecture – CUDAThe voracious market demand for realtime and high definition 3D graphics led to the introduction of highly parallel, multithreaded, manycore processor Graphic Processor Unit (GPU). Characterized by high memory bandwidth and astounding computational horsepower, the GPU (Fig. 1) serves the demanding requirements of the modern designs and implementations. Its main difference with CPU is that it facilitates compute-intensive and parallel computation. Stemming from the graphics rendering demands, it## ResultsThe features (D2 complexity and SCL values) obtained from the parallel processing of electroencephalographic and autonomic data were analyzed in order to highlight differences among the various emotional states. Each emotional state is characterized by two independent variables (valence and arousal degree).Regarding the multi-channel D2 correlation dimension algorithm, the analysis was performed for each participant and for each one of the four emotional categories. As depicted in Fig. 9 (left## DiscussionThe current work aims to highlight the significant acceleration that may be achieved to emotion aware computing in case of adopting parallel programming on GPU. So, the detailed description of the parallelization techniques are beyond the paper\\\\\\'s scope and may be found in [33], [36]. These recent code execution techniques are exploited in boosting complex and time-consuming computations, such as nonlinear dynamic analysis or processing of dense data arrays. Selected results are included in## ConclusionA novel parallel-programming approach based on the CUDA architecture was proposed in order to accelerate the processing of neurophysiological recordings requiring complex computations. It aims to facilitate the already proposed emotion discrimination methodologies with the computing solution needed in order to perform real-time classification. To this end, the importance of this work towards an integrative approach of providing the machines with the capabilities to adapt their behavior## Conflict of interestThe authors do not report any conflict of interest.Special issue articlesRecommended articles- K. Sergerie _et al._### ### Neuroscience and Biobehavioral Reviews(2008)P. Vuilleumier _et al._### ### Nature Neuroscience(2003)- R.W. Picard _et al._### ### IEEE Transactions on Pattern Analysis and Machine Intelligence(2001)- J. Klein _et al._### ### Interacting with Computers(2002)- F. Nasoz _et al._### - C.A. Frantzidis _et al._### - P.J. Lang _et al._### (1997)- C. Amrhein _et al._### ### International Journal of Psychophysiology(2004)- C. Lithari _et al._### ### Brain Topography(2010)- G. Noaje _et al._- K.C. Berridge _et al._### Affective neuroscience of pleasure: reward in humans and animals### Psychopharmacology(2008)A. Bechara _et al._### Emotion, decision making and the orbitofrontal cortex### Cerebral Cortex(2000)M. Pantic _et al._### Toward an affect-sensitive multimodal human–computer interaction### Proceedings of the IEEE(2003)E. Hudlicka### To feel or not to feel: the role of affect in human–computer interaction### International Journal of Human–Computer Studies(2003)R.W. Picard### Affective computing: challenges### International Journal of Human–Computer Studies(2003)B. Kort _et al._### An affective module for an intelligent tutoring systemE. Hudlicka### Affective computing for game designC.A. Frantzidis _et al._### Description and future trends of ICT solutions offered towards independent living: the case of LLM projectP.D. Bamidis _et al._### An integrated approach to emotion recognition for advanced emotional intelligenceB. Reeves _et al._### The Media Equation(1996)R.W. Picard### Affective Computing(1997)View more references- ### 2019, Information FusionCitation Excerpt :Mehmood et al. utilized the late positive potential (LPP)-based feature extraction method and used two classifiers (SVM an KNN) for emotion detection from EEG [44]. Konstantinidis et al. utilized K-nearest neighbors (KNN) classifiers to detect six basic emotions from a three-channel forehead EEG combined with GSR [45]. Recently, Alarcao et al. [10] presented an analysis which shows that 59% of works in the literature used SVM classifiers to detect emotions, 8% of works used different variations of SVM like adaptive SVM etc.; 14% of the works used the kNN, 6.3% of the works used Linear Discriminant Analysis (LDA) and 3.17% of the works used the Naive Bayes (NB) classifier.Show abstractRecently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.- ### 2017, Computer Methods and Programs in BiomedicineShow abstractUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.- ### 2017, Frontiers in NeuroroboticsShow abstractUsing machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features. However, the conventional subject-specific classifiers may induce additional burdens to each subject for preparing multiple-session EEG data as training sets. To this end, we developed a new EEG feature selection approach, transfer recursive feature elimination (T-RFE), to determine a set of the most robust EEG indicators with stable geometrical distribution across a group of training subjects and a specific testing subject. A validating set is introduced to independently determine the optimal hyper-parameter and the feature ranking of the T-RFE model aiming at controlling the overfitting. The effectiveness of the T-RFE algorithm for such cross-subject emotion classification paradigm has been validated by DEAP database. With a linear least square support vector machine classifier implemented, the performance of the T-RFE is compared against several conventional feature selection schemes and the statistical significant improvement has been found. The classification rate and _F_-score achieve 0.7867, 0.7526, 0.7875, and 0.8077 for arousal and valence dimensions, respectively, and outperform several recent reported works on the same database. In the end, the T-RFE based classifier is compared against two subject-generic classifiers in the literature. The investigation of the computational time for all classifiers indicates the accuracy improvement of the T-RFE is at the cost of the longer training time.- ### 2014, Plos OneShow abstractInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method\\\\\\'s practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.- ### 2014, IEEE Transactions on Affective Computing- ### 2013, Computational and Mathematical Methods in MedicineCopyright © 2012 Elsevier Ireland Ltd. All rights reserved.The Identity Selector: Persistence Service\\'}, {\\'title\\': \\'Towards Emotion-awareness in Programming Education with ...\\', \\'url\\': \\'https://ieeexplore.ieee.org/document/9842520/\\', \\'markdown\\': \"Loading [MathJax]/extensions/TeX/ieee_stixext.jsTowards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation | IEEE Conference Publication | IEEE XploreSkip to Main Content# Towards Emotion-awareness in Programming Education with Behavior-based Emotion EstimationPublisher: IEEECite ThisPDF; ; ; ; ; All AuthorsSign In or Purchase1Cites inPaper239FullText Views- AlertsAdd to Citation Alerts* * *- Download PDF- Download References- Request Permissions- Save to- Alerts## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learnin...Show More## Metadata## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learning to program, rich emotions are experienced which can provide valuable feedback to their teachers. To accurately model stu-dents\\' emotions, this paper adopts the well-recognized model of emotions during complex learning that involves four states: engaged, confused, frustrated, and bored. To perform continuous estimation of students\\' emotions in a non-intrusive manner, this paper proposes to track students\\' programming behavior and estimate their corresponding emotional states. Compare to the existing approaches on acquiring the students\\' emotional states with self-reports or bio-sensors, the proposed approach is more feasible in conducting real-world, and large-scale studies for not requiring extensive human interventions or additional devices. Evaluated using data collected from a real-world course project, the proposed approach is showed to be promising for achieving an estimation accuracy of 72.06 % for the above four emotional states. As an enabling technology, the proposed is potentially useful in supporting many applications and improve the quality of programming education in computer science.**Published in:** **Date of Conference:** 27 June 2022 - 01 July 2022**Date Added to IEEE _Xplore_:** 10 August 2022**ISBN Information:****Print on Demand(PoD) ISSN:** 0730-3157**DOI:** Publisher: IEEE**Conference Location:** Los Alamitos, CA, USA## Funding Agency:![]( Contents* * *### I. IntroductionBoth learning and computer programming are high cog-nitively demanding tasks which involve the experience of complex emotions during the process [1]–[3]. Studies from the areas of education [3], psychology [2], and software engineering [1], [4], [5] have more than once shown the significance of one\\'s emotions in affecting the performance and productivity of the learners and developers, respectively. Bringing the above researches together, this paper studies the problem of understanding the students\\' emotions during the task of learning to program.Sign in to Continue Reading## Authors## Figures## References## Citations## Keywords## Metrics## Footnotes]( Like This2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)Published: 20162024 International Conference on Inventive Computation Technologies (ICICT)Published: 2024Show More**References is not available for this document.**### IEEE Account- - ### Purchase Details- - - ### Profile Information- - - ### Need Help?- **US & Canada:** +1 800 678 4333- **Worldwide:** +1 732 981 0060- - - - - - - - - A not-for-profit organization, IEEE is the world\\'s largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.The Identity Selector: Persistence Service\"}]\\n[{\\'title\\': \\'AI | 2025 Stack Overflow Developer Survey\\', \\'url\\': \\'https://survey.stackoverflow.co/2025/ai\\', \\'markdown\\': \\'### 84% of respondents are using or planning to use AI tools in their development process, an increase over last year (76%). This year we can see 51% of professional developers use AI tools daily.Do you currently use AI tools in your development process?     All RespondentsYes, I use AI tools daily47.1%Yes, I use AI tools weekly17.7%Yes, I use AI tools monthly or infrequently13.7%No, but I plan to soon5.3%No, and I don\\\\\\'t plan to16.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,662(68.7%)Professional DevelopersYes, I use AI tools daily50.6%Yes, I use AI tools weekly17.4%Yes, I use AI tools monthly or infrequently12.8%No, but I plan to soon4.6%No, and I don\\\\\\'t plan to14.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,004(53%)Learning to CodeYes, I use AI tools daily39.5%Yes, I use AI tools weekly18.7%Yes, I use AI tools monthly or infrequently15.1%No, but I plan to soon7.2%No, and I don\\\\\\'t plan to19.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,843(5.8%)Early Career DevsYes, I use AI tools daily55.5%Yes, I use AI tools weekly18.1%Yes, I use AI tools monthly or infrequently11.5%No, but I plan to soon2.5%No, and I don\\\\\\'t plan to12.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,360(13%)Early career defined as 1 - 5 years work experienceMid Career DevsYes, I use AI tools daily52.8%Yes, I use AI tools weekly16.8%Yes, I use AI tools monthly or infrequently13.5%No, but I plan to soon3.7%No, and I don\\\\\\'t plan to13.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,997(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsYes, I use AI tools daily47.3%Yes, I use AI tools weekly17.2%Yes, I use AI tools monthly or infrequently13%No, but I plan to soon6%No, and I don\\\\\\'t plan to16.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 13,001(26.5%)Experienced dev defined as 10+ years work experience### Conversely to usage, positive sentiment for AI tools has decreased in 2025: 70%+ in 2023 and 2024 to just 60% this year. Professionals show a higher overall favorable sentiment (61%) than those learning to code (53%).How favorable is your stance on using AI tools as part of your development workflow?     All RespondentsVery favorable22.9%Favorable36.8%Indifferent17.6%Unsure2.3%Unfavorable10.8%Very unfavorable9.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,412(68.2%)Professional DevelopersVery favorable23.5%Favorable37.7%Indifferent17.4%Unsure1.8%Unfavorable10.6%Very unfavorable9.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,814(52.7%)Learning to CodeVery favorable19.3%Favorable33.5%Indifferent16.6%Unsure4.3%Unfavorable13.6%Very unfavorable12.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,812(5.7%)Early Career DevsVery favorable22.8%Favorable40.3%Indifferent17%Unsure1.3%Unfavorable10.3%Very unfavorable8.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,293(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery favorable23.8%Favorable38.9%Indifferent16.2%Unsure1.5%Unfavorable11%Very unfavorable8.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,957(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery favorable23.9%Favorable36%Indifferent18.1%Unsure2.1%Unfavorable10.3%Very unfavorable9.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,941(26.4%)Experienced devs defined as 10+ years work experience### More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report \"highly trusting\" the output. Experienced developers are the most cautious, with the lowest \"highly trust\" rate (2.6%) and the highest \"highly distrust\" rate (20%), indicating a widespread need for human verification for those in roles with accountability.How much do you trust the accuracy of the output from AI tools as part of your development workflow?     All RespondentsHighly trust3.1%Somewhat trust29.6%Somewhat distrust26.1%Highly distrust19.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,244(67.8%)Professional DevelopersHighly trust2.7%Somewhat trust29.6%Somewhat distrust26.3%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,701(52.4%)Learning to CodeHighly trust6.1%Somewhat trust31.3%Somewhat distrust24.2%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,781(5.7%)Early Career DevsHighly trust3%Somewhat trust31.1%Somewhat distrust25.7%Highly distrust17.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,254(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsHighly trust2.8%Somewhat trust30.3%Somewhat distrust26.1%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,931(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsHighly trust2.5%Somewhat trust28.6%Somewhat distrust26.7%Highly distrust20.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,908(26.3%)Experienced devs defined as 10+ years work experience### In 2024, 35% of professional developers already believed that AI tools struggled with complex tasks. This year, that number has dropped to 29% among professional developers and is consistent amongst experience levels. Complex tasks carry too much risk to spend extra time proving out the efficacy of AI tools.How well do the AI tools you use in your development workflow handle complex tasks?     All RespondentsVery well at handling complex tasks4.4%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.1%Bad at handling complex tasks22%Very poor at handling complex tasks17.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know16.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,230(67.8%)Professional DevelopersVery well at handling complex tasks3.9%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.2%Bad at handling complex tasks22.8%Very poor at handling complex tasks18.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know15.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,695(52.4%)Learning to CodeVery well at handling complex tasks7.9%Good, but not great at handling complex tasks25.8%Neither good or bad at handling complex tasks12.4%Bad at handling complex tasks19%Very poor at handling complex tasks16.3%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,779(5.7%)Early Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks28.1%Neither good or bad at handling complex tasks13.4%Bad at handling complex tasks23.6%Very poor at handling complex tasks19.2%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know11.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,258(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks25.4%Neither good or bad at handling complex tasks13.8%Bad at handling complex tasks23.9%Very poor at handling complex tasks19.5%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know13.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,922(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery well at handling complex tasks3.6%Good, but not great at handling complex tasks23.5%Neither good or bad at handling complex tasks14.9%Bad at handling complex tasks22.1%Very poor at handling complex tasks17.9%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,901(26.3%)Experienced dev career defined as 10+ years work experience### Developers show the most resistance to using AI for high-responsibility, systemic tasks like Deployment and monitoring (76% don\\\\\\'t plan to) and Project planning (69% don\\\\\\'t plan to).Which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently Mostly AISearch for answers54.1%Generating content or synthetic data35.8%Learning new concepts or technologies33.1%Documenting code30.8%Creating or maintaining documentation24.8%Learning about a codebase20.8%Debugging or fixing code20.7%Testing code17.9%Writing code16.9%Predictive analytics11%Project planning10.8%Committing and reviewing code10.2%Deployment and monitoring6.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,202(22.9%)Currently Partially AISearch for answers55.8%Generating content or synthetic data28.6%Learning new concepts or technologies47.4%Documenting code30.3%Creating or maintaining documentation27.3%Learning about a codebase32.7%Debugging or fixing code47.1%Testing code27.5%Writing code59%Predictive analytics12.7%Project planning17.1%Committing and reviewing code22.6%Deployment and monitoring10.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,991(42.8%)Plan to Partially Use AISearch for answers24%Generating content or synthetic data28%Learning new concepts or technologies27.9%Documenting code30.5%Creating or maintaining documentation32.5%Learning about a codebase34.9%Debugging or fixing code30.9%Testing code34.7%Writing code32.4%Predictive analytics25%Project planning24.8%Committing and reviewing code31.4%Deployment and monitoring25%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,518(45.9%)Plan to Mostly Use AISearch for answers17.2%Generating content or synthetic data28.9%Learning new concepts or technologies15.7%Documenting code28.6%Creating or maintaining documentation31.8%Learning about a codebase23.1%Debugging or fixing code14.8%Testing code25.8%Writing code12.4%Predictive analytics23%Project planning14.3%Committing and reviewing code16.3%Deployment and monitoring15.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,790(26.1%)Don\\\\\\'t Plan to Use AI for This TaskSearch for answers19.6%Generating content or synthetic data38.2%Learning new concepts or technologies32.3%Documenting code38.5%Creating or maintaining documentation39.6%Learning about a codebase39.4%Debugging or fixing code36.4%Testing code44.1%Writing code28.9%Predictive analytics65.6%Project planning69.2%Committing and reviewing code58.7%Deployment and monitoring75.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,349(51.7%)### Respondents who said they are currently using mostly AI tools to complete tasks in the development workflow are highly satisfied with and frequently using AI to search for answers or learn new concepts; respondents plan to mostly use AI in the future for documentation and testing tasks and are slightly less satisfied with the tools they are using now.How favorable is your stance on using AI tools as part of your development workflow and which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently mostly AINumber of responses6,053685Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents5.255.35.355.45.455.55.555.65.65%5%10%15%20%25%30%35%40%45%50%55Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,184(22.8%)Currently partially AINumber of responses12,3822,194Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.74.754.84.854.94.9555.055.15.155.25.25%10%15%20%25%30%35%40%45%50%55%60Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,980(42.8%)Plan to partially use AINumber of responses7,8585,400Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents3.73.83.944.14.24.34.44.54.64.74.84.9%24%25%26%27%28%29%30%31%32%33%34%35Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,500(45.9%)Plan to mostly use AINumber of responses4,0561,588Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.64.654.74.754.84.854.94.9555.055.15.155.2%12%14%16%18%20%22%24%26%28%30%32Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,777(26.1%)Don\\\\\\'t plan to use AI for this taskNumber of responses19,2114,953Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents2.42.62.833.23.43.63.84%20%25%30%35%40%45%50%55%60%65%70%75%80Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,332(51.7%)### The biggest single frustration, cited by 66% of developers, is dealing with \"AI solutions that are almost right, but not quite,\" which often leads to the second-biggest frustration: \"Debugging AI-generated code is more time-consuming\" (45%)When using AI tools, which of the following problems or frustrations have you encountered? Select all that apply.All RespondentsAI solutions that are almost right, but not quite66%Debugging AI-generated code is more time-consuming45.2%I don’t use AI tools regularly23.5%I’ve become less confident in my own problem-solving20%It’s hard to understand how or why the code works16.3%Other (write in):11.6%I haven’t encountered any problems4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,476(64.2%)### In a future with advanced AI, the #1 reason developers would still ask a person for help is \"When I don’t trust AI’s answers\" (75%). This positions human developers as the ultimate arbiters of quality and correctness.In the future, if AI can do most coding tasks, in which situations would you still want to ask another person for help? Select all that apply.All RespondentsWhen I don’t trust AI’s answers75.3%When I have ethical or security concerns about code61.7%When I want to fully understand something61.3%When I want to learn best practices58.1%When I’m stuck and can’t explain the problem54.6%When I need help fixing complex or unfamiliar code49.8%When I want to compare different solutions44.1%When I need quick help troubleshooting27.5%Other6.1%I don’t think I’ll need help from people anymore4.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 29,163(59.5%)### Most respondents are not vibe coding (72%), and an additional 5% are emphatic it not being part of their development workflow.In your own words, is \"vibe coding\" part of your professional development work? For this question, we define vibe coding according to the , the process of generating software from LLM prompts.     All RespondentsYes, emphatically0.4%Yes11.9%Yes, somewhat2.8%I have tried it2.1%Not sure1.2%No72.2%No, emphatically5.3%Uncategorized4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,564(54.2%)18-24 years oldYes, emphatically0.3%Yes11.6%Yes, somewhat3.2%I have tried it2.4%Not sure1.2%No72.8%No, emphatically5.1%Uncategorized3.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 4,212(8.6%)25-34 years oldYes, emphatically0.4%Yes11.8%Yes, somewhat3.2%I have tried it1.6%Not sure1.3%No72.3%No, emphatically5.7%Uncategorized3.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,526(17.4%)35-44 years oldYes, emphatically0.5%Yes12%Yes, somewhat2.8%I have tried it2.2%Not sure1.1%No72%No, emphatically5.4%Uncategorized4.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 7,607(15.5%)45-54 years oldYes, emphatically0.5%Yes12.7%Yes, somewhat2.5%I have tried it1.9%Not sure1.3%No71.3%No, emphatically5.2%Uncategorized4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,838(7.8%)55-64 years oldYes, emphatically0.8%Yes11.4%Yes, somewhat2%I have tried it3.1%Not sure1.5%No71.3%No, emphatically4.6%Uncategorized5.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 1,657(3.4%)### AI agents are not yet mainstream. A majority of developers (52%) either don\\\\\\'t use agents or stick to simpler AI tools, and a significant portion (38%) have no plans to adopt them.Are you using AI agents in your work (development or otherwise)? AI agents refer to autonomous software entities that can operate with minimal to no direct human intervention using artificial intelligence techniques.    All RespondentsYes, I use AI agents at work daily14.1%Yes, I use AI agents at work weekly9%Yes, I use AI agents at work monthly or infrequently7.8%No, but I plan to17.4%No, I use AI exclusively in copilot/autocomplete mode13.8%No, and I don\\\\\\'t plan to37.9%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,877(65%)Professional DevelopersYes, I use AI agents at work daily14.9%Yes, I use AI agents at work weekly9.2%Yes, I use AI agents at work monthly or infrequently7.7%No, but I plan to17.2%No, I use AI exclusively in copilot/autocomplete mode14.2%No, and I don\\\\\\'t plan to36.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 24,752(50.5%)Learning to CodeYes, I use AI agents at work daily13.2%Yes, I use AI agents at work weekly7.8%Yes, I use AI agents at work monthly or infrequently7.4%No, but I plan to15.6%No, I use AI exclusively in copilot/autocomplete mode12.1%No, and I don\\\\\\'t plan to44.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,610(5.3%)Professional AI UsersYes, I use AI agents at work daily17.5%Yes, I use AI agents at work weekly10.8%Yes, I use AI agents at work monthly or infrequently8.9%No, but I plan to18.6%No, I use AI exclusively in copilot/autocomplete mode16.3%No, and I don\\\\\\'t plan to27.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,892(42.6%)Learning AI UsersYes, I use AI agents at work daily16.5%Yes, I use AI agents at work weekly9.6%Yes, I use AI agents at work monthly or infrequently8.7%No, but I plan to16.9%No, I use AI exclusively in copilot/autocomplete mode14.7%No, and I don\\\\\\'t plan to33.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,019(4.1%)### 52% of developers agree that AI tools and/or AI agents have had a positive effect on their productivity.Have AI tools or AI agents changed how you complete development work in the past year?All RespondentsYes, to a great extent16.3%Yes, somewhat35.3%Not at all or minimally41.4%No, but my development work has significantly changed due to non-AI factors2.6%No, but my development work has changed somewhat due to non-AI factors4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,636(64.5%)### If you happen to be using AI agents at work and you are a software developer, chances are high that you are using agents for software development (84%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.Industry PurposeSoftware engineering83.5%Data and analytics24.9%IT operations18%Business process automation17.6%Decision intelligence11.3%Customer service support11.2%Marketing8.6%Cybersecurity7.4%Robotics3.9%Other2.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,301(25.1%)### TL;DR: Agents used outside of work are mostly used for language processing tasks (49%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.General PurposeLanguage processing49%Integration with external agents and APIs38.3%MCP servers34.4%Agent/multi-agent orchestration28.1%Vector databases for AI applications24.1%Multi-platform search enablement19.4%Personalized agent creation18.3%Other3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,797(11.8%)### The most recognized impacts are personal efficiency gains, and not team-wide impact. Approximately 70% of agent users agree that agents have reduced the time spent on specific development tasks, and 69% agree they have increased productivity. Only 17% of users agree that agents have improved collaboration within their team, making it the lowest-rated impact by a wide margin.To what extent do you agree with the following statements regarding the impact of AI agents on your work as a developer?All Respondents27.3%35.9%21.3%8.2%7.3%AI agents have accelerated my learning about new technologies or codebases.29.3%34.9%22.4%7%6.4%AI agents have helped me automate repetitive tasks.17.1%31.9%25.3%14.2%11.5%AI agents have helped me solve complex problems more effectively.6.6%10.7%40.5%20%22.2%AI agents have improved collaboration within my team.12.2%25.3%32.4%17.1%13.1%AI agents have improved the quality of my code.27.7%41%20.4%6%4.9%AI agents have increased my productivity.29.3%40.8%17.8%6.9%5.1%AI agents have reduced the time spent on specific development tasks.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,823(26.2%)### Is it a learning curve, or is the tech not there yet? 87% of all respondents agree they are concerned about the accuracy, and 81% agree they have concerns about the security and privacy of data.To what extent do you agree with the following statements regarding AI agents?All Respondents57.1%29.8%9.7%2.3%1.1%I am concerned about the accuracy of the information provided by AI agents.56.1%25.3%11.7%4.7%2.2%I have concerns about the security and privacy of data when using AI agents.16.5%29.7%37.3%12.6%3.9%Integrating AI agents with my existing tools and workflows can be difficult.15.5%27.9%31.8%17.8%6.9%It takes significant time and effort to learn how to use AI agents effectively.13.8%14.4%30.6%15%26.2%My company\\\\\\'s IT and/or InfoSec teams have strict rules that do not allow me to use AI agent tools or platforms25.4%27.9%31.8%10.3%4.6%The cost of using certain AI agent platforms is a barrier.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 28,930(59%)### When it comes to data management for agents, traditional, developer-friendly tools like Redis (43%) are being repurposed for AI, alongside emerging vector-native databases like ChromaDB (20%) and pgvector (18%).You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent memory or data management in the past year?All RespondentsRedis42.9%GitHub MCP Server42.8%supabase20.9%ChromaDB19.7%pgvector17.9%Neo4j12.3%Pinecone11.2%Qdrant8.2%Milvus5.2%Fireproof5%LangMem4.8%Weaviate4.5%LanceDB4.4%mem04%Zep2.8%Letta2.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,398(6.9%)### The agent orchestration space is currently led by open-source tools. Among developers building agents, Ollama (51%) and LangChain (33%) are the most-used frameworks.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent orchestration or agent frameworks in the past year?All RespondentsOllama51.1%LangChain32.9%LangGraph16.2%Vertex AI15.1%Amazon Bedrock Agents14.5%OpenRouter13.4%Llama Index13.3%AutoGen (Microsoft)12%Zapier11.8%CrewAI7.5%Semantic Kernel6%IBM watsonx.ai5.7%Haystack4.4%Smolagents3.7%Agno3.4%phidata2.1%Smol-AGI1.9%Martian1.7%lyzr1.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,758(7.7%)### Developers are primarily adapting their existing, traditional monitoring tools for this new task, rather than adopting new, AI-native solutions. The most used tools for AI agent observability are staples of the DevOps and application monitoring world: Grafana + Prometheus are used by 43% of agent developers, and Sentry is used by 32%.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent observability, monitoring or security in the past year?All RespondentsGrafana + Prometheus43%Sentry31.8%Snyk18.2%New Relic13%LangSmith12.5%Honeycomb8.8%Langfuse8.8%Wiz6.9%Galileo6.2%Adversarial Robustness Toolbox (ART)5.5%Protect AI5%Vectra AI4.4%arize3.7%helicone3.2%Metero2.7%opik2.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,689(5.5%)### ChatGPT (82%) and GitHub Copilot (68%) are the clear market leaders, serving as the primary entry point for most developers using out-of-the-box AI assistance.You indicated you use or develop AI agents as part of your development work. Have you used any of the following out-of-the-box agents, copilots or assistants?All RespondentsChatGPT81.7%GitHub Copilot67.9%Google Gemini47.4%Claude Code40.8%Microsoft Copilot31.3%Perplexity16.2%v0.dev9.1%Bolt.new6.5%Lovable.dev5.7%AgentGPT5%Tabnine5%Replit5%Auto-GPT4.7%Amazon Codewhisperer3.9%Blackbox AI3.5%Roo code (Roo-Cline)3.4%Cody3%Devin AI2.7%Glean (Enterprise Agents)1.3%OpenHands (formerly OpenDevin)1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,323(17%) \\'}, {\\'title\\': \\'AI-Generated Code Stats 2025: How Much Is Written by AI?\\', \\'url\\': \\'https://www.elitebrains.com/blog/aI-generated-code-statistics-2025\\', \\'markdown\\': \"26 min read08/27/2025# AI-Generated Code Stats 2025: How Much Is Written by AI?#### AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development.Hey there, fellow tech enthusiasts! One thing is crystal clear as we look to the future: artificial intelligence (AI) is rapidly changing how we write code. From building entire functions to completing code snippets in milliseconds, AI is helping developers work faster and smarter. But the big question on everyone’s mind is: **How much of that code is AI-generated?** And more importantly, should developers or  worry about their jobs as AI evolves?As the founder of EliteBrains, where we focus on helping developers stay sharp with coding tests and skills development, I want to break down the AI-generated code stats for 2025, explore the tools that are making this happen, and give you a better understanding of what the future holds for software developers in this new world of AI-assisted coding.As AI takes over more of the repetitive coding tasks, the real question becomes how we measure human skill alongside machine-generated output. That’s exactly where **EliteBrains helps.**Put AI to the test and see which of your developers are truly qualified.\\\\xa0Mix and match AI-powered coding tests from 120+ programming languages, choose the right seniority level, and instantly spot who rises above the rest.Create your\\\\xa0**FREE AI coding tests today** **( only\\\\xa03 free slots available)**## **What Is AI-Generated Code?**AI-generated code is programming written or completed by artificial intelligence tools from natural language prompts, existing context or prior code patterns. In 2025, it typically means an AI assistant drafted the solution or produced most of the final lines that a developer then reviews and refines.## **What Percentage of \\\\xa0Code Is AI-Generated in 2025?**AI has been in the software development game for a while now, but its role in coding is reaching new heights in 2025. It’s not just about suggesting code anymore. AI tools are writing **massive** amounts of code, optimizing development, and helping developers be more efficient than ever before.Here’s the shocking truth: . That number alone shows just how integrated AI tools like **GitHub Copilot** have become in developers’ workflows. Whether it’s generating entire functions or suggesting small snippets of code, AI is now deeply embedded into the way we code.## **How Much Code Has AI Written So Far?**But just how much code is AI generating? Let’s break it down.![]( **How Does AI Code Generation Affect Developer Jobs and the Economy?**With AI generating such a massive amount of code, one big question looms: What does this mean for job security and the economy?![]( **How Many Lines of Code Have Been Generated by AI?**Buckle up, because here’s where it gets mind-blowing. As of 2024, **256 billion lines of code have already been generated by AI**. Think about that for a second—**256 billion lines**. That’s an insane amount of code, and we’re just getting started. From simple scripts to full-fledged software systems, AI is helping to write an overwhelming amount of code across all industries.This number is so massive that it\\'s hard to grasp. Consider this: the **total lines of code** written by humans in the last decade likely pales in comparison to this explosion of AI-generated code. And we’re still in the early stages of integrating AI tools into the development process. The future will see even more code churned out by AI as the tools become smarter, faster, and more efficient.## **How Is Google Using AI to Write Code in 2025?**Let’s look at how AI is playing a huge role in tech giants like Google. Google, known for its cutting-edge AI innovations, has been using AI to power some of its most important software. According to Sundar Pichai, Google’s CEO, **over a quarter of all new code generated by Google is written by AI**. This includes everything from Google Cloud services to Android apps. AI is also playing a significant role in developing machine learning models that power a large chunk of Google’s products.Pichai’s announcement isn’t just a marketing stunt—it’s a reflection of how deeply integrated AI has become in Google’s development process. Whether it\\'s AI-driven infrastructure management or software development for consumer-facing products, Google is leading the way in showing just how powerful AI can be for developers.Let’s dive deeper into why Google has taken this approach. With over 20,000 engineers working on various products, relying on AI to handle repetitive, time-consuming tasks such as code generation allows the company to save countless hours. This, in turn, frees up developers to focus on more complex tasks that require problem-solving skills, creativity, and innovation.## **Will AI Replace Software Developers in 2025?**As AI continues to make its mark on software development, many developers are wondering if their jobs are at risk. Should they be worried? The short answer is **no**but let’s unpack that.AI isn’t here to replace developers. Far from it. AI tools are designed to assist developers, not eliminate them. These tools automate repetitive and mundane tasks, such as writing boilerplate code or generating simple functions. But when it comes to the creative, high-level work of designing software, making complex decisions, and debugging, developers are still very much in the driver’s seat.For example, imagine a scenario where a developer is working on building a web application. AI can generate the basic structure of the code such as setting up routing, creating a simple login form, or connecting to a database but it cannot make decisions about the best UI layout for user experience or develop the logic that handles complex business rules. That’s where the developer’s creativity and problem-solving skills come into play.AI is an assistant, not a replacement. It helps developers write code faster, but it doesn’t have the creativity, problem-solving skills, or domain knowledge that human developers bring to the table. In fact, AI is actually helping developers become more productive, allowing them to focus on more important aspects of the development process.The key takeaway here is that AI is making software development more efficient, but it still needs a human touch for the final product to succeed.### **How Much Code Is AI-Generated on GitHub in 2025?**GitHub is one of the largest code repositories in the world, and it has seen a massive increase in AI-generated code in recent years. That’s not a small chunk—it’s a significant portion of the code that powers everything from web applications to mobile apps.This doesn’t mean that developers are being replaced. Instead, it shows that developers are using AI to streamline their workflows and focus on more complex tasks. **GitHub Copilot** is perhaps the most popular AI tool used for code generation on the platform. It suggests entire lines of code and can even write entire functions based on developer input. It’s designed to help developers write code faster, with fewer bugs, and more efficiently.By integrating Copilot into their workflow, developers save time and avoid getting bogged down by menial tasks, ultimately improving both speed and quality.## **Which Programming Languages and Frameworks Does AI Write Best?**One of the most exciting things about AI in software development is its ability to generate code across a wide range of programming languages and frameworks. Here’s a breakdown of the most popular ones:#### **Python:**Python has become one of the most popular programming languages, especially in fields like data science, machine learning, and web development. AI tools like GitHub Copilot are particularly skilled at generating Python code for these use cases, whether you’re building machine learning models or working with web frameworks like Django and Flask.#### **JavaScript:**JavaScript is the backbone of web development, and AI tools are excellent at generating code for JavaScript frameworks like React, Angular, and Vue.js. AI can generate entire components, hooks, and functions in JavaScript, saving developers a ton of time and effort.#### **Java:**Java remains one of the most widely used programming languages in enterprise software development. AI can help developers generate Java code for popular frameworks like Spring and Hibernate. AI can also generate code for building microservices and APIs, speeding up development time.#### **C and C++:**AI tools are also capable of generating code in low-level programming languages like C and C++, which are crucial for systems programming and embedded systems. AI is particularly helpful in optimizing low-level code for performance and efficiency.#### **Ruby, Swift, PHP, and Go:**AI is also being used to generate code in languages like Ruby, Swift (for iOS development), PHP, and Go. These languages are frequently used for web development, mobile apps, and cloud applications, and AI is helping developers write faster and more efficient code across these platforms.## **What Are the Best AI Code Generation Tools in 2025?**Now, let’s take a look at the tools that are making this all happen. These tools are transforming the way developers work, making coding faster and more efficient:#### **GitHub Copilot:**GitHub Copilot is arguably the most well-known AI tool for developers. Powered by OpenAI’s Codex, Copilot offers suggestions for entire lines or blocks of code as developers work. It supports a wide range of languages and frameworks, including Python, JavaScript, Java, and more. Copilot is designed to help developers write code faster and with fewer errors.Source: #### **Amazon CodeWhisperer:**Amazon’s CodeWhisperer is another powerful AI tool designed to assist developers in generating code for cloud-based applications. It integrates seamlessly with Amazon Web Services (AWS), allowing developers to write code for serverless applications, APIs, and more. CodeWhisperer is especially useful for developers working in the AWS ecosystem.#### **Tabnine:**Tabnine is an AI-powered code completion tool that integrates with popular code editors like VS Code, JetBrains, and more. It offers context-aware code suggestions to help developers write code more quickly and efficiently. Tabnine works with over 40 programming languages, making it a versatile tool for developers.#### **Replit Ghostwriter:**Replit’s Ghostwriter is another AI tool designed for real-time collaborative coding. It provides developers with AI-generated code suggestions and helps speed up the development process. Whether you’re working on a solo project or collaborating with others, Ghostwriter helps you write code faster and more efficiently.**Should Developers Be Worried About AI in Coding?**So, let’s get back to the big question: should developers worry about their jobs in an AI-driven world? The answer is no, but there’s a catch.While AI is indeed transforming the way we write code, it’s not replacing developers. Instead, it’s empowering them to be more productive and efficient. Developers are still needed for creative problem-solving, high-level system design, and debugging. AI tools can generate code, but they can’t design systems, make architectural decisions, or think creatively about solutions. That’s where developers come in.The same idea holds true across industries. AI can build websites or generate campaigns, but real growth still comes from people who understand the field. Veterinary practices, for example, often lean on expert guidance like this , just as tech companies depend on developers for the creativity and insight that AI cannot replace.What’s really changing is the developer’s role. Instead of getting bogged down with repetitive tasks like boilerplate code, they now have the chance to focus on more interesting, high-value work. Those who embrace AI as a tool rather than view it as a threat will find their jobs not just safe but more important than ever.### **How Can Developers Stay Ahead of AI in Coding with EliteBrains?**At , we understand the importance of staying ahead of the curve. As AI continues to transform the software development industry, we’re here to help developers stay competitive. With our **coding tests**, developers can hone their skills, stay updated on new trends, and ensure they’re prepared for the future of software development.Our platform allows developers to:- **Practice with AI-assisted tools** and sharpen their skills- **Test their knowledge** in real-world coding environments- Stay up to date with **emerging trends in AI and software development**- **Learn how to integrate AI** into their development workflow to increase efficiencyAt EliteBrains, we’re committed to helping developers succeed in an AI-powered world, ensuring they remain competitive and continue to thrive.### **Conclusion: AI and Developers—A Winning Partnership**The rise of AI in software development is reshaping the way we write code, but it’s not here to replace developers. Instead, it’s here to make their jobs easier and more efficient. By embracing AI tools like GitHub Copilot, Amazon CodeWhisperer, and Tabnine, developers can work smarter, not harder.AI is an assistant, not a replacement. The key for developers is to adapt and use these tools to their advantage. The future of software development is collaborative—AI working alongside developers to create better, faster, and more innovative software.With **EliteBrains**, developers can stay ahead of the curve, test their skills, and learn how to leverage AI to boost their productivity. The future of coding is bright, and it’s full of exciting opportunities for those who are willing to embrace the change.01. 02. 03. 04. 05. 06. 07. 08. 09. 10. #### Our clients say!With senior resources from EliteBrains, we are able to drive our digital harmonization agenda: Migration of different reporting platforms to QlikSense analytics and Harmonization of different digital detailing systems into Veeva CLM.!Jiri Matousek, Head of CRM at Hexal AG!EliteBrains provided us with a top PHP developer who has been working for us for more than 4 years. Jan became indispensable part of our team within few weeks.!Dave Etchells, CEO at Imaging-resource.com!Each time we are in a need of new software developer, we just contact the guys from EliteBrains. Their network is huge so they provide us with demanded sources very quickly.!Martin Svach, CEO at Starkys club#### Useful articles]( Do Resumes Still Matter for Backend Developers in 2025? Here’s How to Build One That Gets You Hired Faster 02. 09. 2025 . 5 min read]( AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development. 27. 08. 2025 . 11 min readSign up now to see more tipsSubscribecancel anytime#### Hiring?We picked the best developers for you#### Looking for a job?## This website uses cookiesWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Accept allManage preferencesChat Widget\"}]', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_MDdhX9RZh3sRp9e9V2HGdIZp', arguments='{\"query\": \"emotion-based coding tools 2024 2025 latest developments\"}', name='web_search_tool'), FunctionCall(id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', arguments='{\"query\": \"바이브 코딩 최신 동향\"}', name='web_search_tool'), FunctionCall(id='call_4PAMAyFwVRDo2ioyrSmVFaI2', arguments='{\"query\": \"emotion-aware programming studies\"}', name='web_search_tool'), FunctionCall(id='call_MZOCIL7HSdei6STfRI8fQssA', arguments='{\"query\": \"AI-assisted coding usage statistics 2023-2025\"}', name='web_search_tool')], results=[FunctionExecutionResult(content='Unexpected error during search: Status code 502. Server returned non-JSON response: \\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>502 Server Error</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Server Error</h1>\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\n<h2></h2>\\n</body></html>\\n - Full response status: 502', name='web_search_tool', call_id='call_MDdhX9RZh3sRp9e9V2HGdIZp', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'2025년 바이브 코딩 AI 개발 동향 보고서\\', \\'url\\': \\'https://seo.goover.ai/report/202506/go-public-report-ko-e086645e-079a-44f8-ae63-13f0358d262d-0-0.html\\', \\'markdown\\': \"![]( 2025년 바이브 코딩 AI 개발 동향 보고서일반 리포트2025년 06월 16일!## 목차1. 요약2. 서론3. 데이터브릭스의 통합 AI 플랫폼 전략4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황5. 최고의 AI 코드 생성 도구 비교 분석6. 미래 전망 및 시사점7. 결론## 1. 요약- 2025년 바이브 코딩 AI 개발 동향 리포트에서는 인공지능 기술의 발전에 따라 변화하고 있는 소프트웨어 개발 환경을 다룹니다. 특히, \\'바이브 코딩\\'이라는 새로운 개념을 중심으로 데이터브릭스의 통합 플랫폼 전략과 스타트업 경쟁 구도를 분석합니다. 이 보고서의 핵심 발견은, 데이터브릭스가 제시하는 레이크베이스 플랫폼과 AI 자동화 도구가 기업들이 데이터와 AI를 효과적으로 통합하고 활용할 수 있는 환경을 제공한다는 것입니다. 바이브 코딩 스타트업인 Cursor, Windsurf, Codeium 등의 사례와 90억 달러 이상의 기업 가치를 통해 AI 코드 생성 시장의 동향이 더욱 뚜렷하게 드러나며, 개발자 역할의 변화와 더불어 AI 도구의 중요성이 지속적으로 증가하고 있음을 보여줍니다.## 2. 서론- AI 기술이 급속히 발전함에 따라 소프트웨어 개발의 패러다임이 전환되고 있습니다. 이러한 변화의 중심에 자리한 \\'바이브 코딩\\'은 개발 지식이 부족한 사용자들도 손쉽게 코드를 작성할 수 있는 기회를 제공합니다. 비록 이 기술이 새로운 가능성을 창출하지만, 동시에 기존의 개발자 일자리에 위협이 되고 있습니다. 본 리포트는 바이브 코딩을 통해 신생 기업과 대규모 플랫폼 간의 경쟁 구도, 그리고 AI 코드 생성 도구의 진화에 대해 심층적으로 분석하고자 합니다. 현재 시장에서 주목받고 있는 데이터브릭스의 레이크베이스 플랫폼 전략을 시작으로, 여러 스타트업 사례와 AI 코드 생성 도구의 특징을 살펴보며, 미래의 개발 환경이 어떤 방향으로 나아갈지를 예측합니다.## 3. 데이터브릭스의 통합 AI 플랫폼 전략- 인공지능의 세계가 진화하는 과정에서 데이터브릭스는 두 가지 측면에서 주목할 만한 변화를 이끌고 있습니다. 첫째, 인공지능의 사용 범위를 확장하는 \\'바이브 코딩(vibe coding)\\' 개념의 도입과 둘째, 이를 바탕으로 한 새로운 플랫폼인 레이크베이스(Lakebase)의 출시입니다. 데이터브릭스는 근본적으로 기업의 데이터 생태계를 통합하고, AI 중심의 혁신을 통해 기업들이 데이터를 더 효과적으로 활용할 수 있도록 새로운 방향성을 제시하고 있습니다.- ### 3-1. ‘바이브 코딩’ 개념과 Databricks의 레이크베이스(Lakebase) 출시- ‘바이브 코딩’은 데이터브릭스가 최근 발표한 혁신적인 개념으로, 자연어 기반의 AI 코드 생성과 업무 자동화를 통합한 것을 의미합니다. 사용자가 원하는 작업을 자연어로 설명하면, 시스템은 내장된 데이터와 AI 모델을 활용하여 적절한 코드를 자동으로 작성합니다. 이 접근 방식은 개발자들이 직접 코드를 입력하는 대신 시스템과 대화하며 작업을 수행할 수 있도록 돕습니다. 이는 기존의 코드 중심 개발 방식을 넘어서는 중요한 이정표로서, AI의 역할을 더 효율적이고 인간 친화적으로 만들어 줍니다.- 데이터브릭스는 데이터+AI 서밋 2025에서 새로운 플랫폼인 레이크베이스를 공식 출시했습니다. 레이크베이스는 서버리스 OLTP(온라인 트랜잭션 처리) 데이터베이스로, PostgreSQL과 호환되며 오브젝트 스토리지와의 통합 기능을 갖추고 있습니다. 이 플랫폼은 데이터 처리의 효율성과 확장성을 모두 만족시키며, 기업들이 데이터 분석, 트랜잭션 처리, AI 개발을 하나의 플랫폼 내에서 유기적으로 연결될 수 있는 환경을 제공합니다. 이는 기업의 데이터 생태계를 통합하는 데 있어 중대한 시점입니다.- ### 3-2. Agent Bricks와 AI 판사 기능 설명- 에이전트 브릭스(Agent Bricks)는 데이터브릭스의 또 다른 혁신으로, 자연어 명령을 통해 정보 추출, 지식 지원, 멀티에이전트 감시 등의 역할을 수행하는 AI 에이전트를 자동으로 생성할 수 있는 도구입니다. 이 시스템은 사용자 오류를 최소화하고, 정보의 질을 높이는 방향으로 설계되었습니다.- 특히 주목할 만한 것은 AI 판사(judges) 기능입니다. 이 기능은 AI 에이전트의 성능을 모니터링하고 평가하는 데 사용되며, 특정 기준에 따라 생성된 결과의 질을 점검하고 개선하는 역할을 합니다. 이를 통해 에이전트들은 학습하고 발전할 수 있으며, 최적의 결과를 얻기 위해 코드 수정 과정을 연구하게 됩니다. 데이터브릭스는 이러한 기능들을 통해 정보 활용의 효율성을 극대화하고, AI의 잠재력을 더욱 확장하고자 합니다.- ### 3-3. 플랫폼 통합으로 달성하는 데이터·AI 워크플로우 혁신- 데이터브릭스의 통합 AI 플랫폼 전략은 단순한 기술 통합을 넘어 데이터와 AI가 밀접하게 연결된 생태계를 형성하는 데 기여하고 있습니다. BI(비즈니스 인텔리전스) 툴과 머신러닝 모델을 통합함으로써 기업들은 데이터 기반 의사결정을 신속히 내릴 수 있는 능력을 갖추게 됩니다. 이는 데이터 흐름을 자동화하고, 실시간으로 분석을 가능하게 하여 비즈니스의 민첩성을 높이는 중요한 단계입니다.- 임무별 에이전트와 AI 분석 툴 간의 협업은 기업 내에서 심층적인 데이터 분석을 수행할 수 있도록 하며, 사용자들은 실시간으로 인사이트를 확보하게 됩니다. 이러한 혁신은 기존의 데이터 분석, 데이터 관리 및 AI 모델에 대한 접근 방식을 바꾸고 있으며, 기업들이 더욱 빠르게 변화하는 시장에서 경쟁력을 유지할 수 있도록 돕습니다. 데이터브릭스는 AI 자동화와 데이터 관리의 통합을 통해 이 전환을 가속화하고 있으며, 이는 향후 기업의 데이터와 AI 활용 방식에 지대한 영향을 미칠 것입니다.## 4. 바이브 코딩 스타트업 경쟁 구도 및 투자 현황- AI 기술의 급속한 발전은 소프트웨어 개발 생태계에 혁신적인 변화를 불러오고 있습니다. 특히, 바이브 코딩(vibe coding)이라는 개념이 등장하면서 일반 사용자도 손쉽게 코드 작성이 가능해졌습니다. 이는 프로그래밍 지식을 가진 개발자들에게만 국한되지 않고, 폭넓은 사용자에게 소프트웨어 개발의 기회를 제공합니다. 이러한 변화는 단순히 기술적인 것에 그치지 않고 투자 시장에서도 큰 물결을 일으키고 있습니다. 이번 섹션에서는 바이브 코딩 스타트업의 경쟁 구도와 최근 투자 현황에 대해 분석하겠습니다.- 2025년 현재, 바이브 코딩 시장에서 주목받고 있는 스타트업은 Cursor, Windsurf, Codeium 등입니다. 특히, Cursor는 최근 90억 달러의 기업 가치에 도달하며 10억 달러 규모의 투자를 유치했습니다. 이들은 \\'AI 코드 생성\\'을 통해 프로그래밍 언어에 대한 깊은 이해 없이도 소프트웨어를 개발하는 것을 가능하게 하여, 이를 통해 많은 사용자가 개발에 참여할 수 있도록 하고 있습니다. Windsurf와 Codeium 역시 30억 달러 가치를 인정받으며, 오픈AI의 인수 협상 대상으로 떠오르는 등 바이브 코딩 시장에서 활발한 활동을 보이고 있습니다.- ### 4-1. Cursor·Windsurf·Codeium 등 주요 스타트업 사례- Cursor는 인공지능을 활용한 코드 생성 플랫폼을 제공하며, 사용자가 쉽게 코드 스니펫을 생성하고 통합할 수 있도록 돕습니다. 또한, 이 플랫폼은 커뮤니티와의 협업 기능을 강화하여 다양한 사용자가 서로의 코드 작업을 공유할 수 있게 하며, 이는 소프트웨어 개발이 전통적인 개발 흐름을 넘어선 새로운 차원으로 나아가게 합니다. Windsurf는 Codeium 개발을 통해 비개발자들도 코드 작성에 참여할 수 있는 환경을 조성하고 있으며, 이를 통해 소프트웨어 개발의 인력 부족 문제를 해결하고 있습니다.- 또한, 이들 스타트업들은 단순한 코드 작성 도구를 넘어, 사용자 경험을 최적화하고 개발자 커뮤니티와의 연결성을 증대시키기 위한 방향으로 발전하고 있습니다. 이들은 AI 코딩 도구의 수요와 더불어, 사용자 친화적인 인터페이스를 제공함으로써 모바일 기기 및 다양한 플랫폼에서의 접근성을 높이고 있습니다.- ### 4-2. AI 코드 생성 스타트업의 밸류에이션 상승 배경- AI 코드 생성 스타트업들의 밸류에이션 상승은 몇 가지 주요 요인에 기인합니다. 첫째, 기업들이 AI 도입을 통해 업무 생산성을 극대화하고 싶어하는 경향이 있습니다. 소프트웨어 개발 분야에서 AI의 활용은 코딩을 자동화하고, 반복적인 업무를 감소시켜 기업들이 원하는 시간 안에 프로젝트를 완수하도록 돕고 있습니다. 이에 따라 AI 코드 생성 스타트업은 높은 수익 성장 잠재력을 지닌 것으로 평가받고 있습니다.- 둘째, AI 기술에 대한 투자자들의 관심이 급증하면서 스타트업들이 쉽게 자금을 조달할 수 있는 환경이 조성되었습니다. 특정 투자자들은 AI가 가져올 수 있는 혁신성과 시장의 미래 가능성을 높게 평가하고 있으며, 이는 스타트업 가치의 상승으로 이어지고 있습니다.- ### 4-3. 엔트리 레벨 개발자 일자리 변화 및 빅테크 진출 현황- AI의 도입은 엔트리 레벨 개발자 일자리에 큰 변화를 일으키고 있습니다. 최근 통계에 따르면, AI가 반복적이고 기본적인 코딩 작업을 대체하면서 1년 미만 경력의 개발자 신규 채용이 24% 감소한 것으로 나타났습니다. 이는 기술 발전이 개발자들의 일자리 환경에 어떠한 영향을 끼치고 있는지를 잘 보여줍니다.- 더불어, 구글, 아마존, 마이크로소프트(MS)와 같은 빅테크 기업들은 자체적인 AI 코드 생성 제품을 출시하며 이 시장에 진입하고 있습니다. 이러한 방식으로 이들 기업은 인건비를 절감하고 효율성을 극대화하고 있습니다. 예를 들어, MS는 올해 6000명 감원을 통해 개발자 역할의 변화가 불가피함을 강조하며, 자동화 기술이 고용에 미치는 영향을 조명하고 있습니다.## 5. 최고의 AI 코드 생성 도구 비교 분석- AI 코드 생성 도구는 현대 소프트웨어 개발의 필수 요소로 자리 잡았습니다. 코드 생성의 혁신이 이루어짐에 따라, 개발자들은 반복적인 작업에서 벗어나 더 창의적이고 전략적인 업무에 집중할 시간을 확보할 수 있게 되었습니다. 이러한 도구들은 단순히 코드를 생성하는 것을 넘어서, 개발자의 작업 흐름을 혁신하고, 팀 협업을 강화하며, 코드 품질을 향상시키는 역할을 합니다. 따라서 다양한 AI 코드 생성 도구들의 기능, 장점과 단점을 분석하는 것은 이 시대의 개발 환경을 이해하는 데 필요합니다.- AI 코드 생성기는 이제 단순한 도우미가 아니라, 실제로 개발과정의 핵심적인 파트너로 성장하고 있습니다. 특정 도구의 선택은 프로젝트의 성공 여부를 크게 좌우할 수 있으며, 여러 도구 중 어떤 것이 가장 적합한지는 사용자의 요구와 팀의 구조에 따라 달라질 수 있습니다. 따라서 이러한 도구들을 비교하고 분석하는 것은 매우 중요한 작업입니다.- ### 5-1. 10가지 주요 AI 코드 생성기 기능·장단점 요약- 다양한 AI 코드 생성기 중에서 10개의 주요 도구를 선정하여 그 기능과 장단점을 요약하였습니다. 각 도구는 특정 기능에 특화되어 있으며, 사용 용도에 따라 효과적인 선택이 이루어져야 합니다. 예를 들어, GitHub Copilot은 강력한 코드 자동 완성 기능과 통합된 학습 기능을 통해 개인 개발자와 팀 모두에게 유용하나, 보안상의 이유로 민감한 프로젝트에는 적합하지 않을 수 있습니다.- 반면에, Replit은 사용자 친화적인 인터페이스와 팀 협업 기능을 제공하여 초보자와 경력 개발자 모두에게 적합합니다. 그러나 상대적으로 많은 기능이 통합되어 인프라 관리에 추가적인 학습이 필요할 수 있습니다. 각 도구의 인상적인 기능은 다음과 같습니다. 1) GitHub Copilot: 예측 코드 생성, 다국어 기능, 지속적인 학습. 2) Replit: 실시간 협업, 객체 저장소, 에이전트 기능. 3) Cursor: 스마트 자동 완성 및 상황 인식. 4) Lovable.dev: 풀스택 개발 환경, 연속적 호스팅, GitHub 통합.- 각 도구의 장단점을 제대로 이해하고 활용한다면, 기업의 코드 작성 프로세스를 획기적으로 개선할 수 있습니다.- ### 5-2. Replit 등 대표 플랫폼의 에이전트·배포·협업 기능- Replit은 클라우드 기반 환경에서 에이전트를 활용하여 애플리케이션 개발을 지원하는 대표적인 플랫폼입니다. 에이전트 v2는 사용자의 대화 내용을 바탕으로 자연어로 명령을 수행하고, 코드를 작성하는 과정에서 기계 학습을 통해 더 나은 결과를 생성합니다. 사용자는 오히려 더 많은 시간을 절약하고 창의적인 작업에 집중할 수 있습니다.- 또한, Replit의 협업 기능은 팀 프로그래밍에 최적화되어 있습니다. 팀원들은 실시간으로 함께 코드 작성, 디버깅 및 배포 작업을 수행할 수 있어 작업 효율성을 높입니다. 이러한 협업 기능은 특히 소규모 팀과 스타트업에서 효과적으로 나타납니다. 하지만 클라우드 기반 서비스이기 때문에 인터넷 연결이 필수적이며, 민감한 데이터를 처리하는 데 있어 일부 사용자에게 불안감을 줄 수 있습니다.- 편리함과 안정성만을 찾는 사용자를 위해 Windsurf와 같은 로컬 IDE 기반의 코드 편집기 또한 주목받고 있습니다. 이는 클라우드와의 연결이 필요 없으며, 오프라인 상태에서도 높은 성능을 발휘합니다. 각 플랫폼의 특성을 고려하여 조합하면 더 효과적인 개발 환경을 구축할 수 있습니다.- ### 5-3. 도구별 활용 시나리오 및 성능 비교- AI 코드 생성 도구들은 각각의 기능과 특성에 따라 다양한 활용 시나리오가 존재합니다. 예를 들어, GitHub Copilot은 대규모 오픈소스 프로젝트에서 일반적으로 사용되며, 누구에게나 접근이 용이한 코드 제안을 통해 신속한 개발을 도와줍니다. 와 같은 대규모 솔루션에서는 보안이 필수적이므로 지극히 주의가 필요합니다.- 반면 Replit은 시작 단계의 스타트업이나 부트캠프 등에서 인기를 끌고 있으며, 초보자들이 스크립트를 작성하고 간단한 애플리케이션을 만들기에 적합합니다. 이러한 교육적인 측면도 고려해야 합니다. Windsurf는 보다 고급 코드 편집 및 오프라인 환경이 필요한 사용자에게 적합하여 프리랜서 개발자들이나 개인 프로젝트를 추진하는 이들에게 이상적입니다.- 기본적으로 각 도구의 성능은 사용자의 요구와 프로젝트의 특성에 따라 매우 다르게 나타납니다. 예를 들어, 고급 사용자는 지연 시간과 성능을 중시할 수 있지만, 초보자는 사용자 경험과 접근성에 중점을 두는 경향이 있습니다. 그러므로 이러한 도구를 선택할 때에는 반드시 사용자의 요구와 환경을 충분히 고려해야 합니다.## 6. 미래 전망 및 시사점- AI의 발전은 단순한 기술 혁신을 넘어, 인류의 생활 방식과 일하는 방식을 근본적으로 변화시키고 있습니다. 특히 바이브 코딩의 확산은 이러한 변화의 중심에서 새로운 가능성을 열어주고 있습니다. 이 과정을 통해 개발자, 기업, 그리고 사회 전체가 어떻게 적응할 것인지에 대한 논의는 앞으로의 방향성을 잡는 데 중요합니다. 따라서 AI의 자동화가 가져올 개발자 역할의 변화와 플랫폼과 스타트업 간의 연계, 기술의 상호 발전 방향에 대한 통찰이 필요합니다.- ### 6-1. 주요 기술의 상호 보완적 발전 방향- AI 코드 생성기와 자연어 프로세싱(NLP) 기술의 발전은 서로를 보완하며 지속적으로 진화하고 있습니다. 예를 들어, Replit과 같은 플랫폼은 비 프로그래머도 소프트웨어 개발에 참여할 수 있도록 도와주며, 이는 결국 개발 생태계의 다변화를 가져옵니다. 이처럼 다양한 도구가 상호 연결됨으로써, 사람들이 코드를 작성하는 방식을 극 대화하는 기회를 제공합니다. 또한, AI 기반 플랫폼은 사용자가 입력한 자연어 명령을 바탕으로 즉각적인 피드백과 결과를 제공하여, 개발 속도를 높이고 품질을 개선합니다. 이는 코드 작성에서의 인간의 역량을 한 단계 끌어올리는 데 기여하고 있으며, 향후 이러한 기술의 발전 방향은 더욱 협업적이고 직관적인 개발 환경으로 이어질 것입니다. 따라서 이러한 혁신이 어떻게 산업 전체에 파급효과를 미칠 것인가에 대한 연구가 필요합니다.## 7. 결론- 이번 보고서를 통해 바이브 코딩 그리고 AI 기술이 소프트웨어 개발의 혁신을 이끌고 있음을 확인할 수 있었습니다. 데이터브릭스의 통합 플랫폼 전략이 기업들이 데이터 활용을 극대화하는 데 기여하는 동시에, 스타트업들이 생겨나는 환경은 기회이자 도전과제로 작용합니다. 특히, AI 코드 생성 도구들은 단순한 코드 생성기를 넘어, 개발자들의 업무 방식을 혁신하며 이들의 지속적인 활용이 요구됩니다. 앞으로의 연구는 이러한 기술들이 기업과 개인의 개발 환경에 미칠 장기적인 영향을 심화하여 분석하고, 변화하는 시장 환경에 대한 비즈니스 전략을 재정립하는 데 초점을 맞춰야 하겠습니다. AI 기술의 발전은 결국 인간의 일하는 방식을 변화시키고 있으며, 이에 따라 새로운 접근과 대처가 필요합니다.## 용어집- 바이브 코딩: 개발 지식이 부족한 사용자도 손쉽게 코드를 작성할 수 있도록 돕는 인공지능 기반의 코드 생성 시스템을 의미.- 데이터브릭스: 기업의 데이터 생태계를 통합하고 AI 중심의 혁신을 통해 데이터를 효과적으로 활용할 수 있도록 지원하는 플랫폼.- 레이크베이스(Lakebase): 데이터브릭스에서 출시한 서버리스 OLTP 데이터베이스로, PostgreSQL과 호환되며 데이터 처리의 효율성과 확장성을 제공.- AI 자동화: 인공지능 기술을 활용하여 반복적이고 기본적인 작업을 자동으로 수행하도록 하는 프로세스.- 에이전트 브릭스(Agent Bricks): 데이터브릭스의 도구로, 자연어 명령을 통해 AI 에이전트를 자동으로 생성 및 관리하는 기능을 가지고 있음.- AI 판사(judges): AI 에이전트의 생성 결과를 평가하고 개선하는 기능으로, 품질을 모니터링하는 역할을 함.- BI(비즈니스 인텔리전스): 기업이 데이터를 분석하고 제공하여 비즈니스 의사결정을 지원하는 기술과 시스템을 통칭.- 스타트업: 신생 기업으로, 주로 혁신적인 제품이나 서비스를 바탕으로 빠른 성장을 목표로 하는 비즈니스 모델.- 코드 생성기: 인공지능 기술을 활용하여 소프트웨어 개발자가 아닌 사용자가 코드 작성을 가능하게 도움.- AI 코드 생성: 인공지능이 자동으로 소프트웨어 코드를 작성하는 기술.- 머신러닝: 인공지능의 한 분야로, 컴퓨터가 데이터를 학습하고 예측할 수 있도록 하는 알고리즘 및 기술.- 서버리스: 서버 관리 없이 클라우드에서 애플리케이션을 실행할 수 있는 아키텍처 모델.- 클라우드 기반: 인터넷을 통해 클라우드 서비스 제공업체가 제공하는 리소스를 활용하는 방식.출처 문서- **데이터브릭스, \\'바이브 코딩\\'으로 기업 AI 생태계 통합 선언 - TokenPost** - **AI \\'바이브 코딩\\' 스타트업, 몸값 고공행진…수익성은 글쎄 : 네이트 뉴스** - **바이브 코딩을 위한 최고의 AI 코드 생성기 10가지 (2025년 XNUMX월) - Unite.AI** \"}, {\\'title\\': \\'개발자는 결국 대체될까? 바이브코딩 트렌드 리포트\\', \\'url\\': \\'https://eopla.net/magazines/32364\\', \\'markdown\\': \\'![]( 결국 대체될까? 바이브코딩 트렌드 리포트2025. 07. 23![]( 이 글은  에서 발행되었습니다.>> 가장 전문적인 바이브코딩 인사이트를 매주 받아보세요.>> [[구독하러 가기]]( 이미지]( 최근 가장 핫하고 변화도 빠른 분야입니다.매일 새로운 서비스가 등장하고 이를 소개하는 컨텐츠도 많은데요,그중\\\\xa0**팀차원에서, 실무수준으로 운영가능한 코드** 를 작성하는 관점의 컨텐츠는 거의 없습니다.이번 레터에서는 바이브코딩 컨설팅업체를 운영하며 느낀 바이브코딩의 현 위치와,\\\\xa0**제대로 사용하기위한 준비사항** 들을 소개해보겠습니다.## **바이브코딩이란?**바이브코딩은 쉽게 말해\\\\xa0AI가 대신 코딩하는 방법론입니다.어려운 개발언어 공부할 필요 없이 \"계산기 만들어줘\"처럼 요청하면 되는거죠.아티클을 더 읽고 싶다면?이오플래닛에 가입해주세요.로그인 후 모든 아티클을 무료로 볼 수 있어요.]( 7]( 뉴스레터]( 어썸데브 · CEO 댓글 5]( studio약 2달 전]( 뉴스레터 님의 아티클이 EO 뉴스레터에 실렸습니다. 이번 주 이오레터를 확인하세요!>>> ]( 0]( l 가능성에 반응하는 사람들약 2달 전]( 쉽게 이해할 수 있어 잘 읽었습니다. 한 두개 프로잭트에 활용해 봐야 겠어요. 고맙습미다.]( 0](\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0개발자약 2달 전]( 실력에서 나온 통찰력 있는 내용이라 배우고 갑니다.다음글도 기대가 되네요~]( 0]( 코리아\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Manager약 2달 전]( 맥을 주사 바늘처럼 정교하게 찔러주셔서많은 영감을 얻고 갑니다. 향후 글 기대 됩니다.]( 1]( 뉴스레터어썸데브\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0CEO약 2달 전]( 정말 감사합니다! 앞으로도 좋은 컨텐츠 남기겠습니다. :)]( 0추천 아티클최바울  페오펫]( 약 3년 전]( 약 3년 전디오니소스  디오니소스\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0Product Owner]( 1년 이상 전]( Product Owner  1년 이상 전[# 사업전략# 마인드셋# 트렌드# 기타‘챗GPT의 아버지’ 샘 알트만에 대해 알아보자 [아티클 한 눈에 보기]0.샘 알트만이라는 독특한 캐릭터1.\"이해 받지 못하는 것이 강점이다\"2.CEO는 어떻게 시간을 써야 하는가3.실리콘밸리 요다의 3가지 깨달음4.핵융합, 기본소득, 규제, 정치적 올바름\\\\xa0\\\\xa02022년 여름부터 연일 헤드라인을 장식하는 사람이 ...](  eo\\\\xa0\\\\xa0\\\\xa0|\\\\xa0\\\\xa0\\\\xa0에디터]( 2년 이상 전]( 에디터  2년 이상 전]( 뉴스레터]( 어썸데브 · CEO ]( 7![](\\'}]', name='web_search_tool', call_id='call_5mCEXZ1B7lEYIJoCuIRMLw5l', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'Real time emotion aware applications: A case study employing ...\\', \\'url\\': \\'https://www.sciencedirect.com/science/article/abs/pii/S0169260712000831\\', \\'markdown\\': \\' - - - - Other access optionsSearch ScienceDirect## Article preview- - - - - ]( \"Go to Computer Methods and Programs in Biomedicine on ScienceDirect\")## , July 2012, Pages 16-27]( Real time emotion aware applications: A case study employing emotion evocative pictures and neuro-physiological sensing enhanced by Graphic Processor UnitsAuthor links open overlay panel , , , Show moreAdd to MendeleyCite ## AbstractIn this paper the feasibility of adopting  Units towards real-time emotion aware computing is investigated for boosting the time  employed in such applications. The proposed methodology was employed in analysis of encephalographic and electrodermal data gathered when participants passively viewed emotional evocative stimuli. The  effectiveness when processing electroencephalographic and electrodermal recordings is demonstrated by comparing the  of chaos/complexity analysis through  (multi-channel correlation dimension/D2) and  (computation of  level/SCL) into various popular programming environments. Apart from the beneficial role of parallel programming, the adoption of special design techniques regarding  may further enhance the time minimization which approximates a factor of 30 in comparison with ANSI C language (single-core sequential execution). Therefore, the use of  parallel capabilities offers a reliable and  for real-time sensing the user\\\\\\'s affective state.## IntroductionEmotion aware computing was for a large period a neglected topic in the scientific community [1]. However, recent neuroscience findings have highlighted the critical role of emotions in a variety of cognitive functions like decision making [2], memory [3] and perception [4]. These arguments demonstrated the significance of emotional intelligence [5] not only when interacting with other people but also between human and machines [6]. Therefore, motivated research efforts investigate how to provide computers with abilities to recognize the user\\\\\\'s emotional state and to naturally adapt to it [7]. The importance of emotion aware computing is desirable only in cases where the user should interact with the machine in order to achieve high performance during the task procedure that should be accomplished [8]. So, providing the machine with the capability to robustly sense the users’ negative feelings [9] (frustration, anger, stress, anxiety, disappointment, etc.) the appropriate feedback may be given to neutralize their mood [10] and to encourage them to improve their performance in several applications like tests controlled through computer [11], virtual gaming [12] or remote monitoring of elderly or disabled people [13], [14]. Initial research attempts have demonstrated that the core element of a successful affective computing system is its ability to emulate the ways that are employed in the communication between human beings [15]. The pioneering work of MIT group led to the introduction of the “Affective Computing” term and to the establishment of a framework that could be adopted for a successful human–computer interaction (HCI) system [16], while also dealing with the challenges that have to be faced and the expectations created by potential applications [8].Previous research attempts have adopted communicative ways like facial expressions [17] and posture recognition [18]. However, several limitations occur since these modalities are highly dependent from the users’ personality [19] and their culture, resulting thus in enhanced inter-subject variability. Robust emotion recognition assumes the utilization of exaggerated expressions that are unlikely to be elicited in real-life situations [20]. Moreover, the use of cameras produces huge amount of data, while also communicates irrelevant information (e.g. subject\\\\\\'s identity) which the user may be unwilling to reveal [7]. Since the aforementioned methodologies are based on the recognition of externally expressed emotions, some innermost may not be easily recognized [8]. Such feelings are not easily communicated even among human beings and may be better recognized by neuro-physiological sensing [7]. Data fusion [21] from both the central and the autonomic nervous system may create discrete emotional patterns for a wide range of emotions [22], which are poorly distinguishable otherwise. However, special care should be given to the experimental methodology used for emotion elicitation.So, a key issue towards the achievement of a robust emotion aware computerized system is the establishment of a framework that is in close connection with the modern emotional theory assuring thus the reliable emotion elicitation. Recent trends regard emotions as behavioral attitudes related with evolutionary processes aiming to assure the human\\\\\\'s survival and perpetuation [23], [24], [25]. Therefore, each situation may be judged as either a pleasant or an unpleasant one. Its importance modulates the activation level needed in order to confront the stimulus appeared. Erotic or life-threatening situations require higher activation degree than melancholic or relaxing occasions. Adopting this notion, a bi-directional model was proposed. According to this approach, emotional processing is governed by two motivational systems which are the appetitive approach dealing with the pleasant situations and the defensive one activated in case of life-threatening occasions. The activation of the aforementioned systems is described through the valence dimension, while the activation degree is represented by the arousal dimension. So, these affective variables form a 2D emotional space.The International Affective Picture System (IAPS) collection adopts the aforementioned emotional model and provides a variety of affective visual stimuli as well as their normative ratings for both the arousal and valence dimension [26]. The use of this picture collection with simultaneous neurophysiological recordings demonstrated the facilitated encoding of emotional stimuli [27]. The combination of central nervous (event-related potentials/ERPs) and autonomic (electrodermal) activity revealed a significant correlation between skin conductance responses (SCRs) and the arousal ratings of the IAPS stimuli [23]. Moreover, late ERPs were more positive for emotional pictures [28], while their time course was influenced by the valence dimension [29]. A recent study investigated whether emotional processing is affected by the subject\\\\\\'s gender. Early (N100) and mid (N200) ERPs were significantly greater for female participants during passive viewing of unpleasant pictures [30].The bi-directional emotion model and the aforementioned neuroscience findings have not been widely adopted until now in the field of emotion aware computing. Relying on these notions, a Mahalanobis distance-based classification scheme was proposed for discriminating emotional instances selected from the IAPS collection. The output of the recognition sub-system was then used by an avatar which emulated the user\\\\\\'s affective state by adapting its face and voice characteristics [14]. However, there was need for further improvement of the classification accuracy by applying data mining (decision trees) and pattern recognition (Support Vector Machines) techniques [31]. Towards the achievement of a reliable emotion-aware application, extended feature fusion from different neuro-physiological modalities was proposed as well as a close connection with the theoretical emotional framework and the independency of the two emotional variables. Moreover, gender specific classifiers were proposed according to [32] in order to further enhance the method\\\\\\'s robustness which reached 81.3% for 4 emotional categories.Despite the adequate classification accuracy that was demonstrated by these research efforts, there are several open issues that should be further investigated prior to the introduction of real world emotion aware applications. The proposed discrimination framework was developed for research purposes. It is oriented towards the achievement of the optimal result employing time-consuming computations that reduce its applicability. Moreover, it has been developed as an isolated application under controlled lab environments which may differ from generic real-life applications. So, an integrative approach should be adopted for linking the emotion methodology with the acquisition subsystem as well as with the avatar behavior-generation routines. Then, the proposed system would be able to gather short segments of neuro-physiological data which are processed within fractions of seconds. The user\\\\\\'s affective state is recognized and serves as an input to the avatar which adapts its behavior either to mirror or to neutralize the user\\\\\\'s affective state.The current study investigates the feasibility of the Graphics Processing Unit (GPU) for the fast processing of neuro-physiological data. Short segments from both the central (ERPs) and the autonomic (SCRs) nervous system serve as an input to the system. These data are parallel processed during the feature extraction stage by algorithmic procedures that were re-designed in order to provide the optimal solution regarding the memory management. So, the aim of this paper is to demonstrate that the adoption of parallel processing may be greatly beneficial for the development of real-time emotion aware applications. Therefore, it is not focused to the extensive description of the parallelization techniques adopted. Moreover, it highlights some significant issues like time consumption on data transfer between host and device that should be taken into consideration during the system design in order to further minimize the execution time. So, the work\\\\\\'s contribution lays on the introduction of a framework for the adoption of parallel programming for real time emotion-aware applications.So, the remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture as well as with special programming techniques adopted for the proper parallelization of an algorithm. Then, a brief description of the parallelized algorithms is performed. Within Section 3 results of the algorithms’ implementation and the execution time are presented in Section 4. Finally, the discussion of this paper appears in Section 4.## Section snippets## The NVIDIA GPU architecture – CUDAThe voracious market demand for realtime and high definition 3D graphics led to the introduction of highly parallel, multithreaded, manycore processor Graphic Processor Unit (GPU). Characterized by high memory bandwidth and astounding computational horsepower, the GPU (Fig. 1) serves the demanding requirements of the modern designs and implementations. Its main difference with CPU is that it facilitates compute-intensive and parallel computation. Stemming from the graphics rendering demands, it## ResultsThe features (D2 complexity and SCL values) obtained from the parallel processing of electroencephalographic and autonomic data were analyzed in order to highlight differences among the various emotional states. Each emotional state is characterized by two independent variables (valence and arousal degree).Regarding the multi-channel D2 correlation dimension algorithm, the analysis was performed for each participant and for each one of the four emotional categories. As depicted in Fig. 9 (left## DiscussionThe current work aims to highlight the significant acceleration that may be achieved to emotion aware computing in case of adopting parallel programming on GPU. So, the detailed description of the parallelization techniques are beyond the paper\\\\\\'s scope and may be found in [33], [36]. These recent code execution techniques are exploited in boosting complex and time-consuming computations, such as nonlinear dynamic analysis or processing of dense data arrays. Selected results are included in## ConclusionA novel parallel-programming approach based on the CUDA architecture was proposed in order to accelerate the processing of neurophysiological recordings requiring complex computations. It aims to facilitate the already proposed emotion discrimination methodologies with the computing solution needed in order to perform real-time classification. To this end, the importance of this work towards an integrative approach of providing the machines with the capabilities to adapt their behavior## Conflict of interestThe authors do not report any conflict of interest.Special issue articlesRecommended articles- K. Sergerie _et al._### ### Neuroscience and Biobehavioral Reviews(2008)P. Vuilleumier _et al._### ### Nature Neuroscience(2003)- R.W. Picard _et al._### ### IEEE Transactions on Pattern Analysis and Machine Intelligence(2001)- J. Klein _et al._### ### Interacting with Computers(2002)- F. Nasoz _et al._### - C.A. Frantzidis _et al._### - P.J. Lang _et al._### (1997)- C. Amrhein _et al._### ### International Journal of Psychophysiology(2004)- C. Lithari _et al._### ### Brain Topography(2010)- G. Noaje _et al._- K.C. Berridge _et al._### Affective neuroscience of pleasure: reward in humans and animals### Psychopharmacology(2008)A. Bechara _et al._### Emotion, decision making and the orbitofrontal cortex### Cerebral Cortex(2000)M. Pantic _et al._### Toward an affect-sensitive multimodal human–computer interaction### Proceedings of the IEEE(2003)E. Hudlicka### To feel or not to feel: the role of affect in human–computer interaction### International Journal of Human–Computer Studies(2003)R.W. Picard### Affective computing: challenges### International Journal of Human–Computer Studies(2003)B. Kort _et al._### An affective module for an intelligent tutoring systemE. Hudlicka### Affective computing for game designC.A. Frantzidis _et al._### Description and future trends of ICT solutions offered towards independent living: the case of LLM projectP.D. Bamidis _et al._### An integrated approach to emotion recognition for advanced emotional intelligenceB. Reeves _et al._### The Media Equation(1996)R.W. Picard### Affective Computing(1997)View more references- ### 2019, Information FusionCitation Excerpt :Mehmood et al. utilized the late positive potential (LPP)-based feature extraction method and used two classifiers (SVM an KNN) for emotion detection from EEG [44]. Konstantinidis et al. utilized K-nearest neighbors (KNN) classifiers to detect six basic emotions from a three-channel forehead EEG combined with GSR [45]. Recently, Alarcao et al. [10] presented an analysis which shows that 59% of works in the literature used SVM classifiers to detect emotions, 8% of works used different variations of SVM like adaptive SVM etc.; 14% of the works used the kNN, 6.3% of the works used Linear Discriminant Analysis (LDA) and 3.17% of the works used the Naive Bayes (NB) classifier.Show abstractRecently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.- ### 2017, Computer Methods and Programs in BiomedicineShow abstractUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states.DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%.The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.- ### 2017, Frontiers in NeuroroboticsShow abstractUsing machine-learning methodologies to analyze EEG signals becomes increasingly attractive for recognizing human emotions because of the objectivity of physiological data and the capability of the learning principles on modeling emotion classifiers from heterogeneous features. However, the conventional subject-specific classifiers may induce additional burdens to each subject for preparing multiple-session EEG data as training sets. To this end, we developed a new EEG feature selection approach, transfer recursive feature elimination (T-RFE), to determine a set of the most robust EEG indicators with stable geometrical distribution across a group of training subjects and a specific testing subject. A validating set is introduced to independently determine the optimal hyper-parameter and the feature ranking of the T-RFE model aiming at controlling the overfitting. The effectiveness of the T-RFE algorithm for such cross-subject emotion classification paradigm has been validated by DEAP database. With a linear least square support vector machine classifier implemented, the performance of the T-RFE is compared against several conventional feature selection schemes and the statistical significant improvement has been found. The classification rate and _F_-score achieve 0.7867, 0.7526, 0.7875, and 0.8077 for arousal and valence dimensions, respectively, and outperform several recent reported works on the same database. In the end, the T-RFE based classifier is compared against two subject-generic classifiers in the literature. The investigation of the computational time for all classifiers indicates the accuracy improvement of the T-RFE is at the cost of the longer training time.- ### 2014, Plos OneShow abstractInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method\\\\\\'s practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.- ### 2014, IEEE Transactions on Affective Computing- ### 2013, Computational and Mathematical Methods in MedicineCopyright © 2012 Elsevier Ireland Ltd. All rights reserved.The Identity Selector: Persistence Service\\'}, {\\'title\\': \\'Towards Emotion-awareness in Programming Education with ...\\', \\'url\\': \\'https://ieeexplore.ieee.org/document/9842520/\\', \\'markdown\\': \"Loading [MathJax]/extensions/TeX/ieee_stixext.jsTowards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation | IEEE Conference Publication | IEEE XploreSkip to Main Content# Towards Emotion-awareness in Programming Education with Behavior-based Emotion EstimationPublisher: IEEECite ThisPDF; ; ; ; ; All AuthorsSign In or Purchase1Cites inPaper239FullText Views- AlertsAdd to Citation Alerts* * *- Download PDF- Download References- Request Permissions- Save to- Alerts## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learnin...Show More## Metadata## Abstract:Existing studies in both psychology and software engineering have shown the importance of emotions in complex learning and programming tasks. For students who are learning to program, rich emotions are experienced which can provide valuable feedback to their teachers. To accurately model stu-dents\\' emotions, this paper adopts the well-recognized model of emotions during complex learning that involves four states: engaged, confused, frustrated, and bored. To perform continuous estimation of students\\' emotions in a non-intrusive manner, this paper proposes to track students\\' programming behavior and estimate their corresponding emotional states. Compare to the existing approaches on acquiring the students\\' emotional states with self-reports or bio-sensors, the proposed approach is more feasible in conducting real-world, and large-scale studies for not requiring extensive human interventions or additional devices. Evaluated using data collected from a real-world course project, the proposed approach is showed to be promising for achieving an estimation accuracy of 72.06 % for the above four emotional states. As an enabling technology, the proposed is potentially useful in supporting many applications and improve the quality of programming education in computer science.**Published in:** **Date of Conference:** 27 June 2022 - 01 July 2022**Date Added to IEEE _Xplore_:** 10 August 2022**ISBN Information:****Print on Demand(PoD) ISSN:** 0730-3157**DOI:** Publisher: IEEE**Conference Location:** Los Alamitos, CA, USA## Funding Agency:![]( Contents* * *### I. IntroductionBoth learning and computer programming are high cog-nitively demanding tasks which involve the experience of complex emotions during the process [1]–[3]. Studies from the areas of education [3], psychology [2], and software engineering [1], [4], [5] have more than once shown the significance of one\\'s emotions in affecting the performance and productivity of the learners and developers, respectively. Bringing the above researches together, this paper studies the problem of understanding the students\\' emotions during the task of learning to program.Sign in to Continue Reading## Authors## Figures## References## Citations## Keywords## Metrics## Footnotes]( Like This2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)Published: 20162024 International Conference on Inventive Computation Technologies (ICICT)Published: 2024Show More**References is not available for this document.**### IEEE Account- - ### Purchase Details- - - ### Profile Information- - - ### Need Help?- **US & Canada:** +1 800 678 4333- **Worldwide:** +1 732 981 0060- - - - - - - - - A not-for-profit organization, IEEE is the world\\'s largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.The Identity Selector: Persistence Service\"}]', name='web_search_tool', call_id='call_4PAMAyFwVRDo2ioyrSmVFaI2', is_error=False), FunctionExecutionResult(content='[{\\'title\\': \\'AI | 2025 Stack Overflow Developer Survey\\', \\'url\\': \\'https://survey.stackoverflow.co/2025/ai\\', \\'markdown\\': \\'### 84% of respondents are using or planning to use AI tools in their development process, an increase over last year (76%). This year we can see 51% of professional developers use AI tools daily.Do you currently use AI tools in your development process?     All RespondentsYes, I use AI tools daily47.1%Yes, I use AI tools weekly17.7%Yes, I use AI tools monthly or infrequently13.7%No, but I plan to soon5.3%No, and I don\\\\\\'t plan to16.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,662(68.7%)Professional DevelopersYes, I use AI tools daily50.6%Yes, I use AI tools weekly17.4%Yes, I use AI tools monthly or infrequently12.8%No, but I plan to soon4.6%No, and I don\\\\\\'t plan to14.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,004(53%)Learning to CodeYes, I use AI tools daily39.5%Yes, I use AI tools weekly18.7%Yes, I use AI tools monthly or infrequently15.1%No, but I plan to soon7.2%No, and I don\\\\\\'t plan to19.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,843(5.8%)Early Career DevsYes, I use AI tools daily55.5%Yes, I use AI tools weekly18.1%Yes, I use AI tools monthly or infrequently11.5%No, but I plan to soon2.5%No, and I don\\\\\\'t plan to12.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,360(13%)Early career defined as 1 - 5 years work experienceMid Career DevsYes, I use AI tools daily52.8%Yes, I use AI tools weekly16.8%Yes, I use AI tools monthly or infrequently13.5%No, but I plan to soon3.7%No, and I don\\\\\\'t plan to13.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,997(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsYes, I use AI tools daily47.3%Yes, I use AI tools weekly17.2%Yes, I use AI tools monthly or infrequently13%No, but I plan to soon6%No, and I don\\\\\\'t plan to16.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 13,001(26.5%)Experienced dev defined as 10+ years work experience### Conversely to usage, positive sentiment for AI tools has decreased in 2025: 70%+ in 2023 and 2024 to just 60% this year. Professionals show a higher overall favorable sentiment (61%) than those learning to code (53%).How favorable is your stance on using AI tools as part of your development workflow?     All RespondentsVery favorable22.9%Favorable36.8%Indifferent17.6%Unsure2.3%Unfavorable10.8%Very unfavorable9.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,412(68.2%)Professional DevelopersVery favorable23.5%Favorable37.7%Indifferent17.4%Unsure1.8%Unfavorable10.6%Very unfavorable9.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,814(52.7%)Learning to CodeVery favorable19.3%Favorable33.5%Indifferent16.6%Unsure4.3%Unfavorable13.6%Very unfavorable12.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,812(5.7%)Early Career DevsVery favorable22.8%Favorable40.3%Indifferent17%Unsure1.3%Unfavorable10.3%Very unfavorable8.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,293(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery favorable23.8%Favorable38.9%Indifferent16.2%Unsure1.5%Unfavorable11%Very unfavorable8.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,957(12.2%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery favorable23.9%Favorable36%Indifferent18.1%Unsure2.1%Unfavorable10.3%Very unfavorable9.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,941(26.4%)Experienced devs defined as 10+ years work experience### More developers actively distrust the accuracy of AI tools (46%) than trust it (33%), and only a fraction (3%) report \"highly trusting\" the output. Experienced developers are the most cautious, with the lowest \"highly trust\" rate (2.6%) and the highest \"highly distrust\" rate (20%), indicating a widespread need for human verification for those in roles with accountability.How much do you trust the accuracy of the output from AI tools as part of your development workflow?     All RespondentsHighly trust3.1%Somewhat trust29.6%Somewhat distrust26.1%Highly distrust19.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,244(67.8%)Professional DevelopersHighly trust2.7%Somewhat trust29.6%Somewhat distrust26.3%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,701(52.4%)Learning to CodeHighly trust6.1%Somewhat trust31.3%Somewhat distrust24.2%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,781(5.7%)Early Career DevsHighly trust3%Somewhat trust31.1%Somewhat distrust25.7%Highly distrust17.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,254(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsHighly trust2.8%Somewhat trust30.3%Somewhat distrust26.1%Highly distrust19.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,931(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsHighly trust2.5%Somewhat trust28.6%Somewhat distrust26.7%Highly distrust20.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,908(26.3%)Experienced devs defined as 10+ years work experience### In 2024, 35% of professional developers already believed that AI tools struggled with complex tasks. This year, that number has dropped to 29% among professional developers and is consistent amongst experience levels. Complex tasks carry too much risk to spend extra time proving out the efficacy of AI tools.How well do the AI tools you use in your development workflow handle complex tasks?     All RespondentsVery well at handling complex tasks4.4%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.1%Bad at handling complex tasks22%Very poor at handling complex tasks17.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know16.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 33,230(67.8%)Professional DevelopersVery well at handling complex tasks3.9%Good, but not great at handling complex tasks25.2%Neither good or bad at handling complex tasks14.2%Bad at handling complex tasks22.8%Very poor at handling complex tasks18.6%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know15.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,695(52.4%)Learning to CodeVery well at handling complex tasks7.9%Good, but not great at handling complex tasks25.8%Neither good or bad at handling complex tasks12.4%Bad at handling complex tasks19%Very poor at handling complex tasks16.3%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,779(5.7%)Early Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks28.1%Neither good or bad at handling complex tasks13.4%Bad at handling complex tasks23.6%Very poor at handling complex tasks19.2%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know11.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 6,258(12.8%)Early career defined as 1 - 5 years work experienceMid Career DevsVery well at handling complex tasks4%Good, but not great at handling complex tasks25.4%Neither good or bad at handling complex tasks13.8%Bad at handling complex tasks23.9%Very poor at handling complex tasks19.5%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know13.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,922(12.1%)Mid career defined as 5 - 10 years work experienceExperienced DevsVery well at handling complex tasks3.6%Good, but not great at handling complex tasks23.5%Neither good or bad at handling complex tasks14.9%Bad at handling complex tasks22.1%Very poor at handling complex tasks17.9%I don\\\\\\'t use AI tools for complex tasks / I don\\\\\\'t know18%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,901(26.3%)Experienced dev career defined as 10+ years work experience### Developers show the most resistance to using AI for high-responsibility, systemic tasks like Deployment and monitoring (76% don\\\\\\'t plan to) and Project planning (69% don\\\\\\'t plan to).Which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently Mostly AISearch for answers54.1%Generating content or synthetic data35.8%Learning new concepts or technologies33.1%Documenting code30.8%Creating or maintaining documentation24.8%Learning about a codebase20.8%Debugging or fixing code20.7%Testing code17.9%Writing code16.9%Predictive analytics11%Project planning10.8%Committing and reviewing code10.2%Deployment and monitoring6.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,202(22.9%)Currently Partially AISearch for answers55.8%Generating content or synthetic data28.6%Learning new concepts or technologies47.4%Documenting code30.3%Creating or maintaining documentation27.3%Learning about a codebase32.7%Debugging or fixing code47.1%Testing code27.5%Writing code59%Predictive analytics12.7%Project planning17.1%Committing and reviewing code22.6%Deployment and monitoring10.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,991(42.8%)Plan to Partially Use AISearch for answers24%Generating content or synthetic data28%Learning new concepts or technologies27.9%Documenting code30.5%Creating or maintaining documentation32.5%Learning about a codebase34.9%Debugging or fixing code30.9%Testing code34.7%Writing code32.4%Predictive analytics25%Project planning24.8%Committing and reviewing code31.4%Deployment and monitoring25%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,518(45.9%)Plan to Mostly Use AISearch for answers17.2%Generating content or synthetic data28.9%Learning new concepts or technologies15.7%Documenting code28.6%Creating or maintaining documentation31.8%Learning about a codebase23.1%Debugging or fixing code14.8%Testing code25.8%Writing code12.4%Predictive analytics23%Project planning14.3%Committing and reviewing code16.3%Deployment and monitoring15.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,790(26.1%)Don\\\\\\'t Plan to Use AI for This TaskSearch for answers19.6%Generating content or synthetic data38.2%Learning new concepts or technologies32.3%Documenting code38.5%Creating or maintaining documentation39.6%Learning about a codebase39.4%Debugging or fixing code36.4%Testing code44.1%Writing code28.9%Predictive analytics65.6%Project planning69.2%Committing and reviewing code58.7%Deployment and monitoring75.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,349(51.7%)### Respondents who said they are currently using mostly AI tools to complete tasks in the development workflow are highly satisfied with and frequently using AI to search for answers or learn new concepts; respondents plan to mostly use AI in the future for documentation and testing tasks and are slightly less satisfied with the tools they are using now.How favorable is your stance on using AI tools as part of your development workflow and which parts of your development workflow are you currently integrating into AI or using AI tools to accomplish or plan to use AI to accomplish over the next 3 - 5 years? Please select one for each scenario.    Currently mostly AINumber of responses6,053685Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents5.255.35.355.45.455.55.555.65.65%5%10%15%20%25%30%35%40%45%50%55Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 11,184(22.8%)Currently partially AINumber of responses12,3822,194Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.74.754.84.854.94.9555.055.15.155.25.25%10%15%20%25%30%35%40%45%50%55%60Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,980(42.8%)Plan to partially use AINumber of responses7,8585,400Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents3.73.83.944.14.24.34.44.54.64.74.84.9%24%25%26%27%28%29%30%31%32%33%34%35Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 22,500(45.9%)Plan to mostly use AINumber of responses4,0561,588Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents4.64.654.74.754.84.854.94.9555.055.15.155.2%12%14%16%18%20%22%24%26%28%30%32Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,777(26.1%)Don\\\\\\'t plan to use AI for this taskNumber of responses19,2114,953Average AI Sentiment Recoded (1 - Very Unfavorable to 6 - Very Favorable)Percent of respondents2.42.62.833.23.43.63.84%20%25%30%35%40%45%50%55%60%65%70%75%80Commit/ReviewDocsDebug/fixOpsDocumenting codeContent/DataLeaning codebaseLearning techPredictive analyticsProject planningAnswersTesting codeWriting codeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 25,332(51.7%)### The biggest single frustration, cited by 66% of developers, is dealing with \"AI solutions that are almost right, but not quite,\" which often leads to the second-biggest frustration: \"Debugging AI-generated code is more time-consuming\" (45%)When using AI tools, which of the following problems or frustrations have you encountered? Select all that apply.All RespondentsAI solutions that are almost right, but not quite66%Debugging AI-generated code is more time-consuming45.2%I don’t use AI tools regularly23.5%I’ve become less confident in my own problem-solving20%It’s hard to understand how or why the code works16.3%Other (write in):11.6%I haven’t encountered any problems4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,476(64.2%)### In a future with advanced AI, the #1 reason developers would still ask a person for help is \"When I don’t trust AI’s answers\" (75%). This positions human developers as the ultimate arbiters of quality and correctness.In the future, if AI can do most coding tasks, in which situations would you still want to ask another person for help? Select all that apply.All RespondentsWhen I don’t trust AI’s answers75.3%When I have ethical or security concerns about code61.7%When I want to fully understand something61.3%When I want to learn best practices58.1%When I’m stuck and can’t explain the problem54.6%When I need help fixing complex or unfamiliar code49.8%When I want to compare different solutions44.1%When I need quick help troubleshooting27.5%Other6.1%I don’t think I’ll need help from people anymore4.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 29,163(59.5%)### Most respondents are not vibe coding (72%), and an additional 5% are emphatic it not being part of their development workflow.In your own words, is \"vibe coding\" part of your professional development work? For this question, we define vibe coding according to the , the process of generating software from LLM prompts.     All RespondentsYes, emphatically0.4%Yes11.9%Yes, somewhat2.8%I have tried it2.1%Not sure1.2%No72.2%No, emphatically5.3%Uncategorized4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 26,564(54.2%)18-24 years oldYes, emphatically0.3%Yes11.6%Yes, somewhat3.2%I have tried it2.4%Not sure1.2%No72.8%No, emphatically5.1%Uncategorized3.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 4,212(8.6%)25-34 years oldYes, emphatically0.4%Yes11.8%Yes, somewhat3.2%I have tried it1.6%Not sure1.3%No72.3%No, emphatically5.7%Uncategorized3.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,526(17.4%)35-44 years oldYes, emphatically0.5%Yes12%Yes, somewhat2.8%I have tried it2.2%Not sure1.1%No72%No, emphatically5.4%Uncategorized4.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 7,607(15.5%)45-54 years oldYes, emphatically0.5%Yes12.7%Yes, somewhat2.5%I have tried it1.9%Not sure1.3%No71.3%No, emphatically5.2%Uncategorized4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,838(7.8%)55-64 years oldYes, emphatically0.8%Yes11.4%Yes, somewhat2%I have tried it3.1%Not sure1.5%No71.3%No, emphatically4.6%Uncategorized5.4%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 1,657(3.4%)### AI agents are not yet mainstream. A majority of developers (52%) either don\\\\\\'t use agents or stick to simpler AI tools, and a significant portion (38%) have no plans to adopt them.Are you using AI agents in your work (development or otherwise)? AI agents refer to autonomous software entities that can operate with minimal to no direct human intervention using artificial intelligence techniques.    All RespondentsYes, I use AI agents at work daily14.1%Yes, I use AI agents at work weekly9%Yes, I use AI agents at work monthly or infrequently7.8%No, but I plan to17.4%No, I use AI exclusively in copilot/autocomplete mode13.8%No, and I don\\\\\\'t plan to37.9%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,877(65%)Professional DevelopersYes, I use AI agents at work daily14.9%Yes, I use AI agents at work weekly9.2%Yes, I use AI agents at work monthly or infrequently7.7%No, but I plan to17.2%No, I use AI exclusively in copilot/autocomplete mode14.2%No, and I don\\\\\\'t plan to36.7%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 24,752(50.5%)Learning to CodeYes, I use AI agents at work daily13.2%Yes, I use AI agents at work weekly7.8%Yes, I use AI agents at work monthly or infrequently7.4%No, but I plan to15.6%No, I use AI exclusively in copilot/autocomplete mode12.1%No, and I don\\\\\\'t plan to44.1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,610(5.3%)Professional AI UsersYes, I use AI agents at work daily17.5%Yes, I use AI agents at work weekly10.8%Yes, I use AI agents at work monthly or infrequently8.9%No, but I plan to18.6%No, I use AI exclusively in copilot/autocomplete mode16.3%No, and I don\\\\\\'t plan to27.8%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 20,892(42.6%)Learning AI UsersYes, I use AI agents at work daily16.5%Yes, I use AI agents at work weekly9.6%Yes, I use AI agents at work monthly or infrequently8.7%No, but I plan to16.9%No, I use AI exclusively in copilot/autocomplete mode14.7%No, and I don\\\\\\'t plan to33.6%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,019(4.1%)### 52% of developers agree that AI tools and/or AI agents have had a positive effect on their productivity.Have AI tools or AI agents changed how you complete development work in the past year?All RespondentsYes, to a great extent16.3%Yes, somewhat35.3%Not at all or minimally41.4%No, but my development work has significantly changed due to non-AI factors2.6%No, but my development work has changed somewhat due to non-AI factors4.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 31,636(64.5%)### If you happen to be using AI agents at work and you are a software developer, chances are high that you are using agents for software development (84%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.Industry PurposeSoftware engineering83.5%Data and analytics24.9%IT operations18%Business process automation17.6%Decision intelligence11.3%Customer service support11.2%Marketing8.6%Cybersecurity7.4%Robotics3.9%Other2.2%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,301(25.1%)### TL;DR: Agents used outside of work are mostly used for language processing tasks (49%).What industry purposes or specific tasks are you using AI agents in your development work? Select all that apply from both lists.General PurposeLanguage processing49%Integration with external agents and APIs38.3%MCP servers34.4%Agent/multi-agent orchestration28.1%Vector databases for AI applications24.1%Multi-platform search enablement19.4%Personalized agent creation18.3%Other3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 5,797(11.8%)### The most recognized impacts are personal efficiency gains, and not team-wide impact. Approximately 70% of agent users agree that agents have reduced the time spent on specific development tasks, and 69% agree they have increased productivity. Only 17% of users agree that agents have improved collaboration within their team, making it the lowest-rated impact by a wide margin.To what extent do you agree with the following statements regarding the impact of AI agents on your work as a developer?All Respondents27.3%35.9%21.3%8.2%7.3%AI agents have accelerated my learning about new technologies or codebases.29.3%34.9%22.4%7%6.4%AI agents have helped me automate repetitive tasks.17.1%31.9%25.3%14.2%11.5%AI agents have helped me solve complex problems more effectively.6.6%10.7%40.5%20%22.2%AI agents have improved collaboration within my team.12.2%25.3%32.4%17.1%13.1%AI agents have improved the quality of my code.27.7%41%20.4%6%4.9%AI agents have increased my productivity.29.3%40.8%17.8%6.9%5.1%AI agents have reduced the time spent on specific development tasks.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 12,823(26.2%)### Is it a learning curve, or is the tech not there yet? 87% of all respondents agree they are concerned about the accuracy, and 81% agree they have concerns about the security and privacy of data.To what extent do you agree with the following statements regarding AI agents?All Respondents57.1%29.8%9.7%2.3%1.1%I am concerned about the accuracy of the information provided by AI agents.56.1%25.3%11.7%4.7%2.2%I have concerns about the security and privacy of data when using AI agents.16.5%29.7%37.3%12.6%3.9%Integrating AI agents with my existing tools and workflows can be difficult.15.5%27.9%31.8%17.8%6.9%It takes significant time and effort to learn how to use AI agents effectively.13.8%14.4%30.6%15%26.2%My company\\\\\\'s IT and/or InfoSec teams have strict rules that do not allow me to use AI agent tools or platforms25.4%27.9%31.8%10.3%4.6%The cost of using certain AI agent platforms is a barrier.Strongly agreeSomewhat agreeNeutralSomewhat disagreeStrongly disagreeDownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 28,930(59%)### When it comes to data management for agents, traditional, developer-friendly tools like Redis (43%) are being repurposed for AI, alongside emerging vector-native databases like ChromaDB (20%) and pgvector (18%).You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent memory or data management in the past year?All RespondentsRedis42.9%GitHub MCP Server42.8%supabase20.9%ChromaDB19.7%pgvector17.9%Neo4j12.3%Pinecone11.2%Qdrant8.2%Milvus5.2%Fireproof5%LangMem4.8%Weaviate4.5%LanceDB4.4%mem04%Zep2.8%Letta2.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,398(6.9%)### The agent orchestration space is currently led by open-source tools. Among developers building agents, Ollama (51%) and LangChain (33%) are the most-used frameworks.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent orchestration or agent frameworks in the past year?All RespondentsOllama51.1%LangChain32.9%LangGraph16.2%Vertex AI15.1%Amazon Bedrock Agents14.5%OpenRouter13.4%Llama Index13.3%AutoGen (Microsoft)12%Zapier11.8%CrewAI7.5%Semantic Kernel6%IBM watsonx.ai5.7%Haystack4.4%Smolagents3.7%Agno3.4%phidata2.1%Smol-AGI1.9%Martian1.7%lyzr1.5%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 3,758(7.7%)### Developers are primarily adapting their existing, traditional monitoring tools for this new task, rather than adopting new, AI-native solutions. The most used tools for AI agent observability are staples of the DevOps and application monitoring world: Grafana + Prometheus are used by 43% of agent developers, and Sentry is used by 32%.You indicated you use or develop AI agents as part of your development work. Have you used any of the following tools for AI agent observability, monitoring or security in the past year?All RespondentsGrafana + Prometheus43%Sentry31.8%Snyk18.2%New Relic13%LangSmith12.5%Honeycomb8.8%Langfuse8.8%Wiz6.9%Galileo6.2%Adversarial Robustness Toolbox (ART)5.5%Protect AI5%Vectra AI4.4%arize3.7%helicone3.2%Metero2.7%opik2.3%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 2,689(5.5%)### ChatGPT (82%) and GitHub Copilot (68%) are the clear market leaders, serving as the primary entry point for most developers using out-of-the-box AI assistance.You indicated you use or develop AI agents as part of your development work. Have you used any of the following out-of-the-box agents, copilots or assistants?All RespondentsChatGPT81.7%GitHub Copilot67.9%Google Gemini47.4%Claude Code40.8%Microsoft Copilot31.3%Perplexity16.2%v0.dev9.1%Bolt.new6.5%Lovable.dev5.7%AgentGPT5%Tabnine5%Replit5%Auto-GPT4.7%Amazon Codewhisperer3.9%Blackbox AI3.5%Roo code (Roo-Cline)3.4%Cody3%Devin AI2.7%Glean (Enterprise Agents)1.3%OpenHands (formerly OpenDevin)1%DownloadI acknowledge that the downloaded file is licensed under the Share  Responses: 8,323(17%) \\'}, {\\'title\\': \\'AI-Generated Code Stats 2025: How Much Is Written by AI?\\', \\'url\\': \\'https://www.elitebrains.com/blog/aI-generated-code-statistics-2025\\', \\'markdown\\': \"26 min read08/27/2025# AI-Generated Code Stats 2025: How Much Is Written by AI?#### AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development.Hey there, fellow tech enthusiasts! One thing is crystal clear as we look to the future: artificial intelligence (AI) is rapidly changing how we write code. From building entire functions to completing code snippets in milliseconds, AI is helping developers work faster and smarter. But the big question on everyone’s mind is: **How much of that code is AI-generated?** And more importantly, should developers or  worry about their jobs as AI evolves?As the founder of EliteBrains, where we focus on helping developers stay sharp with coding tests and skills development, I want to break down the AI-generated code stats for 2025, explore the tools that are making this happen, and give you a better understanding of what the future holds for software developers in this new world of AI-assisted coding.As AI takes over more of the repetitive coding tasks, the real question becomes how we measure human skill alongside machine-generated output. That’s exactly where **EliteBrains helps.**Put AI to the test and see which of your developers are truly qualified.\\\\xa0Mix and match AI-powered coding tests from 120+ programming languages, choose the right seniority level, and instantly spot who rises above the rest.Create your\\\\xa0**FREE AI coding tests today** **( only\\\\xa03 free slots available)**## **What Is AI-Generated Code?**AI-generated code is programming written or completed by artificial intelligence tools from natural language prompts, existing context or prior code patterns. In 2025, it typically means an AI assistant drafted the solution or produced most of the final lines that a developer then reviews and refines.## **What Percentage of \\\\xa0Code Is AI-Generated in 2025?**AI has been in the software development game for a while now, but its role in coding is reaching new heights in 2025. It’s not just about suggesting code anymore. AI tools are writing **massive** amounts of code, optimizing development, and helping developers be more efficient than ever before.Here’s the shocking truth: . That number alone shows just how integrated AI tools like **GitHub Copilot** have become in developers’ workflows. Whether it’s generating entire functions or suggesting small snippets of code, AI is now deeply embedded into the way we code.## **How Much Code Has AI Written So Far?**But just how much code is AI generating? Let’s break it down.![]( **How Does AI Code Generation Affect Developer Jobs and the Economy?**With AI generating such a massive amount of code, one big question looms: What does this mean for job security and the economy?![]( **How Many Lines of Code Have Been Generated by AI?**Buckle up, because here’s where it gets mind-blowing. As of 2024, **256 billion lines of code have already been generated by AI**. Think about that for a second—**256 billion lines**. That’s an insane amount of code, and we’re just getting started. From simple scripts to full-fledged software systems, AI is helping to write an overwhelming amount of code across all industries.This number is so massive that it\\'s hard to grasp. Consider this: the **total lines of code** written by humans in the last decade likely pales in comparison to this explosion of AI-generated code. And we’re still in the early stages of integrating AI tools into the development process. The future will see even more code churned out by AI as the tools become smarter, faster, and more efficient.## **How Is Google Using AI to Write Code in 2025?**Let’s look at how AI is playing a huge role in tech giants like Google. Google, known for its cutting-edge AI innovations, has been using AI to power some of its most important software. According to Sundar Pichai, Google’s CEO, **over a quarter of all new code generated by Google is written by AI**. This includes everything from Google Cloud services to Android apps. AI is also playing a significant role in developing machine learning models that power a large chunk of Google’s products.Pichai’s announcement isn’t just a marketing stunt—it’s a reflection of how deeply integrated AI has become in Google’s development process. Whether it\\'s AI-driven infrastructure management or software development for consumer-facing products, Google is leading the way in showing just how powerful AI can be for developers.Let’s dive deeper into why Google has taken this approach. With over 20,000 engineers working on various products, relying on AI to handle repetitive, time-consuming tasks such as code generation allows the company to save countless hours. This, in turn, frees up developers to focus on more complex tasks that require problem-solving skills, creativity, and innovation.## **Will AI Replace Software Developers in 2025?**As AI continues to make its mark on software development, many developers are wondering if their jobs are at risk. Should they be worried? The short answer is **no**but let’s unpack that.AI isn’t here to replace developers. Far from it. AI tools are designed to assist developers, not eliminate them. These tools automate repetitive and mundane tasks, such as writing boilerplate code or generating simple functions. But when it comes to the creative, high-level work of designing software, making complex decisions, and debugging, developers are still very much in the driver’s seat.For example, imagine a scenario where a developer is working on building a web application. AI can generate the basic structure of the code such as setting up routing, creating a simple login form, or connecting to a database but it cannot make decisions about the best UI layout for user experience or develop the logic that handles complex business rules. That’s where the developer’s creativity and problem-solving skills come into play.AI is an assistant, not a replacement. It helps developers write code faster, but it doesn’t have the creativity, problem-solving skills, or domain knowledge that human developers bring to the table. In fact, AI is actually helping developers become more productive, allowing them to focus on more important aspects of the development process.The key takeaway here is that AI is making software development more efficient, but it still needs a human touch for the final product to succeed.### **How Much Code Is AI-Generated on GitHub in 2025?**GitHub is one of the largest code repositories in the world, and it has seen a massive increase in AI-generated code in recent years. That’s not a small chunk—it’s a significant portion of the code that powers everything from web applications to mobile apps.This doesn’t mean that developers are being replaced. Instead, it shows that developers are using AI to streamline their workflows and focus on more complex tasks. **GitHub Copilot** is perhaps the most popular AI tool used for code generation on the platform. It suggests entire lines of code and can even write entire functions based on developer input. It’s designed to help developers write code faster, with fewer bugs, and more efficiently.By integrating Copilot into their workflow, developers save time and avoid getting bogged down by menial tasks, ultimately improving both speed and quality.## **Which Programming Languages and Frameworks Does AI Write Best?**One of the most exciting things about AI in software development is its ability to generate code across a wide range of programming languages and frameworks. Here’s a breakdown of the most popular ones:#### **Python:**Python has become one of the most popular programming languages, especially in fields like data science, machine learning, and web development. AI tools like GitHub Copilot are particularly skilled at generating Python code for these use cases, whether you’re building machine learning models or working with web frameworks like Django and Flask.#### **JavaScript:**JavaScript is the backbone of web development, and AI tools are excellent at generating code for JavaScript frameworks like React, Angular, and Vue.js. AI can generate entire components, hooks, and functions in JavaScript, saving developers a ton of time and effort.#### **Java:**Java remains one of the most widely used programming languages in enterprise software development. AI can help developers generate Java code for popular frameworks like Spring and Hibernate. AI can also generate code for building microservices and APIs, speeding up development time.#### **C and C++:**AI tools are also capable of generating code in low-level programming languages like C and C++, which are crucial for systems programming and embedded systems. AI is particularly helpful in optimizing low-level code for performance and efficiency.#### **Ruby, Swift, PHP, and Go:**AI is also being used to generate code in languages like Ruby, Swift (for iOS development), PHP, and Go. These languages are frequently used for web development, mobile apps, and cloud applications, and AI is helping developers write faster and more efficient code across these platforms.## **What Are the Best AI Code Generation Tools in 2025?**Now, let’s take a look at the tools that are making this all happen. These tools are transforming the way developers work, making coding faster and more efficient:#### **GitHub Copilot:**GitHub Copilot is arguably the most well-known AI tool for developers. Powered by OpenAI’s Codex, Copilot offers suggestions for entire lines or blocks of code as developers work. It supports a wide range of languages and frameworks, including Python, JavaScript, Java, and more. Copilot is designed to help developers write code faster and with fewer errors.Source: #### **Amazon CodeWhisperer:**Amazon’s CodeWhisperer is another powerful AI tool designed to assist developers in generating code for cloud-based applications. It integrates seamlessly with Amazon Web Services (AWS), allowing developers to write code for serverless applications, APIs, and more. CodeWhisperer is especially useful for developers working in the AWS ecosystem.#### **Tabnine:**Tabnine is an AI-powered code completion tool that integrates with popular code editors like VS Code, JetBrains, and more. It offers context-aware code suggestions to help developers write code more quickly and efficiently. Tabnine works with over 40 programming languages, making it a versatile tool for developers.#### **Replit Ghostwriter:**Replit’s Ghostwriter is another AI tool designed for real-time collaborative coding. It provides developers with AI-generated code suggestions and helps speed up the development process. Whether you’re working on a solo project or collaborating with others, Ghostwriter helps you write code faster and more efficiently.**Should Developers Be Worried About AI in Coding?**So, let’s get back to the big question: should developers worry about their jobs in an AI-driven world? The answer is no, but there’s a catch.While AI is indeed transforming the way we write code, it’s not replacing developers. Instead, it’s empowering them to be more productive and efficient. Developers are still needed for creative problem-solving, high-level system design, and debugging. AI tools can generate code, but they can’t design systems, make architectural decisions, or think creatively about solutions. That’s where developers come in.The same idea holds true across industries. AI can build websites or generate campaigns, but real growth still comes from people who understand the field. Veterinary practices, for example, often lean on expert guidance like this , just as tech companies depend on developers for the creativity and insight that AI cannot replace.What’s really changing is the developer’s role. Instead of getting bogged down with repetitive tasks like boilerplate code, they now have the chance to focus on more interesting, high-value work. Those who embrace AI as a tool rather than view it as a threat will find their jobs not just safe but more important than ever.### **How Can Developers Stay Ahead of AI in Coding with EliteBrains?**At , we understand the importance of staying ahead of the curve. As AI continues to transform the software development industry, we’re here to help developers stay competitive. With our **coding tests**, developers can hone their skills, stay updated on new trends, and ensure they’re prepared for the future of software development.Our platform allows developers to:- **Practice with AI-assisted tools** and sharpen their skills- **Test their knowledge** in real-world coding environments- Stay up to date with **emerging trends in AI and software development**- **Learn how to integrate AI** into their development workflow to increase efficiencyAt EliteBrains, we’re committed to helping developers succeed in an AI-powered world, ensuring they remain competitive and continue to thrive.### **Conclusion: AI and Developers—A Winning Partnership**The rise of AI in software development is reshaping the way we write code, but it’s not here to replace developers. Instead, it’s here to make their jobs easier and more efficient. By embracing AI tools like GitHub Copilot, Amazon CodeWhisperer, and Tabnine, developers can work smarter, not harder.AI is an assistant, not a replacement. The key for developers is to adapt and use these tools to their advantage. The future of software development is collaborative—AI working alongside developers to create better, faster, and more innovative software.With **EliteBrains**, developers can stay ahead of the curve, test their skills, and learn how to leverage AI to boost their productivity. The future of coding is bright, and it’s full of exciting opportunities for those who are willing to embrace the change.01. 02. 03. 04. 05. 06. 07. 08. 09. 10. #### Our clients say!With senior resources from EliteBrains, we are able to drive our digital harmonization agenda: Migration of different reporting platforms to QlikSense analytics and Harmonization of different digital detailing systems into Veeva CLM.!Jiri Matousek, Head of CRM at Hexal AG!EliteBrains provided us with a top PHP developer who has been working for us for more than 4 years. Jan became indispensable part of our team within few weeks.!Dave Etchells, CEO at Imaging-resource.com!Each time we are in a need of new software developer, we just contact the guys from EliteBrains. Their network is huge so they provide us with demanded sources very quickly.!Martin Svach, CEO at Starkys club#### Useful articles]( Do Resumes Still Matter for Backend Developers in 2025? Here’s How to Build One That Gets You Hired Faster 02. 09. 2025 . 5 min read]( AI now generates 41% of all code, with 256 billion lines written in 2024 alone. Is your developer job at risk? Discover the latest AI-generated code statistics for 2025 and what they mean for job security, productivity, and the future of software development. 27. 08. 2025 . 11 min readSign up now to see more tipsSubscribecancel anytime#### Hiring?We picked the best developers for you#### Looking for a job?## This website uses cookiesWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Accept allManage preferencesChat Widget\"}]', name='web_search_tool', call_id='call_MZOCIL7HSdei6STfRI8fQssA', is_error=False)]), ToolCallRequestEvent(id='9ea97a10-8912-4948-9290-03ccf6e4a77e', source='research_agent', models_usage=RequestUsage(prompt_tokens=42516, completion_tokens=923), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 33, 23, 666029, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_URZ5D9aGNTS35obDjthzg375', arguments='{\"query\": \"emotion-aware programming education IEEE 2022\"}', name='web_search_tool'), FunctionCall(id='call_0QfXVfV9DNu7IVPtgrFNajy7', arguments='{\"query\": \"affective computing programming software engineering mood-based development 2023 2024\"}', name='web_search_tool'), FunctionCall(id='call_d74ZAGK7DVpc4VJFOMsgev51', arguments='{\"query\": \"AI-assisted coding impact study 2024 2025 Stack Overflow Developer Survey\"}', name='web_search_tool'), FunctionCall(id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', arguments='{\"query\": \"emotion-based coding tools 2024 2025\"}', name='web_search_tool'), FunctionCall(id='call_S9VoFTZt8NefRPNRbUH4tZQg', arguments='{\"query\": \"cursor windsour codeium vibe coding 2025 article\"}', name='web_search_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(id='15da5bed-12dc-4283-a556-535bfd813640', source='research_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 33, 30, 908432, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 7, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_URZ5D9aGNTS35obDjthzg375', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Affective Software Engineering\\', \\'url\\': \\'https://affectivese.org/\\', \\'markdown\\': \"!We are a group of computer scientists whose research focuses on using Affective Computing methods in the field of Software Engineering. Our interests include, but are not limited to, the role of emotions in the software development process, reusing datasets from Affective Computing experiments as well as improving the teaching process of future software engineers.We are open to any kind of cooperation. Do you want to do research together? Prepare a paper? Submit a project proposal? If you are conducting research at the intersection of the fields of affective computing and software engineering feel free to contact us via . Take a look at our current research and projects and get in touch!### DevEmo datasetThe DevEmo dataset consists of 217 video clips showing students\\' facial expressions as they solve programming tasks. The recordings were collected in the participants\\' actual work environment. The video clips are annotated with five labels, including four emotions (anger, confusion, happiness and surprise) and a neutral state.In order to facilitate the annotation of a large number of video recordings, we have developed AffecTube. It is a comprehensive solution that leverages crowdsourcing to annotate videos directly on the YouTube platform, resulting in ready-to-use emotion-annotated datasets. AffecTube provides a low-resource environment with an intuitive interface and customizable options, making it a versatile tool applicable not only to emotion annotation, but also to various video-based behavioral annotation processes.If you are interested in using the DevEmo dataset or AffecTube check out the projects\\' websites:Journal papers:### Affective data reausabilityModern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration.In the research conducted so far, we have proposed the ROAD (Recording Ontology for Affective-related Datasets) ontology and its implementation with the graph knowledge base in the form of the GRISERA framework.If you are interested in our approach take a look at:Journal papers:### Emotions in the software development processEmotions play an important role in any human being\\'s life, including software developers. The aim of our research is to determine what emotions are optimal in the software development process, how to recognize them and how (if at all) to influence them.Publications:### Affect-aware software engineering educationWith the Covid-19, the role of distance learning has increased. The aim of the research is to develop methods that, using emotion recognition techniques, will allow for more effective remote learning of programming.Publications:### Affective loop in Socially Assistive Robotics as an intervention tool for children with autismEMBOA project entitled “Affective loop in Socially Assistive Robotics as an intervention tool for children with autism” is a research and didactic project that aims at enhancement of social robot intervention in children with autism with affective computing technologies. The project is implemented in years 2019-2022 under EU Erasmus Plus Strategic Partnership for Higher Education Programme.More information on project website: Wrobel MR., Landowska A., Makuch K., , Proceedings of the 32nd International Conference on Information Systems Development (ISD 2024), 2024Barkana DE., Bartl-Pokorny KD., Kose H, Landowska A., Milling M., Robins B., Schuller BW., Uluer P., Wrobel MR., Zorcec T., , International Journal of Social Robotics, 2024Kulas D., Wrobel MR., , SoftwareX, 2023Manikowska M., Sadowski D., Sowinski A., Wrobel MR, , Applied Sciences, 2023Zawadzka T, Wierciński T, Waloszek W, Wróbel MR, , IEEE Access, 2023Zawadzka T, Wierciński T, Meller G, Rock M, Zwierzycki R, Wróbel MR, , Sensors, 2021Zawadzka T, Waloszek W, Karpus A, Zapałowska S, Wróbel MR, , IEEE Access , 2021Bartl-Pokorny KD, Pykala M, Uluer P, Barkana DE, Baird A, Kose H, Zorcec T, Robins B, Schuller BW, Landowska A, , IEEE Access , 2021Wróbel MR, , IEEE Access , 2020Blincoe K, Springer O, Wrobel MR, , IEEE Software, 2019Wrobel MR, Zielke AW, , 2018 Federated Conference on Computer Science and Information Systems (FedCSIS), 2018Wrobel MR, , Applied Sciences, 2018Landowska A, Brodny G, , IEEE Access, 2018Landowska A , , Applied Sciences , 2018Wrobel MR, , 2016 Federated Conference on Computer Science and Information Systems (FedCSIS), 2016Brodny G, Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 2016 9th International Conference on Human System Interactions (HSI), 2016Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Information Systems Development and Applications, 2015Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Human-Computer Systems Interaction: Backgrounds and Applications 3, 2014Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013\"}, {\\'title\\': \\'Affective Computing: Recent Advances, Challenges, and Future ...\\', \\'url\\': \\'https://spj.science.org/doi/10.34133/icomputing.0076\\', \\'markdown\\': \\'AdvertisementMain content starts hereContents## AbstractAffective computing is a rapidly growing multidisciplinary field that encompasses computer science, engineering, psychology, neuroscience, and other related disciplines. Although the literature in this field has progressively grown and matured, the lack of a comprehensive bibliometric analysis limits the overall understanding of the theory, technical methods, and applications of affective computing. This review presents a quantitative analysis of 33,448 articles published in the period from 1997 to 2023, identifying challenges, calling attention to 10 technology trends, and outlining a blueprint for future applications. The findings reveal that the emerging forces represented by China and India are transforming the global research landscape in affective computing, injecting transformative power and fostering extensive collaborations, while emphasizing the need for more consensus regarding standard setting and ethical norms. The 5 core research themes identified via cluster analysis not only represent key areas of international interest but also indicate new research frontiers. Important trends in affective computing include the establishment of large-scale datasets, the use of both data and knowledge to drive innovation, fine-grained sentiment classification, and multimodal fusion, among others. Amid rapid iteration and technology upgrades, affective computing has great application prospects in fields such as brain–computer interfaces, empathic human–computer dialogue, assisted decision-making, and virtual reality.#### SIGN UP FOR _Intelligent Computing_ eTOC ALERTS!Get the latest SPJ table of contents delivered right to you!GET OUR E-ALERTS## IntroductionAccording to basic emotion theory, emotion is the grammar of social living and serves as a crucial means of exchanging information, maintaining relationships, and communicating ideas between individuals. Moreover, it is a fundamental psychological element that ensures basic human survival while shaping social habits and supporting advanced thinking , ]. Given its central role in numerous human intellectual activities such as perception, learning, decision-making, reasoning, and socializing, emotion is an important force driving the continuous and diverse prosperity of human civilization.The importance of emotions to human beings can be summarized in 5 crucial aspects. First, the survival function is a learned physiological response that allows individuals to adapt positively to their environment ]. Emotions play a pivotal role in strengthening the capacity to adapt to the environment by regulating attention, memory, perception, and other cognitive processes. This ensures a greater chance of survival and development during the evolutionary process. Second, the communication function highlights the importance of emotions for the accurate expression and understanding of human intentions ]. The same words spoken with different emotions carry different connotations. Thus, emotions are inseparable from natural language and are critical for semantic disambiguation. Third, emotions have a decision-making function that manifests in both fast and slow modes of thinking. The commonly used unconscious “System 1” mainly relies on emotions and experiences, while the conscious “System 2” depends on rational deliberation ]. Therefore, emotions are widely involved in higher-level thinking and decision-making processes that profoundly affect the results and efficiency of decisions. Fourth, emotions serve a motivational function in stimulating and sustaining individuals’ behaviors, thereby affecting the degree of resource input, behavioral persistence, and evaluation of outcomes ]. Finally, emotions perform a maintenance function as bonds between members of ethnic groups, families, social circles, social classes, and other groups. During human socialization, emotions serve as the core of low-cost maintenance of social relations, forming potential social interaction contracts, and are closely tied to individual moral constraints and codes of conduct , ]. Hence, the nature and functions of emotions ensure that they are inseparable from human survival and development.As the era of a human–machine symbiotic society approaches, endowing machines with emotional intelligence becomes increasingly crucial. Emotional intelligence represents a fundamental technology and an essential prerequisite for realizing naturalized and anthropomorphic human–computer interaction. It is of great value for opening up the era of intelligence and digitization. Picard is credited with being the first to propose a comprehensive definition of affective computing. In her 1997 book, _Affective computing_, she defined it as “computing that relates to, arises from, or deliberately influences emotions” ]. The goal of affective computing is to create a computing system capable of perceiving, recognizing, and understanding human emotions and responding intelligently, sensitively, and naturally, thus making human–computer interaction more natural. The epochal importance of affective computing lies in its impact on changing how emotions are perceived as abstractions within psychology, making it possible for emotions to be measured, computed, and machine-learned.Affective computing encompasses various disciplines, including computer science, engineering science, brain and psychological science, and social sciences. Computer science and engineering science focus on providing various information technology tools and engineering capabilities to enable digital reconstruction and computational realization of emotion perception, recognition, understanding, and feedback, allowing machines to possess human-like emotional and cognitive functions. The psychological and consciousness aspects of the brain and psychological sciences provide theories on the basic definition of human emotions and the structure of related elements, laying the foundation for modeling emotion theories. Cognitive neuroscience, another branch of the brain and psychological sciences, examines the emotion-processing mechanism of the human brain and establishes a functional network of psychological elements associated with emotions, providing important inspiration and strategic guidance for developing affective computing models. Social and medical sciences offer numerous opportunities for the application of affective computing and serve as a resource for designing application scenarios for such technologies.### Research in affective computingThe research content of affective computing primarily covers 5 aspects. The first aspect is the fundamental theory of emotion, which currently relies on the discrete emotion model and the dimensional emotion model from the field of psychology to define various types of emotions, ranging from basic to compound. The second aspect involves collecting emotional signals, such as text, speech, facial expressions, gestures, and physiological signals, to establish corresponding datasets. The third aspect is sentiment analysis, which utilizes machine-learning and deep-learning algorithms to model and identify emotional signals. The fourth aspect is multimodal fusion, which leverages multimodal emotional features and fusion algorithms to enhance the accuracy of emotional classification. Finally, the fifth aspect is generating and expressing emotions, processes that enable robots to express emotional states through facial expressions, voice intonation, body movements, etc., and facilitates natural, anthropomorphic, and personified human–robot interaction. Figure  illustrates the specific content and development status of these 5 aspects.![](\\\\xa01. Research content of affective computing.![](\\\\xa02. Annual scientific production on “affective computing” from 1997 January 1 to 2023 September 25.#### Basic theory of emotionThe field of affective psychology has numerous grounded theories of emotion and serves as an important source of inspiration for the development of computable emotion models. The discrete emotion model and the dimensional emotion model are the most commonly used theoretical models for artificial intelligence emotion modeling. The discrete emotion model categorizes emotions individually rather than in correlated groups, as does Ekman’s basic emotion classification model, which is based on facial expression analysis ] and comprises happiness, sadness, anger, disgust, surprise, fear, and contempt. Although the discrete emotion model is clearly defined, interpretable, easy to understand, and capable of semantically integrating vocabulary and concepts, it lacks granularity and provides a limited quantitative description of emotions. In contrast, dimensional affective models represent different emotions through multidimensional vectors in affective space. Such models include the valence–arousal affective model ] and the 3-dimensional pleasure–arousal–dominance model , ]. These models are highly quantitative, abstract, and inductive and have continuous emotional value vectors. They are suitable for handling changes in emotional states over time but are not intuitively interpretable; thus, it is difficult for machines to use them to develop rich coping strategies for emotional interactions. The selection of the model depends on the actual application tasks and scene requirements, as both discrete and dimensional emotion models have advantages and disadvantages.#### Collection of emotional signalsTo support data acquisition and the comparison of algorithms in affective computing, numerous open-source databases have been established. They contain datasets that can be categorized as textual, speech/audio, visual, physiological, or multimodal. The characteristics of these databases considerably impact model design and network architecture in affective computing.Text-based resources on various communication carriers serve as massive datasets for emotional text mining ]. Representative datasets include the internet movie database (IMDb) ], the Stanford sentiment treebank, which contains sentences from movie reviews ], and the Multi-Domain Sentiment Dataset, which contains Amazon.com product reviews ]. Speech is another crucial modality for decoding emotions in human intercommunication. Speech signals comprise both the emotional content of the speech and the emotional characteristics of the sound itself. Representative datasets include EmoDB ], the SEMAINE database ], and CSED ]. Visual-emotional signals such as body movements and facial expressions are now more convenient to gather because of low-cost sensors such as cameras and camcorders, and they do not require direct contact with the user ]. This field has vast amounts of data and many related research papers with considerable data collected directly from real-world scenarios, making it more conducive to grounded applications ]. Representative datasets include the Expression-in-the-Wild (ExpW) dataset ], AffectNet ], the Real-world Affective Faces Database (RAF-DB) ], and SMIC, a database of spontaneous microexpressions ].Physiological data have an advantage over signal data such as text, speech, and facial expressions in that they can more directly, objectively, and accurately reflect an individual’s emotional state while being less influenced by subjective consciousness , ]. Consequently, physiological data have become a research hotspot in affective computing. Commonly used physiological data in this field include electroencephalograms (EEGs), skin electricity, cardiac electricity, electromyography (EMG), eye electricity, respiration, skin temperature, and blood volume pulse. However, obtaining physiological data requires the use of complex sensors. Thus, such data are expensive and challenging to collect for use in practical applications. Consequently, the scale of physiological data used in laboratory research is generally small ]. Representative datasets include the Database for Emotion Analysis using Physiological Signals (DEAP) ], the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) ], and WESAD, a dataset for wearable stress and affect detection ].#### Sentiment analysis**Text analysis.** This method focuses on extracting, analyzing, understanding, and generating emotional information in natural language. Early text affective recognition relied mainly on manually constructed affective dictionaries and rules for affective analysis. These methods judge sentiment polarity by matching sentiment words with grammatical rules in a text , ]. However, this approach is limited by emotional lexicon coverage and rules, making it challenging to support multidomain sentiment analysis. With the advancement of machine learning, text emotion recognition methods based on statistical and machine learning algorithms have emerged. By training on large-scale text datasets, machine learning models can automatically learn emotional expression and semantic features, enhancing the accuracy and generalization ability of sentiment classification , ]. In recent years, deep-learning technology has considerably impacted text emotion recognition. Neural network-based models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, bidirectional encoder representation from transformers (BERT), and generative pre-trained transformers (GPT), have been successful in various sentiment analysis tasks – ]. They can capture contextual information and semantic relationships to better understand and analyze sentiments.**Speech analysis.** Speech emotion recognition is the process by which a computer automatically recognizes the emotional state signaled by speech. Speech contains emotional information, such as speech rate and intonation, in addition to semantic information. Speech emotion analysis combines linguistic and acoustics-related technologies to analyze the syntax, semantics, and acoustic feature information related to the speaker’s emotional state ]. This analysis mainly revolves around rhyme, spectrum, and sound quality features. The numerous acoustic features related to affective states include fundamental frequency, duration, speech rate, resonance peaks, pitch, mel-filter bank (MFB), log-frequency power coefficients (LFPC), linear predictive cepstral coefficients (LPCC), and mel-frequency cepstral coefficients (MFCC) – ]. These features are represented as fixed dimensional feature vectors, with each component representing the statistical value of each acoustic parameter, including the mean, variance, maximum or minimum value, and range of variation. Recently, the ability of neural networks to extract suitable feature parameters has received increasing attention. Deep speech emotion features are learned from speech signals or spectrograms through tasks related to speech emotion recognition. Deep speech features learned from large-scale training data are widely used as speech emotion features in speech event detection and speech emotion recognition tasks, as in the VGGish and wav2vec projects , ], for example. In recent years, algorithms such as ConvNet learning ], ConvNet-RNN ], and adversarial learning ] have considerably improved speech emotion recognition performance.**Visual analysis.** Visual emotion recognition research primarily focuses on facial expression recognition (FER) and emotional body gesture recognition. The conventional method involves feature extraction followed by classification. Typically, handcrafted features for static image analysis include local binary pattern (LBP), histogram of oriented gradients (HOG), local phase quantization (LPQ), and Gabor features , ]. Some scholars have proposed dynamic feature extraction methods, such as LBP on three orthogonal planes (LBP-TOP) ]. Features are usually classified using pattern recognition classification methods such as K-nearest neighbors, support vector machines (SVMs), or multi-layer perceptrons (MLPs). Another approach is the feature learning approach, which combines the end-to-end training of feature representations and classifiers on a given task target, typically a combination of the entire connection layer and softmax. The feature-learning method employs features learned from big data through layer-by-layer feature transformation and can describe the intrinsic information of data better than handcrafted features. However, supervised training methods such as deep CNNs are not universal and rely on large amounts of sample data. Therefore, it is too early to abandon traditional feature-extraction methods. In visual emotion analysis, automatic training features can be extracted and integrated with traditional features, which may further improve system performance.**Physiological signal analysis.** Physiological changes that occur with emotions, including brain electrical activity, heart rate changes, electrical skin response, muscle tension, and respiration rate, are supported by mainstream theories, such as the physiological theory of emotion ] and Lange’s theory of emotion ]. By detecting changes in these physiological signals, patterns associated with emotions can be recognized and then used to develop computer systems that can automatically recognize emotions. Physiological signals are more challenging to recognize than text, speech, and facial expression signals mentioned above, and they have unique properties. For example, computing EEG data requires more complex preprocessing, including electrode position localization, bandpass filtering, reference conversion, segment analysis interception, artifact removal, and bad electrode interpolation. Researchers must have cross-field knowledge to apply machine learning or deep learning methods to recognize emotions from physiological signals ].Affective computing mainly employs peripheral nervous system (PNS) features, such as facial EMG, galvanic skin potential (GSP), photoplethysmography (PPG), heart rate variability (HRV), respiratory rate, and electrocardiogram (ECG), whereas central nervous system (CNS) features include EEG, near-infrared, and brain-imaging features. EEG features have dominated the studies published on this topic. For instance, manual feature extraction involves multidimensional feature extraction from EEG signals in the time, frequency, time–frequency, and nonlinear domains for emotion recognition and classification. Recent studies have emphasized the integrity and relevance of these features. To construct functional brain networks, many studies have started defining a channel as a node and quantifying the relationship between individual nodes using phase synchronization, inter-correlation, and mutual information, treating strength as the functional connectivity between the brain regions of the corresponding channel. Complex network measures, including efficiency, clustering coefficients, degree distribution, small-world features, and average shortest distance, are then used to extract functional brain network features. Since 2018, deep learning methods such as CNNs, RNNs, deep belief networks (DBNs), and stacked autoencoders (SAEs) – ] are being increasingly used for emotional computation of EEG data, generalizing sentiment analysis to various physiological signals.#### Multimodal fusionEarly affective computing primarily involved unimodal data analysis and emotion recognition, focusing on a single modality, such as text, speech, facial expression, body movement, or physiological signals. However, this approach fails to conform to the human perception and expression patterns of emotions and has limitations in terms of the information obtained for emotion recognition ]. Humans communicate their emotions through multiple channels, including language, tone of voice, facial expressions, and body movements. Textual, auditory, and visual information together provide more comprehensive emotional information than they do individually, just as the brain relies on multiple sensory input sources to validate events. Moreover, unimodal information is insufficient and can be easily affected by various external factors ]. Emotional signals can be disguised or affected by other signals from a single channel, for example, when facial expressions are obscured or when noise interferes with speech, resulting in a considerable reduction in emotion analysis performance. Multimodal emotion analysis considers the complementarity of emotion expression among modalities and is thus more robust and aligned with natural human behavior expression. Therefore, research on multimodal fusion of affective computation has received increasing attention. Multimodal fusion algorithms integrate information from different modalities into a stable multimodal representation, enabling comprehensive processing and coordinated optimization to identify human emotions as accurately as possible ]. Common multimodal fusion methods can be categorized into feature-, model-, and decision-layer-based fusion depending on the fusion stage ].#### Generation and expression of emotionsAffective computing enables machines to provide empathic feedback based on deep contextual understanding. Robots and other agents can deliver expressions and responses, conveying the emotional temperature to the user through facial expressions, emotional text responses, and body movements , ] by building on the results of sentiment analysis and recognition. Emotional text generation and speech synthesis are the most-studied areas of research. Emotional text generation involves the automatic generation of emotional response content that matches the message of the dialogue and is consistent with the machine’s strategy, which is chosen according to the context ]. For instance, a traffic enforcement robot may exhibit a fundamental difference in the language used for persuasion and the language used for enforcement, a difference that is crucial to obtaining effective practical traffic management results. The goal of emotional text generation is for the model to generate text that conforms to a specified sentiment category, as expressed by emotion-related keywords or techniques such as metaphors ]. Pretrained models such as GPTs are increasingly being utilized as a base for emotionally controllable text generation and achieving powerful results ]. Responding to text content with emotional color is only the first step. The generated text needs to be expressed using a related emotional voice. Emotional coding information is integrated into the speech synthesis model to make human–machine dialogue less cold and mechanical, thereby allowing individuals to perceive “machine empathy” and feel warmth and affinity. Emotional speech synthesis uses a specific voice style and combines text content with emotional tags to provide a robot or agent with a voice that expresses a particular emotion ]. This process inputs textual content and a specific voice style into a neural network that synthesizes an output voice in that style by utilizing the spectral, rhythmic, and linguistic features of human voices that express emotion.### Applications of affective computingAffective computing is a technology that advances according to the actual needs of the industry, which drives progress and iteration. To build up reliability, general applications initially focused on recreation, leisure, or serving people with urgent needs, then gradually expanded to more fields, transforming the technology and contributing to productive endeavors. In 2021, the value of affective computing reached $21.6 billion, and it is expected to double by 2024 , ]. As the industry grows, the creative applications of affective computing technologies will flourish, yielding satisfactory results in various fields.#### EducationIn the field of education, affective computing is primarily used to recognize the emotional state of learners and provide corresponding feedback and adjustment ]. For example, teachers can utilize intelligent emotional teaching systems to better understand students’ engagement levels and adjust the pace and content of their teaching to improve the learning experience. An intelligent system can recommend customized learning content based on the sentiment analysis of students’ interests. Students can provide authentic teaching feedback through intelligent systems to improve the comprehensiveness and accuracy of teaching evaluations. One advantage of an intelligent system is that it can be used in both traditional and online classrooms to strengthen the contextualization of online teaching, enhance emotional interaction between teachers and students, and improve teaching quality. Affective computing techniques are also conducive to the research and development of educational games and robots ], providing improved human–computer interaction and achieving educational objectives more effectively.#### HealthcareAffective computing research has expanded into various psychiatric disorders in the affective disorders category, such as Alzheimer’s ], Parkinson’s ], bipolar disorder ], and post-traumatic stress ], and into healthcare areas including relaxation service healthcare ] and health office systems ]. Affective computing enables the scientific and objective identification and judgment of patients’ emotions, particularly in psychological disorder treatments, providing a useful complement to more subjective traditional diagnostic tools such as behavioral observation and scale filling. Objective data collection can improve personalized and precise medical treatment ]. In addition, affective computing can be used for the initial screening and efficacy assessment of diseases. For instance, patients with social anxiety disorder exhibit important differences in emotional facial processing compared to the normal population, differences that can be identified by automated monitoring of differential features ].#### Business servicesIn marketing, where the consumer experience is highly correlated with emotions, affective computing is widely used to understand and recognize the user’s emotional state. The application of affective computing can reveal the user’s true preferences and improve and streamline the buying process ]. In the field of financial credit, affective computing technologies can be used to analyze the emotional state and moral level of a customer based on voice and tone, determine the probability of the customer lying, and provide a guide for lending decisions. In the field of stock investment, investor decisions are influenced by irrational judgments. The price trend of a stock is determined not only by a company’s fundamentals but also to a large extent by fluctuations in investor emotions. The study of investor sentiment from social media data (e.g., data from X, formerly known as Twitter) can help identify investors’ emotional preferences and cognitive biases for the purpose of predicting the direction of the stock market ].#### Integration of science and artIn the current digital era, image, audio, and video data have become plentiful and important. Extracting useful information from them and retrieving and mining them effectively are crucial. For example, in recommending music to users, resource management and audio search efficiency are essential. Traditional music search methods match content using text (e.g., song title, artist name, or lyrics). Including sentiment, a high-level semantic feature of music, improves the match between user preferences and music, thus aiding in the primary task in music sentiment analysis ]. Affective computing also empowers automated poetry generation, where deep learning methods such as RNNPG, an RNN-based poem generator, and SeqGAN, a sequence generative adversarial network, are gradually replacing Word Salada, genetic algorithms, and statistical machine translation methods – ]. Expressing emotions more richly is key in making generated poetry spiritual, i.e., in moving beyond resemblance of form to resemblance in spirit.### Importance of this studyThe field of affective computing has grown considerably and exploded in popularity in the last decade for 2 reasons: technological developments providing tools for affective computing and the growth and expansion of demand. In the era of human–machine symbiosis, the deepened human understanding of emotional connotation and the improvement of the “double quotient” (i.e., IQ + EQ) of intelligent machines will become a vital innovative force promoting the affective computing discipline, technological evolution, and industrial progress. Despite the rapid development in affective computing, a comprehensive review of research and systematic analysis of hotspots and trends is lacking. Continuous innovation in algorithmic technology, broadening application requirements, and increasing research efforts necessitate that existing research be summarized and future technological directions be identified. Doing so will enable academia and industry to better understand the development of affective computing technology, thus will facilitate affective computing research, empower applications, and benefit society.This study aims to fill the gaps in existing research through a comprehensive review of affective computing from 1997, when Picard formally proposed the concept, up to 2023. We adopted a bibliometric analysis method to accurately portray the current status of the development of the field and provide insights into present challenges and future trends. The main contributions of this study are as follows. (a) Facing the academic frontier, we list the research hotspots and trends that we identified by analyzing full-scale papers. This allows readers to quickly and comprehensively grasp the development dynamics of the field and understand key common and frontier-leading technologies. (b) Facing major needs and the main battlefield of the economy, we provide blueprints for technological development and insights into current applications. This promotes the application and transformation of affective computing, facilitating high-quality economic development and digital transformation. (c) Facing future trends, we introduce challenges and developments in the field of affective computing, along with predictions for future technology and industry application directions. This serves as a forward-looking guide to the field.## Materials and Methods### Data collectionThis study searched for papers published in affective computing from January 1997 to September 2023 in the Web of Science Core Collection (WoSCC), which includes the Science Citation Index Expanded, Social Sciences Citation Index, Arts & Humanities Citation Index, Emerging Sources Citation Index, Conference Proceedings Citation Index—Science (CPCI-S), and Conference Proceedings Citation Index—Social Sciences & Humanities (CPCI-SSH). The search strategy is summarized in Table .| Index field | Search strategy || --- | --- || Theme keywords | “affective recognition” or “mood recognition” or “affective computing” or “artificial emotional intelligence” or “emotion AI” or “expression recognition” or “emotion recognition” or “emotion learning” or “sentiment analysis” or “sentiment recognition” || Literature types | proceedings papers, articles, review articles, early access |Table\\\\xa01. Search strategy for this studyThe reason this study uses 1997 as the starting point of the timeline is that the book _Affective computing_ ], which was published in that year, is regarded as the work that established affective computing as an independent academic research field. Papers outside this time range were not included in the calculation of citation statistics. In the statistics of Chinese papers, Hong Kong, Macau, and Taiwan are included. The results show that 33,448 papers were published worldwide. Among them, 16,097 (48.13%) were conference papers and 17,351 (51.87%) were journal papers. It should be noted that the names of institutions were standardized using machine and manual methods. However, when scientists publish papers, the writing of the names of institutions is not standardized, which may have caused the omission of papers in the statistics and a deviation in the index calculation results.In addition, this study combined the following 3 databases for data acquisition: (a) Incites: This database is based on the publication date of all document types in the major index databases of the WoSCC. It performs publication count and index calculations to provide research performance analysis. (b) Essential Science Indicators (ESI): This is an in-depth analytical research tool based on the Web of Science. ESI can identify influential countries, institutions, papers, and publications, as well as the cutting-edge in a research field. (c) Journal Citation Reports (JCRs): This is a multidisciplinary journal evaluation tool that provides journal evaluation resources based on citation data statistics. By citing and counting references, the JCR can measure the influence of research at the journal level, revealing the relationships between citing and cited journals.### Data analysisStatistical analysis was performed using a bibliometric method. Bibliometrics applies quantitative methods such as mathematics and statistics to the literature of a scientific or other field and processes statistical data based on information science theory. This widely accepted approach provides quantitative analysis pathways and innovative insights into the assessment of research trends based on previous literature , ]. Unlike peer review and expert judgment, bibliometrics can provide quantitative indicators to ensure objectivity through statistical analysis of academic achievements ]. Bibliometric analysis enables monitoring and summarizes the status, hotspots, and trends of a particular topic, helping researchers identify future research directions ]. In this study, we first cleaned and analyzed the data using the Derwent Data Analyzer (DDA, version 10, Clarivate, London, UK), which is well integrated with the source data from the Web of Science platform. DDA was used for multidimensional data mining, preprocessing, standardization, and statistical analysis. Subsequently, the bibliometric analysis and knowledge visualization software tool VOSviewer (version 1.6.15, Leiden University, Leiden, Netherlands) was employed. This analysis tool provides valuable insights into the structure, advancement, and collaboration in the field of affective computing. Notably, its distinctive feature lies in the graphical representation of bibliometric maps, which is particularly suitable for large-scale data analysis ]. VOSviewer was used to visualize the data in this study.## Results### Publication trendsFrom 1997 to 2009, the number of articles published in this field steadily increased, exhibiting an overall growth trend despite occasional fluctuations (Fig.\\\\xa02). From 2010 to 2019, with the rise of deep learning, a rapid development was observed in the field of affective computing, and the number of articles published in the field rose rapidly, indicating an explosive growth stage of research. After 2019, because of a plateau in the innovation of deep learning methods and the impact of the coronavirus disease 2019 (COVID-19) pandemic on academia, research in the field of affective computing also reached a plateau, and the rising trend slowed down.### Comparison of countriesTo analyze the main research positions in the field of affective computing, the country/region fields of all the authors and the first author of the paper were counted. As shown in Table , among the top 20 countries with publications in the field of affective computing, China is the country with the largest number of publications, accounting for 26.2% of all authors and 24.6% of first authors. China, the United States, India, the United Kingdom, and Germany rank among the top 5 in the number of papers published counting all authors or first author, and are the most important in terms of research in the field of affective computing. The United States ranks second in the number of papers published counting all authors, but third in the number of papers published counting only first author, after India.| No. | Country | Number of papers | Country | Number of papers || --- | --- | --- | --- | --- || (All authors) | (All authors) | (First author) | (First author) || --- | --- | --- | --- || 1 | China | 8,780 | China | 8,223 || 2 | USA | 4,715 | India | 3,632 || 3 | India | 3,829 | USA | 3,274 || 4 | UK | 2,535 | UK | 1,432 || 5 | Germany | 1,706 | Germany | 1,253 || 6 | Japan | 1,321 | Italy | 1,022 || 7 | Italy | 1,302 | Japan | 977 || 8 | Australia | 1,234 | South Korea | 931 || 9 | Spain | 1,178 | Spain | 862 || 10 | South Korea | 1,121 | Australia | 788 || 11 | Canada | 1,100 | Canada | 720 || 12 | France | 943 | France | 587 || 13 | Netherlands | 778 | Turkey | 581 || 14 | Saudi Arabia | 765 | Netherlands | 484 || 15 | Turkey | 691 | Malaysia | 479 || 16 | Singapore | 640 | Pakistan | 454 || 17 | Malaysia | 609 | Brazil | 443 || 18 | Pakistan | 595 | Greece | 413 || 19 | Brazil | 522 | Iran | 398 || 20 | Greece | 483 | Singapore | 394 |Table\\\\xa02. The top 20 countries in the field of affective computingIn addition to the 2-year step in 2021–2022, a 4-year step was used to count the publication volume of the top 10 countries in the field of affective computing. The results are shown in Fig. . Given that the concept of “affective computing” originated in the United States, which has been a major research force in this field, we chose the United States as the benchmark. During the entire period, the relative volume of publications by China and the United States changed considerably, as shown in Fig. . From 1997 to 2004, the number of papers published by the United States far exceeded that of China. From 1997 to 2000, the total number of papers published by China was 20% of that of the United States. From 2001 to 2004, the total number of papers published by China rose to 31% of that of the United States. In the period from 2005 to 2008, the number of papers published by China surpassed that of the United States, and the number of papers published by China in 2021–2022 is about 3 times that of the United States. It can be seen that in recent years, China’s research in the field of affective computing has accumulated rapidly, and its large volume of research has certain advantages compared with that of the United States. In addition, in 2021–2022, the number of papers published by India surpassed that of the United States for the first time. India has gradually become a major research center in the field of affective computing because of its advantages in computer science, engineering, and other disciplines.![](\\\\xa03. Comparison between the top 10 countries and the United States in the number of publications.### Main journalsThis section analyzes basic data on journal papers. The 17,351 published papers were distributed in 1,300 journals, among which _IEEE Access_ [impact factor (IF) 3.9] had the most (875), as shown in Table . Across all journals, 1,209 had an IF listed in the 2022 JCRs. The distribution of the IFs of the 1,209 journals is shown in Table . Among them, 54 journals have IFs greater than 10, and the 5 journals with the highest IFs are _World Psychiatry_ (73.3), _Lancet Psychiatry_ (64.3), _Nature Reviews Neuroscience_ (34.7), _Nature Human Behaviour_ (29.2), and _JAMA Psychiatry_ (25.8). The IFs of most journals are distributed in the 2 intervals of 2 ≤ IF < 4 and 4 ≤ IF < 7. It is worth noting that _IEEE Transactions on Affective Computing_ (IF 11.2) is a high-level journal focusing on the field of affective computing. It is a cross-disciplinary and international archive journal aimed at disseminating the results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. In addition, _Expert Systems with Applications_, _Knowledge-Based Systems_, _Information Processing & Management_, _IEEE Transactions on Multimedia_, _Neurocomputing_, _Information Sciences_, _Pattern Recognition_, _Applied Soft Computing_, _Decision Support Systems_, and _Future Generation Computer Systems_ are also high-level journals favored by scholars in the field of affective computing.| No. | Journal | Number of papers || --- | --- | --- || 1 | _IEEE Access_ | 875 || 2 | _Multimedia Tools and Applications_ | 474 || 3 | _IEEE Transactions on Affective Computing_ | 419 || 4 | _Sensors_ | 378 || 5 | _Frontiers in Psychology_ | 362 || 6 | _Applied Sciences-Basel_ | 349 || 7 | _Expert Systems with Applications_ | 290 || 8 | _International Journal of Advanced Computer Science and Applications_ | 272 || 9 | _Neurocomputing_ | 248 || 10 | _Knowledge-Based Systems_ | 226 || 11 | _Psychiatry Research_ | 191 || 12 | _Electronics_ | 167 || 13 | _Journal of Intelligent & Fuzzy Systems_ | 151 || 14 | _Neural Computing & Applications_ | 144 || 15 | _Neuropsychologia_ | 137 || 16 | _Schizophrenia Research_ | 135 || 17 | _Information Processing & Management_ | 132 || 18 | _Computational Intelligence and Neuroscience_ | 114 || 19 | _Cognitive Computation_ | 112 || 20 | _Information Sciences_ | 110 |Table\\\\xa03. Top 20 journals with the largest number of articles in the field of affective computing| Journal impact factor | Number of journals || --- | --- || IF ≥ 10 | 54 || 7 ≤ IF < 10 | 74 || 4 ≤ IF < 7 | 255 || 2 ≤ IF < 4 | 406 || 1 ≤ IF < 2 | 245 || IF ≤ 1 | 175 |Table\\\\xa04. Journal impact factor distribution### High-level international conferencesCombining ESI’s highly cited and hot papers with the “China Computer Federation Recommended International Academic Conferences” and CORE Computer Science Conference Rankings, we identified the high-level international conferences related to affective computing. These include the ACM International Conference on Multimedia (ACM MM), AAAI Conference on Artificial Intelligence (AAAI), Annual Meeting of the Association for Computational Linguistics (ACL), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), and the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).### Discipline distributionThis section analyzes the distribution of research fields based on statistics on the Web of Science categories of papers in the field of affective computing. Studies related to the topic of affective computing involve computer science, communication, engineering, psychology, medicine, and other disciplines, reflecting distinct interdisciplinary characteristics. The top 20 categories with the largest number of publications are listed in Table . The category with the largest proportion is “Computer Science, Artificial Intelligence,” with 12,678 publications (37.93% of the total), followed by “Engineering, Electrical & Electronic,” with 9,820 publications (29.36% of the total).| Web of Science category | Number of papers | Percentage (%) || --- | --- | --- || Computer Science, Artificial Intelligence | 12,687 | 37.90 || Engineering, Electrical & Electronic | 9,820 | 29.36 || Computer Science, Information Systems | 8,714 | 26.05 || Computer Science, Theory & Methods | 8,405 | 25.13 || Computer Science, Interdisciplinary Applications | 3,930 | 11.75 || Telecommunications | 3,133 | 9.37 || Computer Science, Software Engineering | 2,982 | 8.92 || Neurosciences | 2,376 | 7.10 || Psychiatry | 2,100 | 6.28 || Computer Science, Cybernetics | 1,904 | 5.69 || Imaging Science & Photographic Technology | 1,077 | 3.22 || Engineering, Multidisciplinary | 1,045 | 3.12 || Automation & Control Systems | 997 | 2.98 || Computer Science, Hardware & Architecture | 981 | 2.93 || Psychology, Multidisciplinary | 884 | 2.64 || Robotics | 793 | 2.37 || Engineering, Biomedical | 735 | 2.20 || Acoustics | 724 | 2.16 || Linguistics | 637 | 1.90 || Clinical Neurology | 610 | 1.82 |Table\\\\xa05. Top 20 categories with the most papers in the field of affective computing### Technology transfer and conversionThis study searched the Derwent Innovation Index, the world’s most comprehensive database of value-added patent information. Among effective invention patents with transfer records and high value, the transferred patents with an IncoPat patent value of 10 (the highest level) include “Cognitive content display device” (US10902058B2, transferred from IBM to Kyndryl Inc.) and “Signal processing approach to sentiment analysis for entities in documents” (US9436674B2, transferred from Attivio Inc. to Servicenow Inc.). However, the number of patent transfer records related to affective computing is small, indicating that technology transfer activity needs to be improved.### Global distribution of scholarsThis section presents statistical analysis of publications based on the country of the first author to provide a macroscopic understanding of the global distribution of scholars in the field of affective computing. As shown in Table , China has the largest number (4,240), followed by India (2,391) and the United States (2,390). In Fig. , darker shading indicates a larger number of scholars. It can be seen that Asia and North America are the regions with the most concentrated distribution of scholars in the field of affective computing.| No. | Country | Number of scholars | No. | Country | Number of scholars || --- | --- | --- | --- | --- | --- || 1 | China | 4,240 | 11 | Canada | 533 || 2 | India | 2,391 | 12 | France | 425 || 3 | USA | 2,390 | 13 | Turkey | 403 || 4 | UK | 999 | 14 | Netherlands | 349 || 5 | Germany | 825 | 15 | Malaysia | 331 || 6 | Italy | 690 | 16 | Pakistan | 324 || 7 | Japan | 631 | 17 | Brazil | 366 || 8 | South Korea | 514 | 18 | Greece | 248 || 9 | Spain | 545 | 19 | Iran | 270 || 10 | Australia | 496 | 20 | Singapore | 229 |Table\\\\xa06. Number of first authors in the field of affective computing (top 20 countries)![](\\\\xa04. Global distribution of scholars in the field of affective computing.### International collaborationThere is a wide range of international cooperation in the field of affective computing. A count of collaborations between the top 20 countries is shown in Table . The number of articles published by China and the United States is the largest (641), followed by China and the United Kingdom (343). Although cooperation between China and the United States has been challenging in recent years, in the field of affective computing, they remain each other’s largest partners, maintaining a vital and continuous cooperation.|  | C1 | U1 | I1 | U2 | G1 | J | I2 | A | S1 | S2 | C2 | F | N | S3 | T | S4 | M | P | B | G2 || --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- || C1 | / | 641 | 79 | 343 | 79 | 256 | 59 | 218 | 43 | 61 | 137 | 44 | 28 | 61 | 12 | 161 | 38 | 57 | 6 | 4 || U1 | 641 | / | 128 | 232 | 174 | 42 | 120 | 122 | 62 | 83 | 153 | 100 | 105 | 55 | 42 | 71 | 10 | 36 | 53 | 23 || I1 | 79 | 128 | / | 73 | 15 | 15 | 22 | 37 | 19 | 33 | 26 | 26 | 8 | 67 | 10 | 48 | 26 | 11 | 3 | 2 || U2 | 343 | 232 | 73 | / | 294 | 40 | 132 | 119 | 96 | 15 | 60 | 89 | 160 | 77 | 25 | 64 | 30 | 43 | 30 | 48 || G1 | 79 | 174 | 15 | 294 | / | 41 | 69 | 47 | 39 | 11 | 43 | 59 | 99 | 4 | 17 | 17 | 5 | 7 | 13 | 19 || J | 256 | 42 | 15 | 40 | 41 | / | 4 | 20 | 17 | 7 | 27 | 15 | 11 | 6 | 5 | 18 | 16 | 1 | 3 | 1 || I2 | 59 | 120 | 22 | 132 | 69 | 4 | / | 22 | 58 | 13 | 25 | 65 | 53 | 10 | 13 | 37 | 4 | 10 | 6 | 7 || A | 218 | 122 | 37 | 119 | 47 | 20 | 22 | / | 26 | 14 | 30 | 22 | 24 | 27 | 14 | 35 | 25 | 23 | 11 | 3 || S1 | 43 | 62 | 19 | 96 | 39 | 17 | 58 | 26 | / | 15 | 15 | 42 | 46 | 23 | 12 | 11 | 5 | 10 | 24 | 18 || S2 | 61 | 83 | 33 | 15 | 11 | 7 | 13 | 14 | 15 | / | 10 | 16 | 6 | 29 |  | 2 | 9 | 71 | 3 | 3 || C2 | 137 | 153 | 26 | 60 | 43 | 27 | 25 | 30 | 15 | 10 | / | 45 | 19 | 40 | 10 | 9 | 2 | 12 | 17 | 3 || F | 44 | 100 | 26 | 89 | 59 | 15 | 65 | 22 | 42 | 16 | 45 | / | 39 | 11 | 4 | 2 | 8 | 16 | 18 | 10 || N | 28 | 105 | 8 | 160 | 99 | 11 | 53 | 24 | 46 | 6 | 19 | 39 | / |  | 19 | 8 | 3 | 1 | 11 | 14 || S3 | 61 | 55 | 67 | 77 | 4 | 6 | 10 | 27 | 23 | 29 | 40 | 11 |  | / | 8 | 4 | 41 | 120 | 1 | 3 || T | 12 | 42 | 10 | 25 | 17 | 5 | 13 | 14 | 12 |  | 10 | 4 | 19 | 8 | / | 5 | 7 | 3 | 2 | 1 || S4 | 161 | 71 | 48 | 64 | 17 | 18 | 37 | 35 | 11 | 2 | 9 | 2 | 8 | 4 | 5 | / | 4 | 2 |  | 1 || M | 38 | 10 | 26 | 30 | 5 | 16 | 4 | 25 | 5 | 9 | 2 | 8 | 3 | 41 | 7 | 4 | / | 40 |  | 3 || P | 57 | 36 | 11 | 43 | 7 | 1 | 10 | 23 | 10 | 71 | 12 | 16 | 1 | 120 | 3 | 2 | 40 | / | 3 |  || B | 6 | 53 | 3 | 30 | 13 | 3 | 6 | 11 | 24 | 3 | 17 | 18 | 11 | 1 | 2 |  |  | 3 | / |  || G2 | 4 | 23 | 2 | 48 | 19 | 1 | 7 | 3 | 18 | 3 | 3 | 10 | 14 | 3 | 1 | 1 | 3 |  |  | / |Table\\\\xa07. Collaborations between the top 20 countries in the field of affective computingNote: C1, China; U1, USA; I1, India; U2, UK; G1, Germany; J, Japan; I2, Italy; A, Australia; S1, Spain; C2, Canada; S2, South Korea; F, France; N, Netherlands; T, Turkey; S3, Saudi Arabia; S4, Singapore; M, Malaysia; P, Pakistan; B, Brazil; G2, Greece.### Important research institutionsThe top 10 institutions in the world by number of publications (counting all authors) are listed in Table . This study used indicators such as Citation Impact, Category Normalized Citation Impact (CNCI), and Highly Cited Papers to further evaluate the influence of various institutions in the field of affective computing. Among them, CNCI is a valuable and unbiased impact indicator that excludes the influence of publication year, subject field, and document type. A CNCI value of 1 indicates that the cited performance of a group of papers is equivalent to the global average level, a value greater than 1 indicates higher performance, and a value less than 1 indicates lower performance; a value of 2 indicates performance twice as high as the global average. The top 5 institutions according to CNCI rankings were Nanyang Technological University (5.06), Imperial College London (3.58), Tsinghua University (3.23), the Chinese Academy of Sciences (3.15), and the University of California System (2.77).| No. | Institution | Number of papers | Citation impact | Category Normalized Citation Impact | H-index | Percentage in Q1 journals | Country || --- | --- | --- | --- | --- | --- | --- | --- || 1 | Chinese Academy of Sciences | 699 | 20.97 | 3.15 | 60 | 59.87 | China || 2 | University of London | 443 | 50.26 | 2.29 | 77 | 69.45 | UK || 3 | UDICE-French Research Universities | 388 | 18.86 | 1.37 | 42 | 50.43 | France || 4 | Centre National de la Recherche Scientifique (CNRS) | 377 | 19.33 | 1.36 | 42 | 51.56 | France || 5 | University of California System | 371 | 40.83 | 2.77 | 64 | 58.72 | USA || 6 | National Institute of Technology (NIT System) | 364 | 9.68 | 1.46 | 29 | 26.43 | India || 7 | Indian Institute of Technology System (IIT System) | 360 | 13.51 | 1.99 | 36 | 44.7 | India || 8 | Nanyang Technological University | 350 | 46.35 | 5.06 | 69 | 68.99 | Singapore || 9 | Tsinghua University | 302 | 24.87 | 3.23 | 44 | 62.93 | China || 10 | Imperial College London | 300 | 41.00 | 3.58 | 49 | 70.75 | UK |Table\\\\xa08. Institutions with a top 10 publication in affective computingNotes: 1. Citation impact: The citation impact of a set of documents is calculated by dividing the total number of citations of the set of documents by the number of documents. Citation impact shows the average number of citations received by a document in the group. 2. Category Normalized Citation Impact (CNCI): The CNCI of a document is obtained by dividing the actual number of citations by the expected number of citations of documents of the same type, publication year, and subject. When a document is classified into multiple subject areas, the average value of the ratio of actual citations to expected citations is used. The CNCI of a country is the average of the CNCIs of the publications of that country.### Citation network analysisThis section analyzes the direct citations of all authors in the field of affective computing. To highlight the key authors, 40 authors who had published no fewer than 30 papers were selected for analysis. The results are shown in Fig. . Authors in clusters of the same color have strong correlations and inheritance in research content. Representative scholars from the 5 clusters are listed in Table .![](\\\\xa05. Citation network of scholars.| Scholar | Organization | Research fields || --- | --- | --- || Baoliang Lu | Shanghai Jiaotong University, China | Brain-like computing, neural networks, deep learning, emotion AI, affective brain–computer interface || Bjoern Schuller | Imperial College London, UK | Machine intelligence, signal processing, affective computing, digital health, speech recognition || Erik Cambria | Nanyang Technological University, Singapore | Affective computing, sentiment analysis, commonsense reasoning, natural language understanding || Fuji Ren | The University of Tokushima, Japan the University of Electronic Science and Technology of China, China | Natural language processing, artificial intelligence, affective computing, and emotional robots || Wenming Zheng | Southeast University, China | Multimodal affective computing, neural computation, pattern recognition, machine learning, and computer vision |Table\\\\xa09. Representative scholars in the citation network### Word frequency analysisWord frequency refers to the number of times a word occurs in the document being analyzed. In scientometric research, word frequency dictionaries can be established for specific subject areas to quantify the analysis of scientists’ creative activities. Word frequency analysis is the method of extracting keywords or subject words that express the core content of the articles in the literature, to study the development trends and research hotspots of the field through the frequency distribution of these words. The results of conducting frequency and co-occurrence analysis on keywords assigned to papers by authors in the field of affective computing are shown in Table .| No. | Number of occurrences | Technical keyword | Number of co-occurrences with other keywords | Time period | Proportion of occurrences within last 3 years (%) || --- | --- | --- | --- | --- | --- || 1 | 7,621 | Sentiment analysis | Machine learning [958]; Opinion mining [936]; Natural language processing [829] | 2006–2023 | 21 || 2 | 4,566 | Emotion recognition | Feature extraction [422]; Affective computing [397]; Deep learning [372] | 1997–2023 | 24 || 3 | 2,457 | Affective computing | Emotion recognition [397]; Machine learning [191]; Emotion [137] | 2000–2023 | 15 || 4 | 2,232 | Deep learning | Sentiment analysis [691]; Emotion recognition [372]; Machine learning [268] | 2012–2023 | 40 || 5 | 2,054 | Machine learning | Sentiment analysis [958]; Natural language processing [275]; Deep learning [268] | 2002–2023 | 27 || 6 | 1,816 | Facial expression recognition | Deep learning [182]; Feature extraction [150]; Face recognition [109] | 1997–2023 | 18 || 7 | 1,348 | Natural language processing | Sentiment analysis [829]; Machine learning [275]; Deep learning [209] | 2006–2023 | 30% of 1,348 || 8 | 1,214 | Feature extraction | Emotion recognition [422]; Sentiment analysis [213]; Task analysis [181] | 2003–2023 | 32 || 9 | 1,209 | Opinion mining | Sentiment analysis [936]; Natural language processing [159]; Machine learning [151] | 2006–2023 | 11 || 10 | 1,067 | Emotion | Affective computing [137]; Emotion recognition ]; Facial expression ] | 1999–2023 | 13 || 11 | 1,007 | Twitter | Sentiment analysis [770]; Machine learning [160]; Social media [145] | 2011–2023 | 18 || 12 | 975 | Speech emotion recognition | Deep learning ]; Feature extraction ]; Emotion recognition ] | 2006–2023 | 29 || 13 | 852 | Social media | Sentiment analysis [587]; Twitter [145]; Machine learning [105] | 2009–2023 | 21 || 14 | 732 | Social cognition | Schizophrenia [193]; Emotion recognition [184]; Theory of mind [179] | 2001–2023 | 16 || 15 | 657 | Text mining | Sentiment analysis [486]; Natural language processing ]; Opinion mining ] | 2006–2023 | 15 || 16 | 635 | EEG | Emotion recognition [357]; Affective computing ]; Emotion ] | 2004–2023 | 27 || 17 | 620 | Classification | Sentiment analysis [208]; Machine learning ]; Emotion recognition ] | 2003–2023 | 19 || 18 | 618 | Facial expression | Emotion recognition [175]; Emotion ]; Affective computing ] | 1998–2023 | 15 || 19 | 582 | Convolutional neural network | Deep learning [146]; Facial expression recognition ]; Emotion recognition ] | 2003–2023 | 30 || 20 | 535 | Schizophrenia | Social cognition [193]; Emotion recognition ]; Theory of mind ] | 1998–2023 | 8 || 21 | 478 | Support vector machine | Sentiment analysis [123]; Facial expression recognition ]; Emotion recognition ] | 2002–2023 | 9 || 22 | 470 | Feature selection | Sentiment analysis [119]; Emotion recognition ]; Feature extraction ] | 2001–2023 | 16 || 23 | 423 | Face recognition | Feature extraction [155]; Emotion recognition [124]; Facial expression recognition [109] | 1997–2023 | 29 || 24 | 422 | Transfer learning | Emotion recognition ]; Deep learning ]; Sentiment analysis ] | 2009–2023 | 40 || 25 | 404 | Data mining | Sentiment analysis [251]; Feature extraction ]; Machine learning ] | 2006–2023 | 22 |Table\\\\xa010. Frequency analysis of top 25 keywords in affective computingThe Thomson Data Analyzer was used to automatically and manually clean the keywords assigned by the authors of papers in the dataset. Subsequently, VOSviewer was used to cluster the core (high-frequency) subject words and set a certain co-occurrence frequency and co-occurrence intensity according to the size of the dataset to cluster the keywords. Combined with expert interpretation, each cluster was named and interpreted, and the topics of the journal articles were identified and analyzed. After keyword cleaning, 613 keywords appearing more than 20 times were selected as analysis objects for cluster calculation. Five clusters were obtained by clustering the core subject words with the highest co-occurrence intensity, as shown in Table  and Fig. .| No. | Research theme | Number of core subject words | Average number of citations | Average correlation strength || --- | --- | --- | --- | --- || 1 | Natural language processing techniques used for affective computing and opinion mining | 153 | 10.41 | 197.80 || 2 | Facial expression and micro-expression recognition and analysis | 134 | 15.89 | 178.77 || 3 | Affective computing studies in human–computer interaction | 121 | 18.69 | 110.38 || 4 | Applied research of affective computing in affective disorder analysis | 30 | 33.5 | 165.59 || 5 | Multimodal sentiment analysis based on deep learning | 81 | 9.8 | 260.95 |Table\\\\xa011. Five research themes in affective computing![](\\\\xa06. Five research themes in affective computing.The average number of citations of a research theme is the average number of times that a paper containing these subject words has been cited since publication, and the average correlation strength of a research theme indicates the closeness of the connection between the core subject words contained in this theme concept. The greater the correlation strength, the greater the co-occurrence intensity between the core subject words and the more concentrated the research. In contrast, relatively lower correlation is associated with more scattered research. Research on the application of affective computing in the analysis of affective disorders has the highest average citation frequency, which shows that interdisciplinary research involving affective computing and medicine, especially research on affective disorders and depression recognition, has a greater influence. The average correlation strength of multimodal sentiment analysis based on deep learning is the largest, which shows that the research on this topic is the most concentrated.## DiscussionThis paper presents a comprehensive analysis and review of systematically collected data on papers and major intellectual property rights in the field of affective computing. The results reveal that over the past 25 years, affective computing has experienced rapid growth in the number of published papers, representing a vibrant academic ecology and an interdisciplinary character with a wide range of disciplines. Additionally, scholars worldwide actively participate in a relatively close cooperation network. In particular, Chinese scholars have led the world in terms of the number of publications, scholars, and collaborative papers in this field. Among important research institutions, Tsinghua University and the Chinese Academy of Sciences stand out, with CNCI values indicating that the average number of citations of their papers was more than twice the global average. Citation network analysis showed that Chinese scholars are representative and have become essential nodes in the citation network, indicating that China is constructing a large-scale talent team for affective computing and progressing in both the quantity and quality of research. However, China also faces disadvantages in academic journals, international conferences, and other aspects, leading to weak dominance, which restricts China’s academic discourse improvement in this field. Notably, in recent years, India’s publication volume has exceeded that of the United States for the first time, revealing a robust development potential linked to its advantages in computing. Nonetheless, India still has room for growth in terms of research quality and paper impact as it lacks representative scholars in the field of affective computing.### Challenges and technology development trends#### Modeling of cultural contextsThis study found that affective computing researchers are distributed across various countries globally and have a wide range of cultural backgrounds. While emotional expression has a degree of consistency across humanity, it is considerably influenced by cultural background. Cultural norms and values determine the different emotional experiences of individuals and how others perceive these emotions. Therefore, affective computing systems developed using a single cultural group may fail in other cultural contexts. For example, Chinese, Germans, and Japanese express emotions relatively implicitly, whereas Americans, British, and Brazilians express emotions more overtly. This indicates that emotion agents must match emotion calculation rules with the cultural context. Many Western cultural standards may not necessarily apply in Eastern contexts. For example, Japanese researchers tend to develop robots that can express emotions implicitly because overly direct expressions of emotions may cause user dissatisfaction ]. Therefore, cultural characteristics must be considered in developing universal cross-cultural emotional agents for people from different cultural backgrounds. Hofstede defined culture in terms of 5 measures—power distance, identity, gender, uncertainty avoidance, and long-term orientation—which can be used to summarize the typical rules of emotional expression in different cultural contexts ]. When it is challenging to obtain culture-specific empirical affective data, it is more feasible to design affective computational models using cultural theories and rules.#### Emotion generation techniquesThe cluster analysis of topic terms in affective computing revealed 5 important core topics, including “natural language processing techniques for affective computing and opinion mining” and “facial expression and micro-expression recognition and analysis.” Current research focuses more on emotion recognition, with relatively limited attention accorded to emotion generation. Emotion recognition and generation are both essential aspects of affective computing and constitute an important technical basis for the closed loop of human–computer interaction. To enable machines to provide more anthropomorphic and natural feedback, it is crucial to focus on the following 2 research areas. (a) Generation of facial expressions. The fact that human emotions are expressed through visual (55%), voice (38%), and verbal (7%) signals is also known as the “3V rule,” which reflects the importance of human facial expressions in emotion analysis ]. Appropriate use of facial expressions by avatars and robots can enhance human–robot interaction. Thus, current research aims to build a lexicon of facial expressions that can translate communicative intent into associated expressive morphology and dynamic features to express various meanings. Meanwhile, a team of animation experts is required to achieve realistic facial rendering effects, including lighting and muscle textures. (b) Generation of emotional body movement. This requires the design of embodied agents using computer models of body expression. This area involves studying human kinematics; however, researchers have yet to determine how to characterize the organic combination of body parts, movement strength, and posture of specific emotional states.#### Fine-grained sentiment classification modelsEkman’s basic emotion theory model is a widely used classification model for emotion computation ]. However, in real life, people’s emotions often exist in a mixed state. For example, people often simultaneously express surprise and joy, sadness and pain, etc. Du et\\\\xa0al. ] proposed the concept of mixed emotions based on research conducted using the Facial Action Coding System (FACS). They suggested that the combination of 2 basic emotions creates mixed emotions and defined different types using scenario examples. Using a FACS-based face recognition algorithm model, microvariations in facial muscles can be analyzed to accurately discriminate between different types of mixed emotions. Martinez ] assessed whether mixed emotions can be semantically labeled correctly. The test tasks included prioritization and forced selection of mixed emotion labels, and the results showed that subjects performed consistent and accurate categorization. Mixed emotion is an essential research direction for expression-based fine-grained emotion classification. This concept extends the core idea of FACS, aiming to reveal the relationship between mixed and basic emotions. It offers a better solution to the problem of differentiation of emotions and clarifies the relationship between differentiated emotions and their original emotions, providing traceable clues and measurement possibilities for the generation, development, and change of emotions. It summarizes complex emotional changes into a logical dynamic composite form with similar configuration effects, resulting in strong interpretability, logic, and unity.#### Code of ethics and technical standardsRecording an individual’s emotional state has implications for privacy, particularly when it comes to recording video or audio. Subjects may not agree to provide researchers with authentic and naturalistic emotional data and may feel uncomfortable being monitored in daily life. For example, the results of AI emotion monitoring tools may be analyzed alongside employee performance evaluations, predictions of the risk of leaving the job, and patterns of employee–team interactions for predicting behavior. Although the use of such technology reduces employee turnover and saves costs for organizations ], employees may experience constant psychological stress, leading to burnout ]. Additionally, individuals may lose autonomy as they become more hesitant to display emotions in public, instead choosing to use a “poker face.” While there should be openness in the use of affective computing, appropriate regulation is necessary to assess potential risks involving privacy and security, and the technology should be reviewed and documented for each industry to maximize benefits while minimizing harm, risks, and costs. Ethical issues are more likely to be overlooked in computing and engineering than in psychology. The collection of individual data, particularly physiological data, should be regulated by human research ethics committees, which are best suited to managing informed consent and privacy issues.Efforts should be made to strengthen the development of international standards in the field of affective computing to form a universally accepted specification. Currently, the available standard is “Information technology—Affective computing user interface (AUI)” (standard number ISO/IEC 30150-1:2022). The first part, “Model,” was released in June 2022, and the second part, “Affective Characteristics,” is under construction. However, there is a lack of standards for data collection, data security, and personal privacy protection in the field of affective computing. Therefore, the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union (ITU) should improve relevant standards and unify them for global use.#### Cognitive neuroscience-inspired affective computingJust as CNN architectures are inspired by biological visual processing and reinforcement learning methods are inspired by behaviorist theories in psychology, impulse network models are inspired by neuroplasticity. Cognitive neuroscience has also developed theories on affective circuits ], multiple-wave models ], embodied cognition ], and other related areas, providing brain-inspired insights into the design of affective computation models. Studies on the physiological representations of different emotions offer theoretical foundations and guidelines for feature extraction in affective computing based on facial expressions, psychophysiological measurements, and neuroimaging. Further human research in the field of cognitive neuroscience will ultimately affect the development of affective computing and artificial intelligence as a whole. The cognitive process of human brain emotion processing, its neural mechanism, and its anatomical basis provide essential inspiration for the development of affective computing models. However, to ensure that machines have genuine emotions rather than just appearing to have emotions, further research in cognitive neuroscience is required. This research may involve exploring the neural basis for the generation of human consciousness, the neural mechanism for the construction of human values, and other key scientific issues. Based on this neural theoretical foundation, simulation and machine implementation are feasible options for providing machines with authentic emotions.#### Construction of large-scale multimodal datasetsThe development of affective computing is highly dependent on the construction of large-scale open datasets. Three major trends are described below. The first trend predicts that dataset sizes will continue to grow to meet the demands of deep learning algorithm training. Deep-learning models have a substantial number of parameters, and the selection of these parameters requires samples that are typically 100 times the number of parameters. A larger dataset size enables the trained model to avoid overfitting, which improves model learning. However, the challenge lies in labeling these massive datasets. Thus, it is necessary to explore active, weakly supervised, and unsupervised learning methods to label the meaningful data in large unlabeled datasets or train machines for labeling. The second trend highlights the need for the collection of multimodal data, the accumulation of richer modal information, and fine-grained alignment between different modalities. At this stage, machines differ from human beings in 2 critical aspects: First, humans exist in a multimodal social environment, as evidenced by their joint expression of intentions and emotions through language, facial expressions, speech, and actions; second, humans can switch between modalities for emotional reasoning when dealing with emotions. They can also switch between different modalities to search for clues, eliminate ambiguities, and conduct emotional reasoning through interconnections. Therefore, creating a large-scale multimodal emotion dataset can contribute to the development of human-like emotion intelligence technology and the realization of more accurate emotion recognition. The third trend focuses on collecting natural-scene data, as emotional data collected in performance or evoked mode may not accurately represent real-life scenarios. However, collecting high-quality labeled emotional-physiological data in daily life remains a challenge due to the lack of hardware collection devices that are sufficiently comfortable and resistant to interference.#### Multimodal fusion technology innovationMultimodal fusion combines information from multiple modalities using multimodal representations for sentiment classification. It can enhance the performance of sentiment-computing models by playing a complementary and disambiguating role ]. Multimodal fusion methods can be classified as model-independent or model-based. Model-independent fusion methods do not rely on a specific deep-learning method, whereas model-based fusion methods do.There are 3 categories of model-independent fusion methods: early fusion (feature-based fusion), late fusion (decision-based fusion), and hybrid fusion (combination of the 2). Early fusion integrates features immediately after they are extracted and uses multiple signals to create a single feature vector, which is then modeled using machine-learning algorithms. The larger the number of features and the greater the variation in these features, the more challenging feature-level fusion becomes and the easier it is to overfit the training data. In contrast, late fusion performs integration only after each model outputs the results (e.g., classification or regression results). It can better handle overfitting but does not allow the classifier to train on all data simultaneously. The Dempster–Shafer theory of evidence is a generalization of Bayesian theory to subjective probability. It is widely used in late fusion models because of its ability to model uncertain knowledge and combine beliefs from different sources to obtain new beliefs that take into account all available evidence. Hybrid fusion combines the outputs of earlier fusion methods and unimodal predictors. Although it is flexible, careful design is required to determine the timing, modalities, and method of fusion based on the specific application problem and research content. Researchers must select the appropriate approach at their discretion.Model-based fusion methods address the multimodal fusion problem through implementation techniques and models, using 3 common methods: multiple kernel learning (MKL), graphical models (GMs), and neural networks (NNs). As these methods easily exploit the spatial and temporal structure of the data, they are particularly suitable for time-related modeling tasks. Additionally, they allow human expert knowledge to be embedded in the model, thereby enhancing interpretability. However, their disadvantage is that they are computationally expensive and challenging to train.Research has shown that synesthesia is generated not only in the cerebral cortex but also in the subcortical limbic system, including the thalamus, amygdala, and hippocampus, which are closely related to emotional processing ]. Inspired by the multistage fusion phenomenon that integrates multisensory information in the brain, a multistage multimodal emotion fusion method can be developed. This would first involve training a unimodal model, splicing it as an implicit state with another modal feature, training the bimodal model similarly, and continuing with this process until a multimodal model is obtained. In conclusion, multimodal fusion technology effectively utilizes the synergistic complementarity of different modal information ], enhances emotional understanding and expression, and improves model robustness and performance. This represents an important direction for future research.#### Data- and knowledge-driven technological innovationIn its early stages, affective computing research relied heavily on collected data to make inferences. However, this data-driven approach is both inefficient and ineffective at the application level. For humans to understand data fully, they must activate other associated information, such as potential knowledge or common sense. The human brain can seamlessly combine this information to enable more generalized, intelligent, and frugal computation for complex problems. Therefore, affective computing requires not only big data and extensive computing power but also the integration of knowledge. Knowledge guidance and inspiration can compensate for insufficient or uneven data quality while conserving computational power. For instance, in constructing a multidisciplinary and multi-faceted emotional knowledge map, fine-grained emotional knowledge integrated through emotional commonsense associations is used to enable the modeling of hierarchical logical relationships between aspect words and emotional words. This approach facilitates the dynamic correlation, aggregation, and reasoning of domain, aspect, and emotional knowledge. It provides an optimal solution for various applications of affective computing, such as efficient real-time online sentiment analysis, emotion-injected dialogue systems, and emotion-injected story generation. These applications provide dynamic and accurate domain-adaptive sentiment knowledge.#### Group affective computingCurrent research in affective computing primarily focuses on sentiment analysis at the individual level, neglecting the potential value of group-affective computing. For instance, emotions felt by individual employees can aggregate and spread to create “collective emotions” in the workplace. These shared emotions can considerably affect the organization by offering insights into absenteeism, intra-team communication, team cohesion and performance, and organizational citizenship behavior. As such, affective computing research could expand its focus from individual to collective affect analysis and the propagation of affect across people. Furthermore, group affective computing can predict consumer behavior. EEG-based hyperscanning technology, which explores dynamic brain activity between 2 or more interacting customers and their underlying neuroemotional activities, can be used to anticipate shared consumption intentions, panic buying, and group-buying marketing effects. Although group affective computing currently lacks a well-established research methodology, it is a promising direction for future studies.#### Unique emotional carriersEmotions are ubiquitous in human political, economic, and cultural life, and the carriers of emotions are continually increasing in number, making them a popular research topic. Several areas have been identified as key carriers of emotions. (a) Political speeches: CORPS is a corpus that contains political speeches with markers indicating audience reactions such as applause, standing ovations, and boos ]. Researchers can use this information to predict emotion-evoking actions and persuasive content that may induce empathy and sympathy in audiences. (b) Music and drama: Affective computing in music and drama provides a basis for the categorized retrieval of relevant emotional carriers. Advancements in artificial intelligence-generated content (AIGC) technology have made machine-generated music possible, and affective computing can enhance the generation of music to conform to emotional classifications. (c) Oil painting: As a representative art form, oil painting allows creators to express their innermost emotions. Its charm lies not in the degree of realism but in the emotions it conveys. Combining affective computing with oil painting would enable the exploration of artificial intelligence methods for emotional expression, the integration of technology and art, and the establishment of a library of emotion-inducing materials for oil paintings, thereby providing resources for the development of affective computing disciplines.### Outlook for future applications#### Affective brain–computer interfacesAffective brain–computer interfaces (aBCIs) are primarily designed to measure emotional states through neurological measurements and to recognize and/or regulate human emotions. Currently, aBCIs are one of the main methods of realizing emotional intelligence. At this stage, the most commonly used physiological signals for emotional brain–computer interfaces are EEG signals, which map closely to an individual’s emotional state. As in motor brain–computer interfaces, the human brain plays the role of a controller for the entire system. The first step involves decoding an individual’s initial emotional state and then recognizing and understanding their emotions. Subsequently, a control strategy or system is designed to achieve the target emotion using control signals or parameters that provide feedback to the brain, thereby forming a closed-loop system.Unlike facial expressions, physiological signals such as EEG signals are difficult to disguise and provide an accurate reflection of the real emotional state of the individual. As a result, affective brain–computer interfaces play a crucial role in clinical diagnostics and therapy. Their uses include detecting workload and mental state, using neurofeedback for stress relief, aiding in the diagnosis of social anxiety and other disorders ], and enabling objective assessment and intervention in depression. Furthermore, affective brain–computer interfaces have considerable potential for military applications. They can help maximize the physiological capabilities of individual soldiers, enhance their endurance and tolerance to extreme environments, and improve their overall physical and mental fitness. These objectives are achieved by installing electroencephalography electrodes inside combat helmets to detect threats and emotional signals emitted by the brain. The signals are then converted into computer language using computer algorithms, analyzed, and confirmed by combat command. Subsequently, threat warnings and reminders about emotional regulation are sent to the affected soldiers, and signals to cooperate in combat are transmitted to surrounding soldiers. In addition, direct transcranial current stimulation, transcranial electromagnetic stimulation, and deep brain cortex stimulation can act on the brain to eliminate fatigue, reduce stress and anxiety, control pain sensation, and enhance cognitive ability. This system helps improve the situational awareness of soldiers on the battlefield, thereby improving their ability to survive.The primary obstacle to the application of affective brain–computer interfaces is their unstable performance. Cross-modal affective models that rely on heterogeneous transfer learning (HTL) may be necessary for establishing reliable and robust aBCI technology in complex real-world environments. To address the missing-modalities problem, cross-modal emotion models comprehensively analyze signals from multiple modalities and extract correlation characteristics during the training process. In the testing stage, predictions are made based on partial modal information. For example, correlating EEG signals with eye movement enables the use of eye movement alone to assess emotions in scenarios where collecting EEG signals is difficult. The HTL approach ensures that performance degradation in the absence of modalities is acceptable, thereby improving model robustness. In addition, transfer-learning techniques based on deep and generative adversarial networks can solve the problem of individual differences. These techniques enable generalization from the source domain to the target domain, thereby expanding the scope of possible applications of affective brain–computer interfaces.#### Empathic human–computer dialogueThere have been 4 waves of change in the way people interact with machines. The first wave, represented by Microsoft, involved the organic fusion of the user interface, operating system, keyboard, and mouse. This greatly reduced the difficulty of human–computer interaction and contributed to the rapid popularization of the personal computer. The second wave, represented by Google, involved the organic integration of search engine and internet technologies. This integration broke down information silos and considerably expanded the boundaries of interaction. The third wave, represented by Apple, involved the miniaturization of computing represented by the smartphone. This breakthrough removed the physical space limitations of human–computer interaction, enabling interconnectivity anytime, anywhere. Currently, we are in the fourth wave, represented by OpenAI. This wave involves the comprehensive application of a human–computer dialogue system that makes human–computer interaction more anthropomorphic and naturalized.The essence of human–computer dialogue is to make human–computer interaction more human-like. Humans exchange information through natural language and multiple senses, and human–computer interaction can imitate this process through multimodal information for joint analysis and decision-making. Human–computer dialogue involves a diverse range of signals, including speech, text, and images (such as individual facial expressions and body movements), conveying information in both the rational and perceptual dimensions. Linguistic text serves as the ontology of intent understanding, but emotional information conveyed through voice intonation, facial expressions, and body movements plays a crucial role in disambiguation, which is essential for in-depth communication between humans and machines. The use of different emotional colors to express the same sentence results in entirely different connotations. As Nobel Prize winner Simon noted, emotion recognition is crucial for the communication and understanding of information. Therefore, affective computing offers machines the ability to achieve deep contextual understanding.In advanced technology fields, research has expanded to include machine expression and action generation, referred to as “multimodal emotional expression generation.” A current focus area is the development of a “virtual human” interface that not only appears human-like but also simulates human demeanor and behavior. For instance, voice-driven facial-expression animation generation technology can create virtual humans with facial expressions and lip, head, and body movements that closely resemble those of real people. The virtual human no longer has an empty skin but appears more 3-dimensional and vivid. The personalization of human–computer interaction lays the crucial foundations for future applications in areas such as elderly companions, intelligent customer service, and mayor hotlines, revealing important prospects for practical use.#### Emotion-assisted decision-makingHuman–computer interaction involves both shallow and deep levels. At the shallow level, machines are equipped with the ability to read and speak, whereas at the deep level, they are capable of thinking and making decisions like humans. Nobel Prize winner Kahneman described human decision-making as entailing 2 processes: fast (System 1) and slow (System 2). The unconscious “System 1” relies on emotions, experience, and rapid judgments, while the conscious “System 2” relies on rational deliberation. Emotions play an important role in advanced human thinking and decision-making. The book “Descartes’ Error” emphasizes that emotions are crucial for rational decision-making and behavior ]. Numerous studies have indicated that purely rational decision-making may not always be the optimal solution for humans when dealing with problems due to the complexity of the social environment. Incorporating emotional factors into the decision-making process may help individuals identify better solutions. Therefore, inputting emotional variables can enable machines to make decisions in a more human-like manner. In building a harmonious human–machine symbiotic society, it is essential to master this high-level function, which is also an important direction in affective computing research. The modeling of machine agents has begun to incorporate patterns of emotional influence on human rational decision-making and mechanisms for deciding and interrupting behaviors based on goals , ].Emotion-assisted decision-making abilities can be applied widely across various fields of human–machine collaboration. For example, in production tool manipulation, the operator’s emotional state regarding operation specifications, safety awareness, and accurate judgment has an impact. Monitoring and early warning of negative emotions, psychological stress, fatigue, and drowsiness, etc., can help identify potential anthropogenic risks to production safety. Machines can then optimize management decisions, intervene early, and intervene intelligently to avoid major accidents. In assisted driving, negative emotions such as anger and anxiety can seriously affect the driver’s concentration and may lead to traffic accidents. Emotion-assisted decision making can be incorporated into driver monitoring systems (DMS) that use facial-expression recognition technology and wearable devices to provide real-time monitoring of the driver’s emotional state. This approach equips the vehicle with enhanced safety performance and improves the overall driving experience ].#### Affective virtual realityThe metaverse is generating considerable interest in both industrial and academic circles as the next generation of immersive, full-fledged internet. It is considered a theme park for digitized human beings, a virtual complex resulting from the development of cutting-edge technologies, and a utopia where the human body and consciousness can cross physical time and space. As a new type of future living space, the development of the metaverse cannot be limited to creating a virtual space parallel to the real world. It should exist in human life like air, enabling humans to shuttle freely between the virtual and real worlds. Affective virtual reality is crucial for constructing the metaverse because it can considerably enhance an individual’s experience of bodily ownership, sense of agency, and situational awareness. In particular, an individual’s avatar in the metaverse, which is a core element of the metaverse construction, includes voice tone, facial expressions, body movements, and gestures that richly and 3-dimensionally express the individual’s emotions and create scenes and spaces for emotional twins ]. As in movies and literature, complex and emotionally rich avatar characters engage audiences more than simple and stable characters do. This appeal creates the illusion that avatars are alive and pass the Turing test, which enhances the audience’s interest and engagement in the virtual world ]. Affective virtual reality has considerable potential for applications in virtual reality socialization, virtual reality anchors, and virtual reality marketing.### LimitationsThis bibliometric analysis has several limitations that should be acknowledged. First, the basic processing unit of information in this study is the article in its entirety, and the full content of the literature has not been systematically broken down, which may result in incomplete analysis and conclusions. Second, the assumption that the articles contain information of equal quality makes it difficult to consider the objective differences in the value of the literature. In future research, a combination of bibliometrics and content analysis could be used to enhance the reliability and accuracy of the analytical results.## ConclusionAffective computing is a rapidly developing field with broad prospects. Emerging forces such as China and India are injecting strong momentum into the field. However, the field of affective computing also faces challenges and development trends in 10 aspects, including cultural background modeling, ethical and moral norms, and multimodal integration. Affective computing has great potential for application in 4 major fields and requires the joint efforts of researchers and industry practitioners. These efforts can make affective computing beneficial to the progress of human society by building a more anthropomorphic, harmonious, and natural human–computer symbiotic social form.## Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.## Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.## References1Keltner D, Sauter D, Tracy J, Cowen A. Emotional expression: Advances in basic emotion theory. _J Nonverbal Behav_. 2019; **43**(2):133–160.2Soleymani M, Garcia D, Jou B, Schuller B, Chang S-F, Pantic M. A survey of multimodal sentiment analysis. _Image Vis Comput_. 2017; **65**:3–14.3Bach DR, Dayan P. Algorithms for survival: A comparative perspective on emotions. _Nat Rev Neurosci_. 2017; **18**:311–319.4Chen L, Zhou M, Wu M, She J, Liu Z, Dong F, Hirota K. Three-layer weighted fuzzy support vector regression for emotional intention understanding in human–robot interaction. _IEEE Trans Fuzzy Syst_. 2018; **26**(5):2524–2538.5Kahneman D. _Thinking_, _fast and slow_. Macmillan, London, UK: Farrar, Straus and Giroux; 2011.6Fanselow MS. Emotion, motivation and function. _Curr Opin Behav Sci_. 2018; **19**:105–109.7Lopes PN, Salovey P, Coté S, Beers M. Emotion regulation abilities and the quality of social interaction. _Emotion_. 2005; **5**:113–118.8Suvilehto JT, Glerean E, Dunbar RIM, Hari R, Nummenmaa L. Topography of social touching depends on emotional bonds between humans. _Proc Natl Acad Sci U S A_. 2015; **112**:13811–13816.9Picard RW. _Affective computing_. Cambridge (MA): MIT Press; 1997.10Ekman P. Are there basic emotions? _Psychol Rev_. 1992; **99**(3):550–553.11Russell JA. A circumplex model of affect. _J Pers Soc Psychol_. 1980; **39**:1161–1178.12Mehrabian A. Framework for a comprehensive description and measurement of emotional states. _Genet Soc Gen Psychol Monogr_. 1995; **121**(3):339–361.13Bakker I, Van Der Voordt T, Vink P, De Boon J. Pleasure, arousal, dominance: Mehrabian and russell revisited. _Curr Psychol_. 2014; **33**:405–421.14Pozzi FA, Fersini E, Messina E, Liu B. Chapter\\\\xa01—Challenges of sentiment analysis in social networks: An overview. In: Pozzi FA, Fersini E, Messina E, Liu B, editors, _Sentiment analysis in social networks_. Boston: Morgan Kaufmann; 2017. p. 1–11.15Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. Poster presented at: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies; Portland, Oregon, USA; 2011. p. 142–150.16Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing; 2013; Seattle, WA, USA. p. 1631–1642.17Blitzer J, Dredze M, Pereira F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Poster presented at: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics; 2007; Prague, Czech Republic. p. 440–447.18Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of German emotional speech. _Interspeech_. 2005; **5**:1517–1520.19McKeown G, Valstar M, Cowie R, Pantic M, Schroder M. The SEMAINE Database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE Trans Affect Comput_. 2011; **3**(1):5–17.20Xu L, Xu M, Yang D. Chinese emotional speech database for the detection of emotion variations. _J Tsinghua Univ Nat Sci._ 2009;49(S1):1413–1418 _._21Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: From unimodal analysis to multimodal fusion. _Inf Fusion_. 2017; **37**:98–125.22Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, Gao S, Sun Y, Ge W, Zhang W, et al. A systematic review on affective computing: Emotion models, databases, and recent advances. _Inf Fusion_. 2022; **83–84**:19–52.23Zhang Z, Luo P, Loy CC, Tang X. From facial expression recognition to interpersonal relation prediction. _Int J Comput Vis_. 2018; **126**:550–569.24Mollahosseini A, Hasani B, Mahoor MH. AffectNet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Trans Affect Comput_. 2019; **10**:18–31.25Li S, Deng W, Du J. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017; . Honolulu, HI. p. 2584–2593.26Li X, Pfister T, Huang X, Zhao G, Pietikäinen M. A spontaneous micro-expression database: Inducement, collection and baseline. Paper presented at: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG); 2013; Shanghai, China. p. 1–6.27Galvão F, Alarcão SM, Fonseca MJ. Predicting exact valence and arousal values from EEG. _Sensors (Basel)_. 2021; **21**(10):3414.28Shalbaf A, Bagherzadeh S, Maghsoudi A. Transfer learning with deep convolutional neural network for automated detection of schizophrenia from EEG signals. _Phys Eng Sci Med_. 2020; **43**(4):1229–1239.29Shirahama K, Grzegorzek M. Emotion recognition based on physiological sensor data using codebook approach. In: Piętka E, Badura P, Kawa J, Wieclawek W, editors. _Information technologies in medicine_. Cham: Springer International Publishing; 2016. p. 27–39.30Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I. DEAP: A database for emotion analysis\\\\xa0using physiological signals. _IEEE Trans Affect Comput_. 2012; **3**(1):18–31.31Duan R-N, Zhu J-Y, Lu B-L. Differential entropy feature for EEG-based emotion classification. Paper presented at: 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER); 2013; San Diego, CA, USA. p. 81–84.32Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K. Introducing WESAD, a multimodal dataset for wearable stress and affect detection. Paper presented at: Proceedings of the 20th ACM International Conference on Multimodal Interaction; 2018; Boulder, CO, USA. p. 400–408.33Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. _Comput Linguist_. 2011; **37**(2):267–307.34Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. Paper presented at: Proceedings of the International Conference on Web Search and Web Data Mining—WSDM ’08; 2008; Palo Alto, CA, USA. p. 231.35Mullen T, Collier N. Sentiment analysis using support vector machines with diverse information sources. Paper presented at: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing; 2004; Barcelona, Spain. p. 412–418.36Pak A, Paroubek P. Text representation using dependency tree subgraphs for sentiment analysis. In: Xu J, Yu G, Zhou S, Unland R, editors. _Database systems for advanced applications_. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. p. 323–332.37Deng J, Ren F. A survey of textual emotion recognition and its challenges. _IEEE Trans Affect Comput_. 2023; **14**(1):49–67.38Heaton CT, Schwartz DM. Language models as emotional classifiers for textual conversation. Paper presented at: Proceedings of the 28th ACM International Conference on Multimedia; 2020; Seattle, WA, USA. p. 2918–2926.39Mao R, Liu Q, He K, Li W, Cambria E. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. _IEEE Trans Affect Comput_. 2022; **14**(3):1743–1753.40Lee CM, Narayanan SS. Toward detecting emotions in spoken dialogs. _IEEE Trans Audio Speech Lang Process_. 2005; **13**(2):293–303.41Lugger M, Yang B. The relevance of voice quality features in speaker independent emotion recognition. Paper presented at: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing—ICASSP ’07; 2007; Honolulu, HI, USA. p. IV-17–IV–20.42Likitha MS, Gupta SRR, Hasitha K, Raju AU. Speech based human emotion recognition using MFCC.Paper presented at: 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); 2017; Chennai, India. p. 2257–2260.43Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. _Speech Commun_. 2010; **52**(7–8):613–625.44Alisamir S, Ringeval F. On the evolution of speech representations for affective computing: A brief history and critical overview. _IEEE Signal Process. Mag_. 2021; **38**(6):12–21.45Stappen L, Baird A, Schumann L, Schuller B. The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements. _IEEE Trans Affect Comput_. 2023; **14**(2):1334–1350.46Huang Z, Dong M, Mao Q, Zhan Y. Speech emotion recognition using CNN. Paper presented at: Proceedings of the 22nd ACM International Conference on Multimedia; 2014; New York, NY, USA. p. 801–804.47Neumann M, Vu NT. Improving speech emotion recognition with unsupervised representation learning on unlabeled speech. Paper presented at: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2019; Brighton, UK. p. 7390–7394.48Abdelwahab M, Busso C. Domain adversarial for acoustic emotion recognition. _IEEE/ACM Trans Audio Speech Lang Process_. 2018; **26**(12):2423–2435.49Shan C, Gong S, McOwan PW. Facial expression recognition based on Local Binary Patterns: A comprehensive study. _Image Vis Comput_. 2009; **27**(6):803–816.50Chao W-L, Ding J-J, Liu J-Z. Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection. _Signal Process_. 2015; **117**:1–10.51James W. Review of la pathologie des emotions by Ch. Féré. _Philos Rev_. 1893; **2**:333–336.52Cannon WB. The James-Lange theory of emotions: A critical examination and an alternative theory. _Am J Psychol_. 1987; **100**:567–586.53Kim M-K, Kim M, Oh E, Kim S-P. A review on the computational methods for emotional state estimation from the human EEG. _Comput Math Methods Med_. 2013; **2013**: Article e573734.54Craik A, He Y, Contreras-Vidal JL. Deep learning for electroencephalogram (EEG) classification tasks: A review. _J Neural Eng_. 2019; **16**(3): Article 031001.55Maria MA, Akhand MAH, Shimamura T. Emotion recognition from EEG with normalized mutual information and convolutional neural network. Paper presented at: 2022 12th International Conference on Electrical and Computer Engineering (ICECE); 2022; Dhaka, Bangladesh. p. 372–375.56Rahman MM, Sarkar AK, Hossain MA, Hossain MS, Islam MR, Hossain MB, Quinn JMW, Moni MA. Recognition of human emotions using EEG signals: A review. _Comput Biol Med_. 2021; **136**: Article 104696.57D’mello SK, Kory J. A review and meta-analysis of multimodal affect detection systems. _ACM Comput Surv_. 2015; **47**(3):1–36.58He Z, Li Z, Yang F, Wang L, Li J, Zhou C, Pan J. Advances in multimodal emotion recognition based on brain–computer interfaces. _Brain Sci_. 2020; **10**(10):687.59Filippini C, Perpetuini D, Cardone D, Chiarelli AM, Merla A. Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review. _Appl Sci_. 2020; **10**(8):2924.60Spezialetti M, Placidi G, Rossi S. Emotion recognition for human-robot interaction: Recent advances and future perspectives. _Front Robot AI_. 2020; **7**: Article 532279.61Peng Y, Fang Y, Xie Z, Zhou G. Topic-enhanced emotional conversation generation with attention mechanism. _Knowl Based Syst_. 2019; **163**:429–437.62Dybala P, Ptaszynski M, Rzepka R, Araki K, Sayama K. Metaphor, humor and emotion processing in human-computer interaction. _Int J Comput Linguist Res_. 2013.63Goswamy T, Singh I, Barkati A, Modi A. Adapting a language model for controlled affective text generation. Paper presented at: Proceedings of the 28th International Conference on Computational Linguistics; 2020; Barcelona, Spain. p. 2787–2801.64Lei Y, Yang S, Wang X, Xie L. MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. _IEEE/ACM Trans Audio Speech Lang Process_. 2022; **30**:853–864.65Crawford K. Time to regulate AI that interprets human emotions. _Nature_. 2021; **592**(7853):167.66Ho M-T, Mantello P, Nguyen H-KT, Vuong Q-H. Affective computing scholarship and the rise of China: A view from 25 years of bibliometric data. _Humanit Soc Sci Commun_. 2021; **8**: Article 282.67Yadegaridehkordi E, Noor NFBM, Ayub MNB, Affal HB, Hussin NB. Affective computing in education: A systematic review and future research. _Comput Educ_. 2019; **142**: Article 103649.68Wu C-H, Huang Y-M, Hwang J-P. Review of affective computing in education/learning: Trends and challenges. _Br J Educ Technol_. 2016; **47**(6):1304–1323.69Liberati G, Veit R, Kim S, Birbaumer N, von Arnim C, Jenner A, Lulé D, Ludolph AC, Raffone A, Belardinelli MO, da Rocha JD, Sitaram R. Development of a binary fMRI-BCI for Alzheimer patients: A semantic conditioning paradigm using affective unconditioned stimuli. Paper presented at: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction; 2013; Geneva, Switzerland. p. 838–842.70Yuvaraj R, Murugappan M, Mohamed Ibrahim N, Iqbal Omar M, Sundaraj K, Mohamad K, Palaniappan R, Mesquita E, Satiyan M. On the analysis of EEG power, frequency and asymmetry in Parkinson’s disease during emotion processing. _Behav Brain Funct_. 2014; **10**:12.71Baki P, Kaya H, Çiftçi E, Güleç H, Salah AA. A multimodal approach for mania level prediction in bipolar disorder. _IEEE Trans Affect Comput_. 2022; **13**(4):2119–2131.72Mohammadi-Ziabari SS, Treur J. Integrative biological, cognitive and affective modeling of a drug-therapy for a post-traumatic stress disorder. In: Fagan D, Martín-Vide C, O’Neill M, Vega-Rodríguez MA, editors. _Theory and practice of natural computing_. Cham: Springer International Publishing; 2018. p. 292–304.73Tivatansakul S, Ohkura M. Healthcare system focusing on emotional aspects using augmented reality—Implementation of breathing control application in relaxation service. Paper presented at: 2013 International Conference on Biometrics and Kansei Engineering; 2013; Tokyo, Japan. p. 218–222.74Zenonos A, Khan A, Kalogridis G, Vatsikas S, Lewis T, Sooriyabandara M. HealthyOffice: Mood recognition at work using smartphones and wearable sensors. Paper presented at: 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops); 2016; Sydney, NSW, Australia. p. 1–6.75Weziak-Bialowolska D, Bialowolski P, Lee MT, Chen Y, VanderWeele TJ, McNeely E. Psychometric properties of flourishing scales from a comprehensive well-being assessment. _Front Psychol_. 2021; **12**: Article 652209.76Pei G, Xiao Q, Pan Y, Li T, Jin J. Neural evidence of face processing in social anxiety disorder: A systematic review with meta-analysis. _Neurosci Biobehav Rev_. 2023; **152**: Article 105283.77Pei G, Li T. A literature review of EEG-based affective computing in marketing. _Front Psychol_. 2021; **12**: Article 602843.78Valle-Cruz D, Fernandez-Cortez V, López-Chau A, Sandoval-Almazán R. Does twitter affect stock market decisions? Financial sentiment analysis during pandemics: A comparative study of the H1N1 and the COVID-19 periods. _Cognit Comput_. 2022; **14**(1):372–387.79Gómez LM, Cáceres MN. Applying data mining for sentiment analysis in music. In: De la Prieta F, Vale Z, Antunes L, Pinto T, Campbell AT, Julián V, Neves AJR, Moreno MN, editors. _Trends in cyber-physical multi-agent systems_. Cham: Springer International Publishing; 2018. p. 198–205.80Yu L, Zhang W, Wang J, Yu Y. SeqGAN: Sequence generative adversarial nets with policy gradient. Paper presented at: Proceedings of the AAAI Conference on Artificial Intelligence; 2017; San Francisco, CA, USA. p. 31.81Oliveira HG. A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation. Paper presented at: Proceedings of the 10th International Conference on Natural Language Generation; 2017; Santiago de Compostela, Spain. p. 11–20.82Zhang X, Lapata M. Chinese Poetry Generation with Recurrent Neural Networks. Paper presented at: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2014; Doha, Qatar. p. 670–680.83Mao G, Liu X, Du H, Zuo J, Wang L. Way forward for alternative energy research: A bibliometric analysis during 1994–2013. _Renew Sustain Energy Rev_. 2015; **48**:276–286.84Haustein S, Larivière V. The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects. In: Welpe I, Wollersheim J, Ringelhan S, Osterloh M, editors. _Incentives and performance: Governance of research organizations_. Cham: Springer International Publishing; 2014. p. 121–139.85Hammarfelt B, Rushforth AD. Indicators as judgment devices: An empirical study of citizen bibliometrics in research evaluation. _Res Eval_. 2017; **26**(3):169–180.86Wang J, Veugelers R, Stephan P. Bias against novelty in science: A cautionary tale for users of bibliometric indicators. _Res Policy_. 2017; **46**(8):1416–1436.87Van Eck NJ, Waltman L. Software survey: VOSviewer, a computer program for bibliometric mapping. _Scientometrics_. 2010; **84**(2):523–538.88Šabanović S. Robots in society, society in robots. _Int J of Soc Robotics_. 2010; **2**:439–450.89Hofstede G. _Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations_. London, UK: Sage; 2001.90Mehrabian A. _Communication without words. Communication theory. 2nd ed._ London, UK: Routledge; 2008.91Du S, Tao Y, Martinez AM. Compound facial expressions of emotion. _Proc Natl Acad Sci U S A_. 2014; 111(15):E1454–E1462.92Martinez AM. Computational models of face perception. _Curr Dir Psychol Sci_. 2017; **26**(3):263–269.93Dragano N, Lunau T. Technostress at work and mental health: Concepts and research results. _Curr Opin Psychiatry_. 2020; **33**(4):407–413.94LeDoux J. _The emotional brain: The mysterious underpinnings of emotional life_. New York, NY, USA: Simon and Schuster; 1998.95Pessoa L, Adolphs R. Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. _Nat Rev Neurosci_. 2010; **11**(11):773–782.96Price TF, Peterson CK, Harmon-Jones E. The emotive neuroscience of embodiment. _Motiv Emot_. 2012; **36**:27–37.97Cytowic RE. _Synesthesia: A union of the senses_. Cambridge, MA, USA: MIT Press; 2002.98Guerini M, Strapparava C, Stock O. CORPS: A corpus of tagged political speeches for persuasive communication processing. _J Inf Technol Politics_. 2008; **5**(1):19–32.99Damasio AR. _Descartes’ error_. New York, NY, USA: Random House; 2006.100Scheutz M. The inherent dangers of unidirectional emotional bonds between humans and social robots. In: Lin P, Abney K, Bekey GA, editors. _Robot ethics: The ethical and social implications of robotics_. Cambridge (MA): MIT Press; 2011. p. 205.101Scheutz M, Schermerhorn P. Dynamic robot autonomy: Investigating the effects of robot decision-making in a human-robot team task. Paper presented at: Under review for the 4th ACM International Conference on Human-Robot Interaction; 2009; La Jolla, CA, USA.102Gill R, Singh J. A review of neuromarketing techniques and emotion analysis classifiers for visual-emotion mining. Paper presented at: 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART); 2020; Moradabad, India. p. 103–108.103Pei G, Li B, Li T, Xu R, Dong J, Jin J. Decoding emotional valence from EEG in immersive virtual reality. Paper presented at: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC); 2022; Chiang Mai, Thailand. p. 1469–1476.104Ochs M, Sadek D, Pelachaud C. A formal model of emotions for an empathic rational dialog agent. _Auton Agent Multi-Agent Syst_. 2012; **24**:410–440.### Information#### Published In![]( ComputingVolume 3#### CopyrightCopyright © 2024 Guanxiong Pei et\\\\xa0al.Exclusive licensee Zhejiang Lab. No claim to original U.S. Government Works. Distributed under a .#### Article versions#### Submission history**Received**: 15 October 2023**Accepted**: 6 December 2023**Published online**: 5 January 2024#### Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.#### Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.### Authors#### AffiliationsGuanxiongPei Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.HaiyingLi National Science Library, Chinese Academy of Sciences, Beijing, China.YandiLu Center for Psychological Sciences, Zhejiang University, Hangzhou, China.YanleiWangDe.InnoScience, Deloitte, Shanghai, China.ShizhenHuaResearch Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.TaihaoLi  Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.#### Funding Informationthe Zhejiang Provincial Natural Science Foundation of China : LQ22C090007the National Natural Science Foundation of China : T2241018the National Science and Technology Major Project of the Ministry of Science and Technology of China : 2021ZD0114303the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2023KFKT003the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2022KFKT002#### Notes*Address correspondence to: ### Metrics#### Article UsageArticle Metrics- - No data available.05001,0001,500Jan 2024Apr 2024Jul 2024Oct 2024Jan 2025Apr 2025Jul 202520,97629- - - - Total number ofdownloads and citation**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.Citation information is sourced from  service.#### Altmetrics]( more details]( up by **10** news outlets]( by **7** X users]( **1** Facebook pages]( readers on Mendeley#### Dimensions32Total citations32Recent citationsn/aField Citation Ration/aRelative Citation Ratio]( 10 citation in Web of Science- 29 citation in Crossref#### scite![]( Citations22010Citing PublicationsSupportingMentioningContrastingSee how this article has been cited at scite shows how a scientific paper has been cited by providing the context of the citation, a classification describing whether it supports, mentions, or contrasts the cited claim, and a label indicating in which section the citation was made.### Citations#### Cite as- Guanxiong Pei,- Haiying Li,- Yandi Lu,- Yanlei Wang,- Shizhen Hua,- Taihao Li.Affective Computing: Recent Advances, Challenges, and Future Trends._Intell Comput._2024;3:0076.DOI: #### Export citationSelect the format you want to export the citation of this publication.Please select one from the listRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorksDirect import01. - Jonas Fritsch,    Sticking with Affect in HCI and Design: from Interaction to Relation, Proceedings of the sixth decennial Aarhus conference: Computing X Crisis, (202-215), (2025).     02. - Yuan Zhou,    - Siamak Khatibi,Exploring Artificial Personality Grouping Through Decision Making in Feature Spaces, AI, **6**, 8, (184), (2025). 03. - FNU Rakhi,    Modeling Students’ Emotions in Computing Education: A Context-Specific Multi-Modal Approach, Proceedings of the 2025 ACM Conference on International Computing Education Research V.2, (59-60), (2025).     04. - Jialin Sun,    Research on the Design of Intelligent Voice Interaction System Based on Affective Computing, International Scientific Technical and Economic Research, (1-15), (2025).     05. - Susanne Draheim,    - Kai von Luck,    - Larissa Putzar,Working with the Gaps in Understanding: Revisiting an Approach to Affective Computing Research in the EmotionBike Project, Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments, (596-601), (2025). 06. - Ken Nee Chee,    Affectagogy Unleashed, Pedagogical Approaches to Bridging Emotional and Cognitive Learning, (277-292), (2025).     07. - Joaquim C. Reis,    - Luzia Travado,    - Michael H. Antoni,Advancing health psychology research in oncology: Biobehavioral models, stress pathways, and stress-management interventions for cancer patients, International Journal of Clinical and Health Psychology, **25**, 3, (100615), (2025). 08. - Manuel Paniagua-Gómez,    - Manuel Fernandez-Carmona,Trends and Challenges in Real-Time Stress Detection and Modulation: The Role of the IoT and Artificial Intelligence, Electronics, **14**, 13, (2581), (2025). 09. - Thuy Thi Tran,    - Thang Van Le,    - Nam Hoang Le,    - Anh Van Thi Dam,    - Thao Thi Nguyen,    - Anh Thi Tuan Nguyen,    - Hue Thi Nguyen,Emotional attachment to artificial intelligence and perceived social isolation among university students: An application of Sternberg’s triangular theory of love, Multidisciplinary Science Journal, **7**, 12, (2025662), (2025). 10. - Jan Kincl,    - Marc T. P. Adam,    - Tanja Pavleska,The Role of\\\\xa0Human Factors in\\\\xa0Cybersecurity Incident Management, HCI for Cybersecurity, Privacy and Trust, (35-53), (2025). 11. See moreLoading...### View options#### PDF formatDownload this article as a PDF file### Share#### Copy the article linkCopy LinkCopied!Copying failed.#### Share on social media        | Now Reading:- - \")- - - - - -  xrefBack.goToxrefBack.goToRequest permissionsExpand AllCollapseExpand for moreShow all referencesSHOW ALL BOOKSCopy link✓Thanks for sharing!Find any serviceA2A![](\\'}]', name='web_search_tool', call_id='call_0QfXVfV9DNu7IVPtgrFNajy7', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 6, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_d74ZAGK7DVpc4VJFOMsgev51', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Best Vibe Coding Tools 2025: Top Picks for Better Coding\\', \\'url\\': \\'https://azumo.com/artificial-intelligence/ai-insights/best-vibe-coding-tools\\', \\'markdown\\': \\'- - - # Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunExplore the 10 best vibe coding tools for 2025, designed to make coding a more enjoyable and efficient experience. These tools enhance developer productivity with features like AI assistance, fast performance, intuitive design, and smooth collaboration. Learn what makes a tool have \"good vibes\" and why selecting the right coding environment is essential for long-term success. Whether you\\\\\\'re looking for minimal setup friction or an elegant interface, these tools will improve your coding journey. From Cursor IDE to GitHub Copilot, find the best fit for your workflow and elevate your development process.Written by:September 2, 2025![]( Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunLet\\\\\\'s be real for a second. You can absolutely build amazing software with basic text editors and command-line tools. But why would you want to? The days of suffering through clunky interfaces and frustrating workflows are long gone. Today\\\\\\'s coding tools aren\\\\\\'t just about functionality anymore; they\\\\\\'re about creating an experience that makes you actually excited to write code.Think about it this way: you probably spend 8+ hours a day staring at your development environment. Shouldn\\\\\\'t it feel good? Shouldn\\\\\\'t it make you more productive instead of fighting you every step of the way? That\\\\\\'s exactly what we\\\\\\'re talking about when we say \"vibe\" in coding tools.The numbers back this up, too. According to Stack Overflow\\\\\\'s 2024 Developer , 76% of developers are now using or planning to use AI coding assistants. The entire software development tools market is projected to hit . This isn\\\\\\'t just about new features anymore; it\\\\\\'s about developer happiness.At Azumo, we’ve worked with leading companies like Meta, Wolters Kluwer, and Wine Enthusiast to deliver cutting-edge AI-driven solutions, helping teams optimize their workflows and enhance business outcomes.Our deep expertise in  and our  give us a unique perspective on the best tools available in the coding space. We understand that the right tools can dramatically improve a developer\\\\\\'s experience and productivity, which is why we’re excited to share our insights on the best vibe coding tools for 2025.## What Makes a Coding Tool Have \"Good Vibes\"?Before we jump into the tools themselves, let\\\\\\'s talk about what we actually mean by \"good vibes\" in a development environment. It\\\\\\'s not just about pretty colors or trendy interfaces (though those don\\\\\\'t hurt).**Smooth Performance That Doesn\\\\\\'t Get in Your Way:** You know that feeling when you\\\\\\'re in the zone and your editor starts lagging? Total vibe killer. Great tools respond instantly to your input, handle large files without breaking a sweat, and never make you wait around while they \"think.\"**Intuitive Design That Feels Natural:** The best coding tools feel like they were designed by someone who actually codes. They put the most important features right where you expect them, use shortcuts that make sense, and don\\\\\\'t make you hunt through menus for basic functionality.**Smart Assistance Without Being Pushy:** Modern tools are getting really good at predicting what you want to do next. The good ones offer helpful suggestions and automate tedious tasks. The great ones do it without interrupting your flow or making you feel like the computer is trying to do your job for you.**Beautiful Aesthetics That Reduce Eye Strain:** Look, we\\\\\\'re not being shallow here. When you\\\\\\'re staring at code for hours, having clean typography, thoughtful color schemes, and well-organized layouts actually reduces cognitive load. Your brain doesn\\\\\\'t have to work as hard to parse what you\\\\\\'re looking at.**Strong Community and Ecosystem:** Tools with great vibes usually have passionate communities behind them. That means better documentation, more extensions, helpful tutorials, and people who actually want to help when you run into problems.**Flexibility to Match Your Workflow:** Everyone codes differently. Some people love keyboard shortcuts, others prefer clicking around. Some want everything in one window, others like to spread things across multiple monitors. The best tools adapt to how you work, not the other way around.**Minimal Setup Friction:** Nothing kills enthusiasm faster than spending three hours trying to configure a tool before you can write your first line of code. Great tools either work beautifully out of the box or make customization genuinely enjoyable.This combination of factors creates what developers often describe as \"flow state\"—that magical zone where you\\\\\\'re completely absorbed in solving problems and everything just works. That\\\\\\'s what we\\\\\\'re looking for in our tool selection.## Top 10 Best Vibe Coding Tools### 1. !If we\\\\\\'re talking about tools that completely change how coding feels, Cursor has to top the list. This isn\\\\\\'t just another editor with AI bolted on; it\\\\\\'s been built from the ground up around the idea of AI-human collaboration.What makes Cursor special is how naturally the AI assistance flows into your regular coding process. Instead of feeling like you\\\\\\'re constantly asking a chatbot for help, it\\\\\\'s more like having a really smart pair programming partner who knows exactly what you\\\\\\'re trying to build. Cursor IDE has emerged as particularly noteworthy, making waves as one of the best AI coding tools available due to its superior auto-completion features and integration capabilities.#### Key Features:- Context-aware code completion that actually understands your project- Natural language to code conversion that works surprisingly well- Built-in chat interface for explaining complex code sections- Multi-file editing with AI understanding the relationships between files- Beautiful, clean interface that stays out of your way**Why It Has Great Vibes:** Cursor feels like coding in the future. The AI suggestions are smart enough to be helpful but not so aggressive that they interrupt your thought process. Plus, the interface is clean and modern without being distracting.### 2. !VS Code basically won the editor wars, and for good reason. Visual Studio Code continues to dominate the IDE landscape, establishing itself as the cornerstone development environment regardless of programming language, whether JavaScript, Python, or C++.What\\\\\\'s amazing about VS Code isn\\\\\\'t just that it\\\\\\'s free and powerful; it\\\\\\'s that Microsoft somehow made it feel lightweight and fast despite having more features than most full IDEs. The extension ecosystem is incredible, which means you can customize it to feel exactly right for any type of project.#### Key Features:- Massive extension marketplace covering every language and framework- Integrated terminal that actually works well- Git integration that makes version control painless- IntelliSense that\\\\\\'s smart without being annoying- Cross-platform consistency**Why It Has Great Vibes:** VS Code strikes the perfect balance between power and simplicity. It\\\\\\'s familiar enough that you can jump right in, but deep enough that you\\\\\\'ll keep discovering new features months later. The community support means there\\\\\\'s always someone who\\\\\\'s solved whatever problem you\\\\\\'re facing.### 3. !GitHub Copilot isn\\\\\\'t an editor itself, but it transforms whatever editor you\\\\\\'re using. According to , GitHub Copilot stands as the second most popular AI coding tool, having been tried by 40% of developers and regularly used by 26% of respondents.What\\\\\\'s wild about Copilot is how it actually learns your coding style over time. It\\\\\\'s not just suggesting random code snippets—it\\\\\\'s suggesting code that looks like something you would write, just faster than you could type it.#### Key Features:- Real-time code suggestions as you type- Support for dozens of programming languages- Context awareness that spans multiple files- Integration with the most popular editors- Comment-to-code generation**Why It Has Great Vibes:** Copilot feels like having a coding buddy who\\\\\\'s really good at remembering syntax and common patterns. It handles the tedious stuff so you can focus on the interesting problems. Plus, when it suggests exactly what you were about to type, it\\\\\\'s genuinely satisfying.### 4. !Zed is the new kid on the block, but it\\\\\\'s making waves for all the right reasons. Built in Rust, it\\\\\\'s designed from the ground up for performance and collaboration. If you\\\\\\'re tired of editors that slow down as your projects get bigger, Zed might be exactly what you need.#### Key Features:- Lightning-fast startup and response times- Real-time collaborative editing- Beautiful, minimalist interface- Built-in AI assistance- Memory-efficient architecture**Why It Has Great Vibes:** Zed feels incredibly snappy and responsive. There\\\\\\'s something satisfying about an editor that never makes you wait. The collaborative features are particularly smooth—you can literally watch your teammates code in real-time without any of the lag or sync issues you get with other tools.### 5.  (via Command Line)!According to , the best AI developer tools in 2025 include Aider, Cursor, Zed, Claude Code, Windsurf, and GitHub Copilot, representing the cutting edge of AI-assisted development.Claude Code brings AI assistance directly to your terminal, which is perfect if you\\\\\\'re someone who lives in the command line. It\\\\\\'s particularly good at understanding complex codebases and making suggestions that actually make sense in context.#### Key Features:- Terminal-native AI assistance- Deep understanding of project structure- Excellent at refactoring and code optimization- Natural language query capabilities- Git integration for tracking changes**Why It Has Great Vibes:** If you\\\\\\'re a terminal person, Claude Code feels like having a really smart assistant who speaks your language. It doesn\\\\\\'t try to force you into a GUI—it works within the workflow you already love.### 6. !Windsurf represents the next generation of AI-integrated development environments. It\\\\\\'s designed around the principle that AI and human developers should work together seamlessly, not as separate entities.#### Key Features:- Deep AI integration across all features- Intelligent project understanding- Advanced debugging assistance- Collaborative AI agent capabilities- Modern, intuitive interface**Why It Has Great Vibes:** Windsurf feels like what development environments will look like in five years. The AI integration is so smooth that it doesn\\\\\\'t feel like you\\\\\\'re using an AI tool—it just feels like you\\\\\\'ve gotten really good at coding really fast.### 7. !Don\\\\\\'t sleep on Sublime Text. According to , Sublime Text maintains its reputation for exceptional performance, known for quick response times and the ability to handle large files with ease, featuring innovative multi-caret editing capabilities that allow developers to edit multiple lines simultaneously.While everyone else has been adding AI and collaborative features, Sublime Text has been perfecting the core editing experience. Sometimes you just want an editor that\\\\\\'s incredibly fast, looks beautiful, and never gets in your way.#### Key Features:- Incredibly fast performance, even with huge files- Multi-caret editing for complex refactoring- Clean, distraction-free interface- Powerful search and replace capabilities- Minimal memory usage**Why It Has Great Vibes:** Sublime Text feels like a precision instrument. Every interaction is crisp and immediate. It\\\\\\'s the editor equivalent of a really good mechanical keyboard: simple, reliable, and satisfying to use.### 8. !Aider is a command-line AI coding assistant that\\\\\\'s particularly good at making changes to existing codebases. If you work with large, complex projects and need AI help that actually understands the bigger picture, Aider is worth checking out.#### Key Features:- Git-aware AI assistance- Excellent at understanding large codebases- Command-line focused workflow- Automated testing integration- Smart refactoring capabilities**Why It Has Great Vibes:** Aider respects your existing workflow while making it significantly more powerful. It doesn\\\\\\'t try to change how you work—it just makes you better at what you\\\\\\'re already doing.### 9. !Replit has transformed from a simple online coding playground into a full-featured development environment. What makes it special is how quickly you can go from idea to running code, especially for collaborative projects.#### Key Features:- Zero-setup development environment- Real-time collaboration features- Built-in deployment and hosting- AI-powered code assistance- Educational-friendly features**Why It Has Great Vibes:** Replit removes all the friction from getting started. You can literally go from having an idea to sharing a working prototype in minutes. It\\\\\\'s particularly great for experimenting with new technologies or collaborating with people who might not have development environments set up.### 10. !Tabnine focuses specifically on AI-powered code completion, and it\\\\\\'s really good at it. What sets it apart is the focus on privacy and the ability to train on your own codebase to provide more relevant suggestions.#### Key Features:- Privacy-focused AI completion- Team training capabilities- Multi-language support- Integration with the most popular editors- On-premises deployment options**Why It Has Great Vibes:** Tabnine feels thoughtful and respectful of your work. The privacy focus means you can use it on sensitive projects, and the team training features help it get better at suggesting code that matches your team\\\\\\'s style and standards.## Productivity Enhancement Features That MatterBeyond the core editing experience, the tools that have the best vibes integrate seamlessly with the rest of your development workflow. According to , IDEs substantially boost developer productivity by consolidating all necessary coding tools into one cohesive workspace, providing access to features like built-in error detection and version control integration rather than requiring manual coding in simple text editors.**Version Control Integration:** The best coding tools make Git feel natural instead of scary. They show you what\\\\\\'s changed, make it easy to stage and commit files, and help you resolve merge conflicts without wanting to throw your computer out the window.**Project Management Connections:** According to , popular project and task management tools that enhance the coding experience include Jira, Trello, Asana, and Microsoft Planner, while continuous integration and deployment tools have become essential components of modern software development practices. When your editor can show you which issues you\\\\\\'re working on or automatically update task status based on your commits, it creates a much smoother workflow.**CI/CD Pipeline Integration:** Nothing kills momentum like having to switch between your editor and a web browser to check if your build passed. The best tools give you build status right in your editor and make it easy to see what went wrong when something breaks.**Real-Time Collaboration:** Whether you\\\\\\'re pair programming or just need to show someone a bug, tools that make collaboration feel natural rather than awkward create much better team experiences.## The Future of Vibe-Focused Development ToolsWe\\\\\\'re in the middle of a major shift in how development tools work. According to , product innovation in the software development tools market continues to expand rapidly, with nearly 57% of newly introduced tools now supporting low-code or no-code development approaches.AI integration is becoming table stakes, but the focus is shifting from \"AI that can code\" to \"AI that helps you code better.\" We\\\\\\'re seeing tools that understand not just syntax but intent, that can refactor entire codebases while maintaining your coding style, and that can explain complex code in plain English.The developer community is also driving innovation in ways we haven\\\\\\'t seen before. According to , which received over 49,000 responses from 177 countries, provides crucial insights into the evolving developer landscape. The survey focuses on 314 different technologies, with new emphasis on AI agent tools, LLMs, and community platforms, indicating where the industry is heading.Visual design and user experience are becoming just as important as functionality. Developers are no longer willing to use tools that feel clunky or outdated, even if they\\\\\\'re technically powerful.## Choose Your Coding VibeHere\\\\\\'s the thing about coding tools—the \"best\" one is the one that fits how you work and think. If you love keyboard shortcuts and never want to touch a mouse, Sublime Text or Vim-style editors might be your jam. If you\\\\\\'re working on a team and need seamless collaboration, Cursor or Zed could be game-changers. If you\\\\\\'re just getting started and want something that works well out of the box, VS Code is still hard to beat.The good news is that most of these tools are free or have generous trial periods, so you can actually try them out and see what clicks. Don\\\\\\'t just read reviews—actually use them for a few days on real projects. You\\\\\\'ll know pretty quickly which ones feel right.The software development industry continues to grow at an incredible pace, with the developer community expanding from 27 million to a projected  in the next decade. As this community grows, the demand for tools that not only function well but actually make coding enjoyable will only increase.Remember, you\\\\\\'re going to spend thousands of hours with whatever tools you choose. Pick ones that make those hours feel productive, creative, and maybe even fun. The best coding tool is the one that gets out of your way and lets you focus on building amazing things.Try a few of these tools this week. Set up Cursor for your next AI-assisted project, give Zed a spin if you want something blazingly fast, or explore how GitHub Copilot could speed up your current workflow. The future of coding is here, and it feels pretty good.**Ready to level up your development experience?** At , we understand that great software comes from developers who have access to the best tools and environments. Our nearshore development teams use cutting-edge tools and AI-powered workflows to build intelligent applications that drive real business results. Whether you\\\\\\'re looking to augment your existing team or build something completely new, we\\\\\\'d love to show you how the right combination of talent and tools can transform your development process.\\\\u200d![]( the Author:ML/AI & Backend EngineerGuillermo Germade, Data Science Expert at Azumo, specializes in building machine learning models and AI systems, focusing on consumer tech, entertainment, and big data. reCAPTCHASelect all images with **cars** Click verify once there are none left|     |     |     || --- | --- | --- || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( |Please try again.Please select all matching images.Please also check the new images.Please select around the object, or reload if there are none.Verify\\'}, {\\'title\\': \\'Top 10 Vibe Coding Tools That Feel Like Magic in 2025\\', \\'url\\': \\'https://dev.to/therealmrmumba/top-10-vibe-coding-tools-that-feel-like-magic-in-2025-1md\\', \\'markdown\\': \\'AI is no longer just assisting developers—it’s transforming how we build software. Whether you\\\\\\'re a solo indie hacker, a startup engineer, or part of an enterprise team, there’s now an AI tool that aligns with your workflow. The best ones don’t simply automate tasks; they feel intuitive, context-aware, and remarkably efficient.* * *## Why These Tools Matter in 2025Software development has evolved. We’ve moved past the era of simple code autocomplete and template generators. Today, developers demand tools that integrate deeply with their tech stack, adapt to their workflow, and help them ship faster without compromising quality.Here are three key trends that shape this new landscape:1. **Autonomous Assistance** – Developers now rely on AI tools that can complete multi-step tasks with minimal guidance, from writing modules to fixing bugs.2. **Context-Aware Coding** – Tools that understand your full codebase, API structure, and data flow are no longer a luxury—they’re a necessity.3. **Real-Time Collaboration** – AI is becoming a collaborative partner in the development process, offering suggestions, generating code, and even reviewing pull requests in real-time.]( list focuses on tools that exemplify these traits and offer genuine improvements to the way developers work in 2025.* * *## The Top 10 Vibe Coding Tools in 2025### 1. Apidog MCP Server**The missing link between your API and AI coding assistants** bridges the gap between your backend and tools like Cursor, Cline, and Copilot Workspace. Once your OpenAPI definitions are connected, you can:- Autogenerate API logic and DTOs based on updated schema- Let AI tools access real-time API documentation for better suggestions- Streamline backend/frontend development across large teams]( especially powerful in teams that maintain frequently changing APIs or use domain-driven design.* * *### 2. Cursor]( code editor designed for pair programming with GPT-4** is built on VS Code but reimagines the editing experience with AI at the center. Unlike plugins, Cursor deeply integrates AI into every layer:- Offers context-aware autocomplete that understands your full project- Supports inline debugging and refactoring with natural language prompts- Can explain, rewrite, or extend code based on your cursor positionIt’s ideal for fast iteration, onboarding into unfamiliar codebases, or accelerating solo projects.* * *### 3. Copilot Workspace]( task-oriented Copilot that tackles full development cycles**Copilot Workspace is a major leap from GitHub Copilot. Rather than suggesting code line by line, it can:- Understand a GitHub issue and break it into subtasks- Write full PRs for those subtasks- Summarize what it did and wait for your feedbackThis makes it perfect for junior developers, product engineers, or teams with many open tickets to manage.* * *### 4. Sweep]( that turns GitHub issues into functioning pull requests** is a bot that integrates directly into your GitHub repo. You describe an issue, and it:- Scans the entire repo to understand the current architecture- Writes a PR that resolves the issue- Updates associated files and testsFor lean teams or solo founders managing multiple products, Sweep can save hours of context-switching.* * *### 5.Fine by Fine.dev]( is the AI assistant that startup CTOs have been dreaming of. This AI-powered development platform automates tasks throughout the software development lifecycle, from coding and debugging to testing and code review.What makes Fine special is its ability to work autonomously—it can create and implement code, conduct pull request reviews, generate tests, and handle common issues without constant human input. The GitHub integration and support for asynchronous work make it particularly suitable for distributed teams.* * *### 6. Continue]( with your codebase in any editor** is an open-source extension that works in VS Code and JetBrains. It allows you to:- Chat with your entire codebase- Ask for refactors, explanations, or generation tasks- Use local models or connect with remote LLMs like Claude, GPT-4, or MistralIt’s the go-to for developers who want full AI capabilities without being locked into a closed ecosystem.* * *### 7. The Windsurf Editor by Codeium]( to Keep you in flow state** is revolutionizing the IDE concept by seamlessly integrating AI as a collaborator rather than just a tool. The platform offers a unique blend of AI copilots and autonomous agents that work together with developers to enhance productivity.The Cascade feature, which provides deep contextual awareness across your codebase, has helped me understand and navigate unfamiliar codebases much faster than traditional methods. The multi-file editing and predictive actions keep me in a constant flow state, which has noticeably improved my productivity.* * *### 8. Lazy AI]( app creation with AI-powered workflows** makes building applications almost effortless. Designed for speed and simplicity, it lets users create full-stack apps without writing code. Its standout features include:- Pre-built workflows for common developer and automation tasks- One-click deployment for both frontend and backend projects- Customizable templates for bots, AI tools, dev utilities, and marketing apps- Minimal skill requirement—ideal for rapid prototyping or solo makersIt’s the perfect fit for anyone looking to bring ideas to life without the overhead of traditional development.* * *### 9. Devika**Your open-source AI software engineer** is an open-source autonomous AI agent that takes high-level instructions and breaks them down into actionable development tasks. Designed to behave like a junior developer, it combines:- Advanced planning and reasoning algorithms- Contextual keyword extraction for precision- Dynamic agent tracking and decision-making- Web browsing to supplement gaps in knowledgeDevika stands out as a community-driven alternative to commercial AI coding platforms. It’s ideal for developers looking to delegate complex coding work while retaining full control and transparency over the process.* * *### 10. Cody by Sourcegraph]( code understanding with full repo context** is designed to:- Answer questions across massive codebases- Perform intelligent refactors based on architecture- Search, navigate, and explain with precisionIts integration with Sourcegraph gives it superpowers in monorepos, enterprise codebases, and multi-language projects.### AI Coding Tools in Practice: Use Case ScenariosTo better understand how these AI coding tools can be applied in real-world settings, here are a few practical scenarios that demonstrate their potential across different developer environments:#### **Solo Developer: Boosting Productivity with Cursor + Continue**Independent developers often juggle multiple roles—from writing code to debugging and managing small deployments. Tools like **Cursor** (an AI-native code editor) combined with **Continue** (a Copilot-like extension that integrates into IDEs) offer a significant productivity boost. Together, they streamline code suggestions, automate refactoring, and provide quick answers without leaving the editor, allowing solo devs to maintain momentum and ship faster.#### **Startup Team: Automating Ticket Resolutions with Sweep + Copilot Workspace**Early-stage startups typically operate with lean engineering teams that need to move fast. Tools like **Sweep**, which auto-generates pull requests from GitHub issues, and **Copilot Workspace**, which can create and manage entire features autonomously, allow small teams to maintain high output. These tools handle boilerplate tasks and bug fixes, freeing up developers to focus on high-impact product features.#### **Enterprise: Using Cody + Tabnine for Large, Secure Codebases**For enterprises with massive codebases and stricter security requirements, **Cody** (from Sourcegraph) and **Tabnine** offer scalable AI integration. Cody excels at understanding large repositories and referencing documentation or historical code changes. Tabnine, known for its privacy-first model, ensures that proprietary code doesn’t leave company boundaries, making it ideal for secure enterprise environments.#### **Data Scientist: Speeding Up EDA with Cogram + CodeSquire**Data scientists benefit greatly from tools that can reduce time spent on repetitive analysis. **Cogram** helps generate SQL queries and Jupyter code based on natural language, while **CodeSquire** personalizes code suggestions based on the user’s unique style and workflows. These tools accelerate exploratory data analysis (EDA) and modeling, letting scientists focus more on insights and less on syntax.* * *## Final Thoughts2025 isn’t about whether you use AI as a developer—it’s about how. The tools listed above aren’t just trendy—they’re reshaping the development experience by embedding intelligence into your daily workflow. Some extend your capabilities. Others remove tedious work. The best ones do both.Choose the ones that match your stack, integrate with your workflow, and make you feel like a better developer—not just a faster one.* * *Sentry]( PromotedDropdown menu- - * * *- ]( Read More!Templates let you quickly answer FAQs or store snippets for re-use.SubmitPreview CollapseExpand]( David]( David]( David![]( of Postiz, an open-source social media scheduling tool.Running Gitroom, the best place to learn how to grow open-source tools.- EducationDidn\\\\\\'t finish high school :(- PronounsNev/Nevo- WorkOSS Chief @ Gitroom- JoinedFeb 23, 2022• Dropdown menu- - Hide- Neat how coding is getting easier, like having a little helper doing chores for you. But if AI keeps taking on bigger parts of the job, what skills should people still work on to stay sharp in tech?CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Totally! As AI handles more, staying sharp in problem-solving, system design, and critical thinking is key. Knowing why things work still sets you apart.CollapseExpand]( Sečkár]( Sečkár]( Sečkár![]( am from Czechia, born in 1988. Studied computer science at KIT VŠE, Prague. Currently working as senior Java & JS developer for Eviden.- Email- LocationPrague, Czechia- EducationKIT VŠE, Prague- PronounsHe- WorkSenior application developer @ Eviden- JoinedOct 21, 2023• Dropdown menu- - Hide- business analysis, software architecture, observability, scalability, security...to name a fewCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- And to continue on the reply of , convert abstract requirements to good software requirements is still key, AI can only hallucinate solutions that most likely be wrong or wrongly translated.Software architecture and applying best practices is also still important, AI only generate some code which might not scale very well. Or doesn\\\\\\'t fit the overal design, or worse it\\\\\\'s using deprecated functionality. So a human developer should be really keen on it and not just accept all the code. How well it even might look like. It\\\\\\'s still just a tool.CollapseExpand]( Lopez]( LopezAudrey Lopez]( JoinedMar 6, 2025• Dropdown menu- - Hide- Nice post, Emmanuel Mumba!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Thanks Lopez! Glad you enjoyed it.CollapseExpand]( Sebastian]( SebastianRalph Sebastian]( JoinedAug 10, 2023• Dropdown menu- - Hide- Nice list. By the way, Windsurf has free GPT 4.1 access for now (it will not last long), worth trying out.CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Oh nice, thanks for the heads-up! I’ll definitely check Windsurf out before the window closes.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Openai is indeed delivering free gpt 4.1 while in preview.But also o4-mini models I believe. Mini-high, Etc.CollapseExpand]( Lee]( LeeJennie Lee]( JoinedDec 18, 2024• Dropdown menu- - Hide- OK, vibe coding is getting hot this time!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Haha, for real! 🔥 Coding’s definitely having its moment, More like the vibe is leveling up every week.CollapseExpand]( the Coder]( the CoderJosh the Coder]( Coder that is Interested in AI, No Code. Learning forever!- JoinedNov 27, 2023• Dropdown menu- - Hide- OK, this is a good list. I am using Cursor but also willing to explore some alternatives. Good work Emmanuel!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Appreciate that! 🙌 Cursor’s solid, always cool to explore what else is out there too. Let me know if you find any gems.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Windsurf is really good. Since they at least providea decent deb file setup, causing less issues on my linux setup than the stupid appimages from cursor.CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- I\\\\\\'ve spent more time trying to fix our juniors vibe coded problems than I can imagine. And what\\\\\\'s worse is having to explain why. They have no idea what the code they are pumping out does. It\\\\\\'s a horrifying trend. Hopefully this doesn\\\\\\'t spread to all industries and we end up with vibe built airplanes LOLCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Haha I was already afraid this will happen 2 years ago. And now we are at this stage where agents are commonly available and used in editors.It\\\\\\'s powerful for experienced devs. But juniors should really not just using it and blindly accept it. That is not how you grow in your knowledge or carrière. Instead of you fixing their issues, they should take full responsibility of the code they put out for review.CollapseExpand]( Orion]( OrionAshe Orion]( JoinedAug 12, 2024• Dropdown menu- - Hide- Okay so as a developer in the making, at what point in my learning process will these tools stop being a hindrance to my education and start being actual real usable tools and not like Nintendo 64 cheat codes because right now, having nothing but HTML CSS and some very intermediate vanilla JavaScript at my disposal (and only as it relates to the front-end. Getting at back end soon) even just the Codium Auto-complete in VS was detracting from my ability to build a solid foundation by typing through my full lines of code. I noticed that when I had auto complete enabled, pretty much immediately anytime I was working in other environments without an autocomplete buddy by my side my syntax absolutely suffered because my monkey brain wasn\\\\\\'t getting the muscle memory of typing out the proper syntax and to be honest it was a little embarrassing in Zoom meetings with my tutors to just sit there and fumble multiple times on things like spacing or back ticks or, dare I out myself, neglected line termination in ; and }However I definitely see the value of not having to type out very obvious return statements or whatever other repetitive this.thing = thething; -esque scenarios that when you add them all up really do burn giant holes your productive work hours. I\\\\\\'m just so terrified of using it too early, leaving the safety of being an \\\\\\'aspiring engineer\\\\\\' for the reality of the job market, finally landing an interview only to realize that while I thought I was learning to code I\\\\\\'d just been surfing the vibe.CollapseExpand]( Enlightened]( EnlightenedAaron Enlightened]( Developer using AI tools for programming but not depending on them completely!- JoinedApr 18, 2025• Dropdown menu- - Hide- LOL ... I know exactly how you feel! I am at a similar junction in my programming journey and this is what I do to not feel like an imposter later on - Open a plain .txt file and just type some HTML and CSS code (along with vanilla JavaScript if you want to) into it, then save the file as \"index.html\", and then open this file in Google Chrome browser and see the result. To modify or edit any code that you feel is giving unexpected results, just use the Chrome Developer tools. This will give you confidence in YOUR coding prowess with ZERO autocomplete using any type of AI. Do this for just 15-20 minutes every day for 1 month and you will have solved the imposter syndrome to a large extent.For all the rest of your tutorials use the autocomplete feature on VSCode Editor, but try to ask ChatGPT \"why\" was some particular code snippet suggested by the AI autocomplete. This will help you in understanding the WHY of AI coding and will help make you a better programmer.BTW, I signed up on DEV just to post this comment. So, thank you for that. ;-)CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- It\\\\\\'s a tough gig for new devs, you are entering a world where tools exist that actually hinder your understanding of what\\\\\\'s going on.At the risk of showing my age, id say it\\\\\\'s akin to why the monk will make the initiate carry pales of water to then monastery before he learns the king fu. To generate an understanding of the fundamentals of movement, patience and discipline.Some people a lot older than me would say using an IDE is not true development and that real devs can code in notepad.Tools evolve and as long as they fill the purpose of making coding easier, but still allow you to learn and gain the knowledge and understanding I wouldn\\\\\\'t see a problem with using them. But as you pointed out, it can be a hindrance, and you actually realizing that and accepting it means you will succeed where many of your peers won\\\\\\'t.AI is just another tool, it\\\\\\'s great when you already have an understanding of how systems should work and just want some snippets. But the problem we are seeing is a misuse of these tools. The blind copy paste vibe coding is not creating developers it\\\\\\'s creating slop.Here\\\\\\'s my ultimate advice though, AI doesn\\\\\\'t have to be just a tool, it can be an amazing teacher. It can explain things to you in anyway you want , if you ask it to simplify and breakdown topics it will. It can explain things to you as if you were a toddler. USE THIS.Vibe code your heart out but every single time it gives you code you do not understand, ask it to explain it you. You will be amazed how quickly it goes from hindrance to game changing.CollapseExpand]( jones]( jonessam jones]( JoinedApr 16, 2025• Dropdown menu- - Hide- Very informative article. Although not at all what I expected from title \"Vibe Coding\". In industry, Vibe Coding is being used to signify low code/no code or citizen development. Basically, the same holy grail that the industry has been searching for since Oracle Forms or earlier.BUt putting the Title aside, looking forward to trying out some of your suggestions.CollapseExpand]( Mottley]( MottleyDaley Mottley]( Consultant | Full Stack Web Developer- Email- LocationBarbados- JoinedOct 8, 2024• Dropdown menu- - Hide- Nice post, Emmanuel!Have you tried ?It\\\\\\'s a pretty decent vibe coding tool. I love it... 😅Thanks for sharing 👍CollapseExpand]( Green]( GreenStuart Green]( JoinedApr 18, 2025• Dropdown menu- - Hide- Crazy times ahead. Can only imagine the downstream impact these tools will have (personalized software, rise of individual creators, etc).I also love platforms like  and  for \"vibe coding\" without the actual code. Though harder to build more advanced features than with Cursor, etc.Some comments may only be visible to logged-in visitors.  to view all comments.Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\\\\\'s .Hide child comments as wellConfirmFor further actions, you may consider blocking this person and/or Embeddable]( PromotedDropdown menu- - * * *- ]( Embed in minutes, load in milliseconds, extend infinitely. Import any chart, connect to any database, embed anywhere. Scale elegantly, monitor effortlessly, CI/CD & version control.👋 Kindness is contagiousDropdown menu- - * * *- CloseDive into this thoughtful piece, beloved in the supportive DEV Community. **Coders of every background** are invited to share and elevate our collective know-how.A sincere \"thank you\" can brighten someone\\\\\\'s day—leave your appreciation below!On DEV, **sharing knowledge smooths our journey** and tightens our community bonds. Enjoyed this? A quick thank you to the author is hugely appreciated.### !We\\\\\\'re a place where coders share, stay up-to-date and grow their careers. ![](\\'}]', name='web_search_tool', call_id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 8, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_S9VoFTZt8NefRPNRbUH4tZQg', is_error=True)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(id='f9eff205-1de3-4c5c-b122-c44dfdaa46b1', source='research_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 33, 30, 910052, tzinfo=datetime.timezone.utc), content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 7, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\\n[{\\'title\\': \\'Affective Software Engineering\\', \\'url\\': \\'https://affectivese.org/\\', \\'markdown\\': \"!We are a group of computer scientists whose research focuses on using Affective Computing methods in the field of Software Engineering. Our interests include, but are not limited to, the role of emotions in the software development process, reusing datasets from Affective Computing experiments as well as improving the teaching process of future software engineers.We are open to any kind of cooperation. Do you want to do research together? Prepare a paper? Submit a project proposal? If you are conducting research at the intersection of the fields of affective computing and software engineering feel free to contact us via . Take a look at our current research and projects and get in touch!### DevEmo datasetThe DevEmo dataset consists of 217 video clips showing students\\' facial expressions as they solve programming tasks. The recordings were collected in the participants\\' actual work environment. The video clips are annotated with five labels, including four emotions (anger, confusion, happiness and surprise) and a neutral state.In order to facilitate the annotation of a large number of video recordings, we have developed AffecTube. It is a comprehensive solution that leverages crowdsourcing to annotate videos directly on the YouTube platform, resulting in ready-to-use emotion-annotated datasets. AffecTube provides a low-resource environment with an intuitive interface and customizable options, making it a versatile tool applicable not only to emotion annotation, but also to various video-based behavioral annotation processes.If you are interested in using the DevEmo dataset or AffecTube check out the projects\\' websites:Journal papers:### Affective data reausabilityModern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration.In the research conducted so far, we have proposed the ROAD (Recording Ontology for Affective-related Datasets) ontology and its implementation with the graph knowledge base in the form of the GRISERA framework.If you are interested in our approach take a look at:Journal papers:### Emotions in the software development processEmotions play an important role in any human being\\'s life, including software developers. The aim of our research is to determine what emotions are optimal in the software development process, how to recognize them and how (if at all) to influence them.Publications:### Affect-aware software engineering educationWith the Covid-19, the role of distance learning has increased. The aim of the research is to develop methods that, using emotion recognition techniques, will allow for more effective remote learning of programming.Publications:### Affective loop in Socially Assistive Robotics as an intervention tool for children with autismEMBOA project entitled “Affective loop in Socially Assistive Robotics as an intervention tool for children with autism” is a research and didactic project that aims at enhancement of social robot intervention in children with autism with affective computing technologies. The project is implemented in years 2019-2022 under EU Erasmus Plus Strategic Partnership for Higher Education Programme.More information on project website: Wrobel MR., Landowska A., Makuch K., , Proceedings of the 32nd International Conference on Information Systems Development (ISD 2024), 2024Barkana DE., Bartl-Pokorny KD., Kose H, Landowska A., Milling M., Robins B., Schuller BW., Uluer P., Wrobel MR., Zorcec T., , International Journal of Social Robotics, 2024Kulas D., Wrobel MR., , SoftwareX, 2023Manikowska M., Sadowski D., Sowinski A., Wrobel MR, , Applied Sciences, 2023Zawadzka T, Wierciński T, Waloszek W, Wróbel MR, , IEEE Access, 2023Zawadzka T, Wierciński T, Meller G, Rock M, Zwierzycki R, Wróbel MR, , Sensors, 2021Zawadzka T, Waloszek W, Karpus A, Zapałowska S, Wróbel MR, , IEEE Access , 2021Bartl-Pokorny KD, Pykala M, Uluer P, Barkana DE, Baird A, Kose H, Zorcec T, Robins B, Schuller BW, Landowska A, , IEEE Access , 2021Wróbel MR, , IEEE Access , 2020Blincoe K, Springer O, Wrobel MR, , IEEE Software, 2019Wrobel MR, Zielke AW, , 2018 Federated Conference on Computer Science and Information Systems (FedCSIS), 2018Wrobel MR, , Applied Sciences, 2018Landowska A, Brodny G, , IEEE Access, 2018Landowska A , , Applied Sciences , 2018Wrobel MR, , 2016 Federated Conference on Computer Science and Information Systems (FedCSIS), 2016Brodny G, Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 2016 9th International Conference on Human System Interactions (HSI), 2016Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Information Systems Development and Applications, 2015Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Human-Computer Systems Interaction: Backgrounds and Applications 3, 2014Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013\"}, {\\'title\\': \\'Affective Computing: Recent Advances, Challenges, and Future ...\\', \\'url\\': \\'https://spj.science.org/doi/10.34133/icomputing.0076\\', \\'markdown\\': \\'AdvertisementMain content starts hereContents## AbstractAffective computing is a rapidly growing multidisciplinary field that encompasses computer science, engineering, psychology, neuroscience, and other related disciplines. Although the literature in this field has progressively grown and matured, the lack of a comprehensive bibliometric analysis limits the overall understanding of the theory, technical methods, and applications of affective computing. This review presents a quantitative analysis of 33,448 articles published in the period from 1997 to 2023, identifying challenges, calling attention to 10 technology trends, and outlining a blueprint for future applications. The findings reveal that the emerging forces represented by China and India are transforming the global research landscape in affective computing, injecting transformative power and fostering extensive collaborations, while emphasizing the need for more consensus regarding standard setting and ethical norms. The 5 core research themes identified via cluster analysis not only represent key areas of international interest but also indicate new research frontiers. Important trends in affective computing include the establishment of large-scale datasets, the use of both data and knowledge to drive innovation, fine-grained sentiment classification, and multimodal fusion, among others. Amid rapid iteration and technology upgrades, affective computing has great application prospects in fields such as brain–computer interfaces, empathic human–computer dialogue, assisted decision-making, and virtual reality.#### SIGN UP FOR _Intelligent Computing_ eTOC ALERTS!Get the latest SPJ table of contents delivered right to you!GET OUR E-ALERTS## IntroductionAccording to basic emotion theory, emotion is the grammar of social living and serves as a crucial means of exchanging information, maintaining relationships, and communicating ideas between individuals. Moreover, it is a fundamental psychological element that ensures basic human survival while shaping social habits and supporting advanced thinking , ]. Given its central role in numerous human intellectual activities such as perception, learning, decision-making, reasoning, and socializing, emotion is an important force driving the continuous and diverse prosperity of human civilization.The importance of emotions to human beings can be summarized in 5 crucial aspects. First, the survival function is a learned physiological response that allows individuals to adapt positively to their environment ]. Emotions play a pivotal role in strengthening the capacity to adapt to the environment by regulating attention, memory, perception, and other cognitive processes. This ensures a greater chance of survival and development during the evolutionary process. Second, the communication function highlights the importance of emotions for the accurate expression and understanding of human intentions ]. The same words spoken with different emotions carry different connotations. Thus, emotions are inseparable from natural language and are critical for semantic disambiguation. Third, emotions have a decision-making function that manifests in both fast and slow modes of thinking. The commonly used unconscious “System 1” mainly relies on emotions and experiences, while the conscious “System 2” depends on rational deliberation ]. Therefore, emotions are widely involved in higher-level thinking and decision-making processes that profoundly affect the results and efficiency of decisions. Fourth, emotions serve a motivational function in stimulating and sustaining individuals’ behaviors, thereby affecting the degree of resource input, behavioral persistence, and evaluation of outcomes ]. Finally, emotions perform a maintenance function as bonds between members of ethnic groups, families, social circles, social classes, and other groups. During human socialization, emotions serve as the core of low-cost maintenance of social relations, forming potential social interaction contracts, and are closely tied to individual moral constraints and codes of conduct , ]. Hence, the nature and functions of emotions ensure that they are inseparable from human survival and development.As the era of a human–machine symbiotic society approaches, endowing machines with emotional intelligence becomes increasingly crucial. Emotional intelligence represents a fundamental technology and an essential prerequisite for realizing naturalized and anthropomorphic human–computer interaction. It is of great value for opening up the era of intelligence and digitization. Picard is credited with being the first to propose a comprehensive definition of affective computing. In her 1997 book, _Affective computing_, she defined it as “computing that relates to, arises from, or deliberately influences emotions” ]. The goal of affective computing is to create a computing system capable of perceiving, recognizing, and understanding human emotions and responding intelligently, sensitively, and naturally, thus making human–computer interaction more natural. The epochal importance of affective computing lies in its impact on changing how emotions are perceived as abstractions within psychology, making it possible for emotions to be measured, computed, and machine-learned.Affective computing encompasses various disciplines, including computer science, engineering science, brain and psychological science, and social sciences. Computer science and engineering science focus on providing various information technology tools and engineering capabilities to enable digital reconstruction and computational realization of emotion perception, recognition, understanding, and feedback, allowing machines to possess human-like emotional and cognitive functions. The psychological and consciousness aspects of the brain and psychological sciences provide theories on the basic definition of human emotions and the structure of related elements, laying the foundation for modeling emotion theories. Cognitive neuroscience, another branch of the brain and psychological sciences, examines the emotion-processing mechanism of the human brain and establishes a functional network of psychological elements associated with emotions, providing important inspiration and strategic guidance for developing affective computing models. Social and medical sciences offer numerous opportunities for the application of affective computing and serve as a resource for designing application scenarios for such technologies.### Research in affective computingThe research content of affective computing primarily covers 5 aspects. The first aspect is the fundamental theory of emotion, which currently relies on the discrete emotion model and the dimensional emotion model from the field of psychology to define various types of emotions, ranging from basic to compound. The second aspect involves collecting emotional signals, such as text, speech, facial expressions, gestures, and physiological signals, to establish corresponding datasets. The third aspect is sentiment analysis, which utilizes machine-learning and deep-learning algorithms to model and identify emotional signals. The fourth aspect is multimodal fusion, which leverages multimodal emotional features and fusion algorithms to enhance the accuracy of emotional classification. Finally, the fifth aspect is generating and expressing emotions, processes that enable robots to express emotional states through facial expressions, voice intonation, body movements, etc., and facilitates natural, anthropomorphic, and personified human–robot interaction. Figure  illustrates the specific content and development status of these 5 aspects.![](\\\\xa01. Research content of affective computing.![](\\\\xa02. Annual scientific production on “affective computing” from 1997 January 1 to 2023 September 25.#### Basic theory of emotionThe field of affective psychology has numerous grounded theories of emotion and serves as an important source of inspiration for the development of computable emotion models. The discrete emotion model and the dimensional emotion model are the most commonly used theoretical models for artificial intelligence emotion modeling. The discrete emotion model categorizes emotions individually rather than in correlated groups, as does Ekman’s basic emotion classification model, which is based on facial expression analysis ] and comprises happiness, sadness, anger, disgust, surprise, fear, and contempt. Although the discrete emotion model is clearly defined, interpretable, easy to understand, and capable of semantically integrating vocabulary and concepts, it lacks granularity and provides a limited quantitative description of emotions. In contrast, dimensional affective models represent different emotions through multidimensional vectors in affective space. Such models include the valence–arousal affective model ] and the 3-dimensional pleasure–arousal–dominance model , ]. These models are highly quantitative, abstract, and inductive and have continuous emotional value vectors. They are suitable for handling changes in emotional states over time but are not intuitively interpretable; thus, it is difficult for machines to use them to develop rich coping strategies for emotional interactions. The selection of the model depends on the actual application tasks and scene requirements, as both discrete and dimensional emotion models have advantages and disadvantages.#### Collection of emotional signalsTo support data acquisition and the comparison of algorithms in affective computing, numerous open-source databases have been established. They contain datasets that can be categorized as textual, speech/audio, visual, physiological, or multimodal. The characteristics of these databases considerably impact model design and network architecture in affective computing.Text-based resources on various communication carriers serve as massive datasets for emotional text mining ]. Representative datasets include the internet movie database (IMDb) ], the Stanford sentiment treebank, which contains sentences from movie reviews ], and the Multi-Domain Sentiment Dataset, which contains Amazon.com product reviews ]. Speech is another crucial modality for decoding emotions in human intercommunication. Speech signals comprise both the emotional content of the speech and the emotional characteristics of the sound itself. Representative datasets include EmoDB ], the SEMAINE database ], and CSED ]. Visual-emotional signals such as body movements and facial expressions are now more convenient to gather because of low-cost sensors such as cameras and camcorders, and they do not require direct contact with the user ]. This field has vast amounts of data and many related research papers with considerable data collected directly from real-world scenarios, making it more conducive to grounded applications ]. Representative datasets include the Expression-in-the-Wild (ExpW) dataset ], AffectNet ], the Real-world Affective Faces Database (RAF-DB) ], and SMIC, a database of spontaneous microexpressions ].Physiological data have an advantage over signal data such as text, speech, and facial expressions in that they can more directly, objectively, and accurately reflect an individual’s emotional state while being less influenced by subjective consciousness , ]. Consequently, physiological data have become a research hotspot in affective computing. Commonly used physiological data in this field include electroencephalograms (EEGs), skin electricity, cardiac electricity, electromyography (EMG), eye electricity, respiration, skin temperature, and blood volume pulse. However, obtaining physiological data requires the use of complex sensors. Thus, such data are expensive and challenging to collect for use in practical applications. Consequently, the scale of physiological data used in laboratory research is generally small ]. Representative datasets include the Database for Emotion Analysis using Physiological Signals (DEAP) ], the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) ], and WESAD, a dataset for wearable stress and affect detection ].#### Sentiment analysis**Text analysis.** This method focuses on extracting, analyzing, understanding, and generating emotional information in natural language. Early text affective recognition relied mainly on manually constructed affective dictionaries and rules for affective analysis. These methods judge sentiment polarity by matching sentiment words with grammatical rules in a text , ]. However, this approach is limited by emotional lexicon coverage and rules, making it challenging to support multidomain sentiment analysis. With the advancement of machine learning, text emotion recognition methods based on statistical and machine learning algorithms have emerged. By training on large-scale text datasets, machine learning models can automatically learn emotional expression and semantic features, enhancing the accuracy and generalization ability of sentiment classification , ]. In recent years, deep-learning technology has considerably impacted text emotion recognition. Neural network-based models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, bidirectional encoder representation from transformers (BERT), and generative pre-trained transformers (GPT), have been successful in various sentiment analysis tasks – ]. They can capture contextual information and semantic relationships to better understand and analyze sentiments.**Speech analysis.** Speech emotion recognition is the process by which a computer automatically recognizes the emotional state signaled by speech. Speech contains emotional information, such as speech rate and intonation, in addition to semantic information. Speech emotion analysis combines linguistic and acoustics-related technologies to analyze the syntax, semantics, and acoustic feature information related to the speaker’s emotional state ]. This analysis mainly revolves around rhyme, spectrum, and sound quality features. The numerous acoustic features related to affective states include fundamental frequency, duration, speech rate, resonance peaks, pitch, mel-filter bank (MFB), log-frequency power coefficients (LFPC), linear predictive cepstral coefficients (LPCC), and mel-frequency cepstral coefficients (MFCC) – ]. These features are represented as fixed dimensional feature vectors, with each component representing the statistical value of each acoustic parameter, including the mean, variance, maximum or minimum value, and range of variation. Recently, the ability of neural networks to extract suitable feature parameters has received increasing attention. Deep speech emotion features are learned from speech signals or spectrograms through tasks related to speech emotion recognition. Deep speech features learned from large-scale training data are widely used as speech emotion features in speech event detection and speech emotion recognition tasks, as in the VGGish and wav2vec projects , ], for example. In recent years, algorithms such as ConvNet learning ], ConvNet-RNN ], and adversarial learning ] have considerably improved speech emotion recognition performance.**Visual analysis.** Visual emotion recognition research primarily focuses on facial expression recognition (FER) and emotional body gesture recognition. The conventional method involves feature extraction followed by classification. Typically, handcrafted features for static image analysis include local binary pattern (LBP), histogram of oriented gradients (HOG), local phase quantization (LPQ), and Gabor features , ]. Some scholars have proposed dynamic feature extraction methods, such as LBP on three orthogonal planes (LBP-TOP) ]. Features are usually classified using pattern recognition classification methods such as K-nearest neighbors, support vector machines (SVMs), or multi-layer perceptrons (MLPs). Another approach is the feature learning approach, which combines the end-to-end training of feature representations and classifiers on a given task target, typically a combination of the entire connection layer and softmax. The feature-learning method employs features learned from big data through layer-by-layer feature transformation and can describe the intrinsic information of data better than handcrafted features. However, supervised training methods such as deep CNNs are not universal and rely on large amounts of sample data. Therefore, it is too early to abandon traditional feature-extraction methods. In visual emotion analysis, automatic training features can be extracted and integrated with traditional features, which may further improve system performance.**Physiological signal analysis.** Physiological changes that occur with emotions, including brain electrical activity, heart rate changes, electrical skin response, muscle tension, and respiration rate, are supported by mainstream theories, such as the physiological theory of emotion ] and Lange’s theory of emotion ]. By detecting changes in these physiological signals, patterns associated with emotions can be recognized and then used to develop computer systems that can automatically recognize emotions. Physiological signals are more challenging to recognize than text, speech, and facial expression signals mentioned above, and they have unique properties. For example, computing EEG data requires more complex preprocessing, including electrode position localization, bandpass filtering, reference conversion, segment analysis interception, artifact removal, and bad electrode interpolation. Researchers must have cross-field knowledge to apply machine learning or deep learning methods to recognize emotions from physiological signals ].Affective computing mainly employs peripheral nervous system (PNS) features, such as facial EMG, galvanic skin potential (GSP), photoplethysmography (PPG), heart rate variability (HRV), respiratory rate, and electrocardiogram (ECG), whereas central nervous system (CNS) features include EEG, near-infrared, and brain-imaging features. EEG features have dominated the studies published on this topic. For instance, manual feature extraction involves multidimensional feature extraction from EEG signals in the time, frequency, time–frequency, and nonlinear domains for emotion recognition and classification. Recent studies have emphasized the integrity and relevance of these features. To construct functional brain networks, many studies have started defining a channel as a node and quantifying the relationship between individual nodes using phase synchronization, inter-correlation, and mutual information, treating strength as the functional connectivity between the brain regions of the corresponding channel. Complex network measures, including efficiency, clustering coefficients, degree distribution, small-world features, and average shortest distance, are then used to extract functional brain network features. Since 2018, deep learning methods such as CNNs, RNNs, deep belief networks (DBNs), and stacked autoencoders (SAEs) – ] are being increasingly used for emotional computation of EEG data, generalizing sentiment analysis to various physiological signals.#### Multimodal fusionEarly affective computing primarily involved unimodal data analysis and emotion recognition, focusing on a single modality, such as text, speech, facial expression, body movement, or physiological signals. However, this approach fails to conform to the human perception and expression patterns of emotions and has limitations in terms of the information obtained for emotion recognition ]. Humans communicate their emotions through multiple channels, including language, tone of voice, facial expressions, and body movements. Textual, auditory, and visual information together provide more comprehensive emotional information than they do individually, just as the brain relies on multiple sensory input sources to validate events. Moreover, unimodal information is insufficient and can be easily affected by various external factors ]. Emotional signals can be disguised or affected by other signals from a single channel, for example, when facial expressions are obscured or when noise interferes with speech, resulting in a considerable reduction in emotion analysis performance. Multimodal emotion analysis considers the complementarity of emotion expression among modalities and is thus more robust and aligned with natural human behavior expression. Therefore, research on multimodal fusion of affective computation has received increasing attention. Multimodal fusion algorithms integrate information from different modalities into a stable multimodal representation, enabling comprehensive processing and coordinated optimization to identify human emotions as accurately as possible ]. Common multimodal fusion methods can be categorized into feature-, model-, and decision-layer-based fusion depending on the fusion stage ].#### Generation and expression of emotionsAffective computing enables machines to provide empathic feedback based on deep contextual understanding. Robots and other agents can deliver expressions and responses, conveying the emotional temperature to the user through facial expressions, emotional text responses, and body movements , ] by building on the results of sentiment analysis and recognition. Emotional text generation and speech synthesis are the most-studied areas of research. Emotional text generation involves the automatic generation of emotional response content that matches the message of the dialogue and is consistent with the machine’s strategy, which is chosen according to the context ]. For instance, a traffic enforcement robot may exhibit a fundamental difference in the language used for persuasion and the language used for enforcement, a difference that is crucial to obtaining effective practical traffic management results. The goal of emotional text generation is for the model to generate text that conforms to a specified sentiment category, as expressed by emotion-related keywords or techniques such as metaphors ]. Pretrained models such as GPTs are increasingly being utilized as a base for emotionally controllable text generation and achieving powerful results ]. Responding to text content with emotional color is only the first step. The generated text needs to be expressed using a related emotional voice. Emotional coding information is integrated into the speech synthesis model to make human–machine dialogue less cold and mechanical, thereby allowing individuals to perceive “machine empathy” and feel warmth and affinity. Emotional speech synthesis uses a specific voice style and combines text content with emotional tags to provide a robot or agent with a voice that expresses a particular emotion ]. This process inputs textual content and a specific voice style into a neural network that synthesizes an output voice in that style by utilizing the spectral, rhythmic, and linguistic features of human voices that express emotion.### Applications of affective computingAffective computing is a technology that advances according to the actual needs of the industry, which drives progress and iteration. To build up reliability, general applications initially focused on recreation, leisure, or serving people with urgent needs, then gradually expanded to more fields, transforming the technology and contributing to productive endeavors. In 2021, the value of affective computing reached $21.6 billion, and it is expected to double by 2024 , ]. As the industry grows, the creative applications of affective computing technologies will flourish, yielding satisfactory results in various fields.#### EducationIn the field of education, affective computing is primarily used to recognize the emotional state of learners and provide corresponding feedback and adjustment ]. For example, teachers can utilize intelligent emotional teaching systems to better understand students’ engagement levels and adjust the pace and content of their teaching to improve the learning experience. An intelligent system can recommend customized learning content based on the sentiment analysis of students’ interests. Students can provide authentic teaching feedback through intelligent systems to improve the comprehensiveness and accuracy of teaching evaluations. One advantage of an intelligent system is that it can be used in both traditional and online classrooms to strengthen the contextualization of online teaching, enhance emotional interaction between teachers and students, and improve teaching quality. Affective computing techniques are also conducive to the research and development of educational games and robots ], providing improved human–computer interaction and achieving educational objectives more effectively.#### HealthcareAffective computing research has expanded into various psychiatric disorders in the affective disorders category, such as Alzheimer’s ], Parkinson’s ], bipolar disorder ], and post-traumatic stress ], and into healthcare areas including relaxation service healthcare ] and health office systems ]. Affective computing enables the scientific and objective identification and judgment of patients’ emotions, particularly in psychological disorder treatments, providing a useful complement to more subjective traditional diagnostic tools such as behavioral observation and scale filling. Objective data collection can improve personalized and precise medical treatment ]. In addition, affective computing can be used for the initial screening and efficacy assessment of diseases. For instance, patients with social anxiety disorder exhibit important differences in emotional facial processing compared to the normal population, differences that can be identified by automated monitoring of differential features ].#### Business servicesIn marketing, where the consumer experience is highly correlated with emotions, affective computing is widely used to understand and recognize the user’s emotional state. The application of affective computing can reveal the user’s true preferences and improve and streamline the buying process ]. In the field of financial credit, affective computing technologies can be used to analyze the emotional state and moral level of a customer based on voice and tone, determine the probability of the customer lying, and provide a guide for lending decisions. In the field of stock investment, investor decisions are influenced by irrational judgments. The price trend of a stock is determined not only by a company’s fundamentals but also to a large extent by fluctuations in investor emotions. The study of investor sentiment from social media data (e.g., data from X, formerly known as Twitter) can help identify investors’ emotional preferences and cognitive biases for the purpose of predicting the direction of the stock market ].#### Integration of science and artIn the current digital era, image, audio, and video data have become plentiful and important. Extracting useful information from them and retrieving and mining them effectively are crucial. For example, in recommending music to users, resource management and audio search efficiency are essential. Traditional music search methods match content using text (e.g., song title, artist name, or lyrics). Including sentiment, a high-level semantic feature of music, improves the match between user preferences and music, thus aiding in the primary task in music sentiment analysis ]. Affective computing also empowers automated poetry generation, where deep learning methods such as RNNPG, an RNN-based poem generator, and SeqGAN, a sequence generative adversarial network, are gradually replacing Word Salada, genetic algorithms, and statistical machine translation methods – ]. Expressing emotions more richly is key in making generated poetry spiritual, i.e., in moving beyond resemblance of form to resemblance in spirit.### Importance of this studyThe field of affective computing has grown considerably and exploded in popularity in the last decade for 2 reasons: technological developments providing tools for affective computing and the growth and expansion of demand. In the era of human–machine symbiosis, the deepened human understanding of emotional connotation and the improvement of the “double quotient” (i.e., IQ + EQ) of intelligent machines will become a vital innovative force promoting the affective computing discipline, technological evolution, and industrial progress. Despite the rapid development in affective computing, a comprehensive review of research and systematic analysis of hotspots and trends is lacking. Continuous innovation in algorithmic technology, broadening application requirements, and increasing research efforts necessitate that existing research be summarized and future technological directions be identified. Doing so will enable academia and industry to better understand the development of affective computing technology, thus will facilitate affective computing research, empower applications, and benefit society.This study aims to fill the gaps in existing research through a comprehensive review of affective computing from 1997, when Picard formally proposed the concept, up to 2023. We adopted a bibliometric analysis method to accurately portray the current status of the development of the field and provide insights into present challenges and future trends. The main contributions of this study are as follows. (a) Facing the academic frontier, we list the research hotspots and trends that we identified by analyzing full-scale papers. This allows readers to quickly and comprehensively grasp the development dynamics of the field and understand key common and frontier-leading technologies. (b) Facing major needs and the main battlefield of the economy, we provide blueprints for technological development and insights into current applications. This promotes the application and transformation of affective computing, facilitating high-quality economic development and digital transformation. (c) Facing future trends, we introduce challenges and developments in the field of affective computing, along with predictions for future technology and industry application directions. This serves as a forward-looking guide to the field.## Materials and Methods### Data collectionThis study searched for papers published in affective computing from January 1997 to September 2023 in the Web of Science Core Collection (WoSCC), which includes the Science Citation Index Expanded, Social Sciences Citation Index, Arts & Humanities Citation Index, Emerging Sources Citation Index, Conference Proceedings Citation Index—Science (CPCI-S), and Conference Proceedings Citation Index—Social Sciences & Humanities (CPCI-SSH). The search strategy is summarized in Table .| Index field | Search strategy || --- | --- || Theme keywords | “affective recognition” or “mood recognition” or “affective computing” or “artificial emotional intelligence” or “emotion AI” or “expression recognition” or “emotion recognition” or “emotion learning” or “sentiment analysis” or “sentiment recognition” || Literature types | proceedings papers, articles, review articles, early access |Table\\\\xa01. Search strategy for this studyThe reason this study uses 1997 as the starting point of the timeline is that the book _Affective computing_ ], which was published in that year, is regarded as the work that established affective computing as an independent academic research field. Papers outside this time range were not included in the calculation of citation statistics. In the statistics of Chinese papers, Hong Kong, Macau, and Taiwan are included. The results show that 33,448 papers were published worldwide. Among them, 16,097 (48.13%) were conference papers and 17,351 (51.87%) were journal papers. It should be noted that the names of institutions were standardized using machine and manual methods. However, when scientists publish papers, the writing of the names of institutions is not standardized, which may have caused the omission of papers in the statistics and a deviation in the index calculation results.In addition, this study combined the following 3 databases for data acquisition: (a) Incites: This database is based on the publication date of all document types in the major index databases of the WoSCC. It performs publication count and index calculations to provide research performance analysis. (b) Essential Science Indicators (ESI): This is an in-depth analytical research tool based on the Web of Science. ESI can identify influential countries, institutions, papers, and publications, as well as the cutting-edge in a research field. (c) Journal Citation Reports (JCRs): This is a multidisciplinary journal evaluation tool that provides journal evaluation resources based on citation data statistics. By citing and counting references, the JCR can measure the influence of research at the journal level, revealing the relationships between citing and cited journals.### Data analysisStatistical analysis was performed using a bibliometric method. Bibliometrics applies quantitative methods such as mathematics and statistics to the literature of a scientific or other field and processes statistical data based on information science theory. This widely accepted approach provides quantitative analysis pathways and innovative insights into the assessment of research trends based on previous literature , ]. Unlike peer review and expert judgment, bibliometrics can provide quantitative indicators to ensure objectivity through statistical analysis of academic achievements ]. Bibliometric analysis enables monitoring and summarizes the status, hotspots, and trends of a particular topic, helping researchers identify future research directions ]. In this study, we first cleaned and analyzed the data using the Derwent Data Analyzer (DDA, version 10, Clarivate, London, UK), which is well integrated with the source data from the Web of Science platform. DDA was used for multidimensional data mining, preprocessing, standardization, and statistical analysis. Subsequently, the bibliometric analysis and knowledge visualization software tool VOSviewer (version 1.6.15, Leiden University, Leiden, Netherlands) was employed. This analysis tool provides valuable insights into the structure, advancement, and collaboration in the field of affective computing. Notably, its distinctive feature lies in the graphical representation of bibliometric maps, which is particularly suitable for large-scale data analysis ]. VOSviewer was used to visualize the data in this study.## Results### Publication trendsFrom 1997 to 2009, the number of articles published in this field steadily increased, exhibiting an overall growth trend despite occasional fluctuations (Fig.\\\\xa02). From 2010 to 2019, with the rise of deep learning, a rapid development was observed in the field of affective computing, and the number of articles published in the field rose rapidly, indicating an explosive growth stage of research. After 2019, because of a plateau in the innovation of deep learning methods and the impact of the coronavirus disease 2019 (COVID-19) pandemic on academia, research in the field of affective computing also reached a plateau, and the rising trend slowed down.### Comparison of countriesTo analyze the main research positions in the field of affective computing, the country/region fields of all the authors and the first author of the paper were counted. As shown in Table , among the top 20 countries with publications in the field of affective computing, China is the country with the largest number of publications, accounting for 26.2% of all authors and 24.6% of first authors. China, the United States, India, the United Kingdom, and Germany rank among the top 5 in the number of papers published counting all authors or first author, and are the most important in terms of research in the field of affective computing. The United States ranks second in the number of papers published counting all authors, but third in the number of papers published counting only first author, after India.| No. | Country | Number of papers | Country | Number of papers || --- | --- | --- | --- | --- || (All authors) | (All authors) | (First author) | (First author) || --- | --- | --- | --- || 1 | China | 8,780 | China | 8,223 || 2 | USA | 4,715 | India | 3,632 || 3 | India | 3,829 | USA | 3,274 || 4 | UK | 2,535 | UK | 1,432 || 5 | Germany | 1,706 | Germany | 1,253 || 6 | Japan | 1,321 | Italy | 1,022 || 7 | Italy | 1,302 | Japan | 977 || 8 | Australia | 1,234 | South Korea | 931 || 9 | Spain | 1,178 | Spain | 862 || 10 | South Korea | 1,121 | Australia | 788 || 11 | Canada | 1,100 | Canada | 720 || 12 | France | 943 | France | 587 || 13 | Netherlands | 778 | Turkey | 581 || 14 | Saudi Arabia | 765 | Netherlands | 484 || 15 | Turkey | 691 | Malaysia | 479 || 16 | Singapore | 640 | Pakistan | 454 || 17 | Malaysia | 609 | Brazil | 443 || 18 | Pakistan | 595 | Greece | 413 || 19 | Brazil | 522 | Iran | 398 || 20 | Greece | 483 | Singapore | 394 |Table\\\\xa02. The top 20 countries in the field of affective computingIn addition to the 2-year step in 2021–2022, a 4-year step was used to count the publication volume of the top 10 countries in the field of affective computing. The results are shown in Fig. . Given that the concept of “affective computing” originated in the United States, which has been a major research force in this field, we chose the United States as the benchmark. During the entire period, the relative volume of publications by China and the United States changed considerably, as shown in Fig. . From 1997 to 2004, the number of papers published by the United States far exceeded that of China. From 1997 to 2000, the total number of papers published by China was 20% of that of the United States. From 2001 to 2004, the total number of papers published by China rose to 31% of that of the United States. In the period from 2005 to 2008, the number of papers published by China surpassed that of the United States, and the number of papers published by China in 2021–2022 is about 3 times that of the United States. It can be seen that in recent years, China’s research in the field of affective computing has accumulated rapidly, and its large volume of research has certain advantages compared with that of the United States. In addition, in 2021–2022, the number of papers published by India surpassed that of the United States for the first time. India has gradually become a major research center in the field of affective computing because of its advantages in computer science, engineering, and other disciplines.![](\\\\xa03. Comparison between the top 10 countries and the United States in the number of publications.### Main journalsThis section analyzes basic data on journal papers. The 17,351 published papers were distributed in 1,300 journals, among which _IEEE Access_ [impact factor (IF) 3.9] had the most (875), as shown in Table . Across all journals, 1,209 had an IF listed in the 2022 JCRs. The distribution of the IFs of the 1,209 journals is shown in Table . Among them, 54 journals have IFs greater than 10, and the 5 journals with the highest IFs are _World Psychiatry_ (73.3), _Lancet Psychiatry_ (64.3), _Nature Reviews Neuroscience_ (34.7), _Nature Human Behaviour_ (29.2), and _JAMA Psychiatry_ (25.8). The IFs of most journals are distributed in the 2 intervals of 2 ≤ IF < 4 and 4 ≤ IF < 7. It is worth noting that _IEEE Transactions on Affective Computing_ (IF 11.2) is a high-level journal focusing on the field of affective computing. It is a cross-disciplinary and international archive journal aimed at disseminating the results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. In addition, _Expert Systems with Applications_, _Knowledge-Based Systems_, _Information Processing & Management_, _IEEE Transactions on Multimedia_, _Neurocomputing_, _Information Sciences_, _Pattern Recognition_, _Applied Soft Computing_, _Decision Support Systems_, and _Future Generation Computer Systems_ are also high-level journals favored by scholars in the field of affective computing.| No. | Journal | Number of papers || --- | --- | --- || 1 | _IEEE Access_ | 875 || 2 | _Multimedia Tools and Applications_ | 474 || 3 | _IEEE Transactions on Affective Computing_ | 419 || 4 | _Sensors_ | 378 || 5 | _Frontiers in Psychology_ | 362 || 6 | _Applied Sciences-Basel_ | 349 || 7 | _Expert Systems with Applications_ | 290 || 8 | _International Journal of Advanced Computer Science and Applications_ | 272 || 9 | _Neurocomputing_ | 248 || 10 | _Knowledge-Based Systems_ | 226 || 11 | _Psychiatry Research_ | 191 || 12 | _Electronics_ | 167 || 13 | _Journal of Intelligent & Fuzzy Systems_ | 151 || 14 | _Neural Computing & Applications_ | 144 || 15 | _Neuropsychologia_ | 137 || 16 | _Schizophrenia Research_ | 135 || 17 | _Information Processing & Management_ | 132 || 18 | _Computational Intelligence and Neuroscience_ | 114 || 19 | _Cognitive Computation_ | 112 || 20 | _Information Sciences_ | 110 |Table\\\\xa03. Top 20 journals with the largest number of articles in the field of affective computing| Journal impact factor | Number of journals || --- | --- || IF ≥ 10 | 54 || 7 ≤ IF < 10 | 74 || 4 ≤ IF < 7 | 255 || 2 ≤ IF < 4 | 406 || 1 ≤ IF < 2 | 245 || IF ≤ 1 | 175 |Table\\\\xa04. Journal impact factor distribution### High-level international conferencesCombining ESI’s highly cited and hot papers with the “China Computer Federation Recommended International Academic Conferences” and CORE Computer Science Conference Rankings, we identified the high-level international conferences related to affective computing. These include the ACM International Conference on Multimedia (ACM MM), AAAI Conference on Artificial Intelligence (AAAI), Annual Meeting of the Association for Computational Linguistics (ACL), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), and the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).### Discipline distributionThis section analyzes the distribution of research fields based on statistics on the Web of Science categories of papers in the field of affective computing. Studies related to the topic of affective computing involve computer science, communication, engineering, psychology, medicine, and other disciplines, reflecting distinct interdisciplinary characteristics. The top 20 categories with the largest number of publications are listed in Table . The category with the largest proportion is “Computer Science, Artificial Intelligence,” with 12,678 publications (37.93% of the total), followed by “Engineering, Electrical & Electronic,” with 9,820 publications (29.36% of the total).| Web of Science category | Number of papers | Percentage (%) || --- | --- | --- || Computer Science, Artificial Intelligence | 12,687 | 37.90 || Engineering, Electrical & Electronic | 9,820 | 29.36 || Computer Science, Information Systems | 8,714 | 26.05 || Computer Science, Theory & Methods | 8,405 | 25.13 || Computer Science, Interdisciplinary Applications | 3,930 | 11.75 || Telecommunications | 3,133 | 9.37 || Computer Science, Software Engineering | 2,982 | 8.92 || Neurosciences | 2,376 | 7.10 || Psychiatry | 2,100 | 6.28 || Computer Science, Cybernetics | 1,904 | 5.69 || Imaging Science & Photographic Technology | 1,077 | 3.22 || Engineering, Multidisciplinary | 1,045 | 3.12 || Automation & Control Systems | 997 | 2.98 || Computer Science, Hardware & Architecture | 981 | 2.93 || Psychology, Multidisciplinary | 884 | 2.64 || Robotics | 793 | 2.37 || Engineering, Biomedical | 735 | 2.20 || Acoustics | 724 | 2.16 || Linguistics | 637 | 1.90 || Clinical Neurology | 610 | 1.82 |Table\\\\xa05. Top 20 categories with the most papers in the field of affective computing### Technology transfer and conversionThis study searched the Derwent Innovation Index, the world’s most comprehensive database of value-added patent information. Among effective invention patents with transfer records and high value, the transferred patents with an IncoPat patent value of 10 (the highest level) include “Cognitive content display device” (US10902058B2, transferred from IBM to Kyndryl Inc.) and “Signal processing approach to sentiment analysis for entities in documents” (US9436674B2, transferred from Attivio Inc. to Servicenow Inc.). However, the number of patent transfer records related to affective computing is small, indicating that technology transfer activity needs to be improved.### Global distribution of scholarsThis section presents statistical analysis of publications based on the country of the first author to provide a macroscopic understanding of the global distribution of scholars in the field of affective computing. As shown in Table , China has the largest number (4,240), followed by India (2,391) and the United States (2,390). In Fig. , darker shading indicates a larger number of scholars. It can be seen that Asia and North America are the regions with the most concentrated distribution of scholars in the field of affective computing.| No. | Country | Number of scholars | No. | Country | Number of scholars || --- | --- | --- | --- | --- | --- || 1 | China | 4,240 | 11 | Canada | 533 || 2 | India | 2,391 | 12 | France | 425 || 3 | USA | 2,390 | 13 | Turkey | 403 || 4 | UK | 999 | 14 | Netherlands | 349 || 5 | Germany | 825 | 15 | Malaysia | 331 || 6 | Italy | 690 | 16 | Pakistan | 324 || 7 | Japan | 631 | 17 | Brazil | 366 || 8 | South Korea | 514 | 18 | Greece | 248 || 9 | Spain | 545 | 19 | Iran | 270 || 10 | Australia | 496 | 20 | Singapore | 229 |Table\\\\xa06. Number of first authors in the field of affective computing (top 20 countries)![](\\\\xa04. Global distribution of scholars in the field of affective computing.### International collaborationThere is a wide range of international cooperation in the field of affective computing. A count of collaborations between the top 20 countries is shown in Table . The number of articles published by China and the United States is the largest (641), followed by China and the United Kingdom (343). Although cooperation between China and the United States has been challenging in recent years, in the field of affective computing, they remain each other’s largest partners, maintaining a vital and continuous cooperation.|  | C1 | U1 | I1 | U2 | G1 | J | I2 | A | S1 | S2 | C2 | F | N | S3 | T | S4 | M | P | B | G2 || --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- || C1 | / | 641 | 79 | 343 | 79 | 256 | 59 | 218 | 43 | 61 | 137 | 44 | 28 | 61 | 12 | 161 | 38 | 57 | 6 | 4 || U1 | 641 | / | 128 | 232 | 174 | 42 | 120 | 122 | 62 | 83 | 153 | 100 | 105 | 55 | 42 | 71 | 10 | 36 | 53 | 23 || I1 | 79 | 128 | / | 73 | 15 | 15 | 22 | 37 | 19 | 33 | 26 | 26 | 8 | 67 | 10 | 48 | 26 | 11 | 3 | 2 || U2 | 343 | 232 | 73 | / | 294 | 40 | 132 | 119 | 96 | 15 | 60 | 89 | 160 | 77 | 25 | 64 | 30 | 43 | 30 | 48 || G1 | 79 | 174 | 15 | 294 | / | 41 | 69 | 47 | 39 | 11 | 43 | 59 | 99 | 4 | 17 | 17 | 5 | 7 | 13 | 19 || J | 256 | 42 | 15 | 40 | 41 | / | 4 | 20 | 17 | 7 | 27 | 15 | 11 | 6 | 5 | 18 | 16 | 1 | 3 | 1 || I2 | 59 | 120 | 22 | 132 | 69 | 4 | / | 22 | 58 | 13 | 25 | 65 | 53 | 10 | 13 | 37 | 4 | 10 | 6 | 7 || A | 218 | 122 | 37 | 119 | 47 | 20 | 22 | / | 26 | 14 | 30 | 22 | 24 | 27 | 14 | 35 | 25 | 23 | 11 | 3 || S1 | 43 | 62 | 19 | 96 | 39 | 17 | 58 | 26 | / | 15 | 15 | 42 | 46 | 23 | 12 | 11 | 5 | 10 | 24 | 18 || S2 | 61 | 83 | 33 | 15 | 11 | 7 | 13 | 14 | 15 | / | 10 | 16 | 6 | 29 |  | 2 | 9 | 71 | 3 | 3 || C2 | 137 | 153 | 26 | 60 | 43 | 27 | 25 | 30 | 15 | 10 | / | 45 | 19 | 40 | 10 | 9 | 2 | 12 | 17 | 3 || F | 44 | 100 | 26 | 89 | 59 | 15 | 65 | 22 | 42 | 16 | 45 | / | 39 | 11 | 4 | 2 | 8 | 16 | 18 | 10 || N | 28 | 105 | 8 | 160 | 99 | 11 | 53 | 24 | 46 | 6 | 19 | 39 | / |  | 19 | 8 | 3 | 1 | 11 | 14 || S3 | 61 | 55 | 67 | 77 | 4 | 6 | 10 | 27 | 23 | 29 | 40 | 11 |  | / | 8 | 4 | 41 | 120 | 1 | 3 || T | 12 | 42 | 10 | 25 | 17 | 5 | 13 | 14 | 12 |  | 10 | 4 | 19 | 8 | / | 5 | 7 | 3 | 2 | 1 || S4 | 161 | 71 | 48 | 64 | 17 | 18 | 37 | 35 | 11 | 2 | 9 | 2 | 8 | 4 | 5 | / | 4 | 2 |  | 1 || M | 38 | 10 | 26 | 30 | 5 | 16 | 4 | 25 | 5 | 9 | 2 | 8 | 3 | 41 | 7 | 4 | / | 40 |  | 3 || P | 57 | 36 | 11 | 43 | 7 | 1 | 10 | 23 | 10 | 71 | 12 | 16 | 1 | 120 | 3 | 2 | 40 | / | 3 |  || B | 6 | 53 | 3 | 30 | 13 | 3 | 6 | 11 | 24 | 3 | 17 | 18 | 11 | 1 | 2 |  |  | 3 | / |  || G2 | 4 | 23 | 2 | 48 | 19 | 1 | 7 | 3 | 18 | 3 | 3 | 10 | 14 | 3 | 1 | 1 | 3 |  |  | / |Table\\\\xa07. Collaborations between the top 20 countries in the field of affective computingNote: C1, China; U1, USA; I1, India; U2, UK; G1, Germany; J, Japan; I2, Italy; A, Australia; S1, Spain; C2, Canada; S2, South Korea; F, France; N, Netherlands; T, Turkey; S3, Saudi Arabia; S4, Singapore; M, Malaysia; P, Pakistan; B, Brazil; G2, Greece.### Important research institutionsThe top 10 institutions in the world by number of publications (counting all authors) are listed in Table . This study used indicators such as Citation Impact, Category Normalized Citation Impact (CNCI), and Highly Cited Papers to further evaluate the influence of various institutions in the field of affective computing. Among them, CNCI is a valuable and unbiased impact indicator that excludes the influence of publication year, subject field, and document type. A CNCI value of 1 indicates that the cited performance of a group of papers is equivalent to the global average level, a value greater than 1 indicates higher performance, and a value less than 1 indicates lower performance; a value of 2 indicates performance twice as high as the global average. The top 5 institutions according to CNCI rankings were Nanyang Technological University (5.06), Imperial College London (3.58), Tsinghua University (3.23), the Chinese Academy of Sciences (3.15), and the University of California System (2.77).| No. | Institution | Number of papers | Citation impact | Category Normalized Citation Impact | H-index | Percentage in Q1 journals | Country || --- | --- | --- | --- | --- | --- | --- | --- || 1 | Chinese Academy of Sciences | 699 | 20.97 | 3.15 | 60 | 59.87 | China || 2 | University of London | 443 | 50.26 | 2.29 | 77 | 69.45 | UK || 3 | UDICE-French Research Universities | 388 | 18.86 | 1.37 | 42 | 50.43 | France || 4 | Centre National de la Recherche Scientifique (CNRS) | 377 | 19.33 | 1.36 | 42 | 51.56 | France || 5 | University of California System | 371 | 40.83 | 2.77 | 64 | 58.72 | USA || 6 | National Institute of Technology (NIT System) | 364 | 9.68 | 1.46 | 29 | 26.43 | India || 7 | Indian Institute of Technology System (IIT System) | 360 | 13.51 | 1.99 | 36 | 44.7 | India || 8 | Nanyang Technological University | 350 | 46.35 | 5.06 | 69 | 68.99 | Singapore || 9 | Tsinghua University | 302 | 24.87 | 3.23 | 44 | 62.93 | China || 10 | Imperial College London | 300 | 41.00 | 3.58 | 49 | 70.75 | UK |Table\\\\xa08. Institutions with a top 10 publication in affective computingNotes: 1. Citation impact: The citation impact of a set of documents is calculated by dividing the total number of citations of the set of documents by the number of documents. Citation impact shows the average number of citations received by a document in the group. 2. Category Normalized Citation Impact (CNCI): The CNCI of a document is obtained by dividing the actual number of citations by the expected number of citations of documents of the same type, publication year, and subject. When a document is classified into multiple subject areas, the average value of the ratio of actual citations to expected citations is used. The CNCI of a country is the average of the CNCIs of the publications of that country.### Citation network analysisThis section analyzes the direct citations of all authors in the field of affective computing. To highlight the key authors, 40 authors who had published no fewer than 30 papers were selected for analysis. The results are shown in Fig. . Authors in clusters of the same color have strong correlations and inheritance in research content. Representative scholars from the 5 clusters are listed in Table .![](\\\\xa05. Citation network of scholars.| Scholar | Organization | Research fields || --- | --- | --- || Baoliang Lu | Shanghai Jiaotong University, China | Brain-like computing, neural networks, deep learning, emotion AI, affective brain–computer interface || Bjoern Schuller | Imperial College London, UK | Machine intelligence, signal processing, affective computing, digital health, speech recognition || Erik Cambria | Nanyang Technological University, Singapore | Affective computing, sentiment analysis, commonsense reasoning, natural language understanding || Fuji Ren | The University of Tokushima, Japan the University of Electronic Science and Technology of China, China | Natural language processing, artificial intelligence, affective computing, and emotional robots || Wenming Zheng | Southeast University, China | Multimodal affective computing, neural computation, pattern recognition, machine learning, and computer vision |Table\\\\xa09. Representative scholars in the citation network### Word frequency analysisWord frequency refers to the number of times a word occurs in the document being analyzed. In scientometric research, word frequency dictionaries can be established for specific subject areas to quantify the analysis of scientists’ creative activities. Word frequency analysis is the method of extracting keywords or subject words that express the core content of the articles in the literature, to study the development trends and research hotspots of the field through the frequency distribution of these words. The results of conducting frequency and co-occurrence analysis on keywords assigned to papers by authors in the field of affective computing are shown in Table .| No. | Number of occurrences | Technical keyword | Number of co-occurrences with other keywords | Time period | Proportion of occurrences within last 3 years (%) || --- | --- | --- | --- | --- | --- || 1 | 7,621 | Sentiment analysis | Machine learning [958]; Opinion mining [936]; Natural language processing [829] | 2006–2023 | 21 || 2 | 4,566 | Emotion recognition | Feature extraction [422]; Affective computing [397]; Deep learning [372] | 1997–2023 | 24 || 3 | 2,457 | Affective computing | Emotion recognition [397]; Machine learning [191]; Emotion [137] | 2000–2023 | 15 || 4 | 2,232 | Deep learning | Sentiment analysis [691]; Emotion recognition [372]; Machine learning [268] | 2012–2023 | 40 || 5 | 2,054 | Machine learning | Sentiment analysis [958]; Natural language processing [275]; Deep learning [268] | 2002–2023 | 27 || 6 | 1,816 | Facial expression recognition | Deep learning [182]; Feature extraction [150]; Face recognition [109] | 1997–2023 | 18 || 7 | 1,348 | Natural language processing | Sentiment analysis [829]; Machine learning [275]; Deep learning [209] | 2006–2023 | 30% of 1,348 || 8 | 1,214 | Feature extraction | Emotion recognition [422]; Sentiment analysis [213]; Task analysis [181] | 2003–2023 | 32 || 9 | 1,209 | Opinion mining | Sentiment analysis [936]; Natural language processing [159]; Machine learning [151] | 2006–2023 | 11 || 10 | 1,067 | Emotion | Affective computing [137]; Emotion recognition ]; Facial expression ] | 1999–2023 | 13 || 11 | 1,007 | Twitter | Sentiment analysis [770]; Machine learning [160]; Social media [145] | 2011–2023 | 18 || 12 | 975 | Speech emotion recognition | Deep learning ]; Feature extraction ]; Emotion recognition ] | 2006–2023 | 29 || 13 | 852 | Social media | Sentiment analysis [587]; Twitter [145]; Machine learning [105] | 2009–2023 | 21 || 14 | 732 | Social cognition | Schizophrenia [193]; Emotion recognition [184]; Theory of mind [179] | 2001–2023 | 16 || 15 | 657 | Text mining | Sentiment analysis [486]; Natural language processing ]; Opinion mining ] | 2006–2023 | 15 || 16 | 635 | EEG | Emotion recognition [357]; Affective computing ]; Emotion ] | 2004–2023 | 27 || 17 | 620 | Classification | Sentiment analysis [208]; Machine learning ]; Emotion recognition ] | 2003–2023 | 19 || 18 | 618 | Facial expression | Emotion recognition [175]; Emotion ]; Affective computing ] | 1998–2023 | 15 || 19 | 582 | Convolutional neural network | Deep learning [146]; Facial expression recognition ]; Emotion recognition ] | 2003–2023 | 30 || 20 | 535 | Schizophrenia | Social cognition [193]; Emotion recognition ]; Theory of mind ] | 1998–2023 | 8 || 21 | 478 | Support vector machine | Sentiment analysis [123]; Facial expression recognition ]; Emotion recognition ] | 2002–2023 | 9 || 22 | 470 | Feature selection | Sentiment analysis [119]; Emotion recognition ]; Feature extraction ] | 2001–2023 | 16 || 23 | 423 | Face recognition | Feature extraction [155]; Emotion recognition [124]; Facial expression recognition [109] | 1997–2023 | 29 || 24 | 422 | Transfer learning | Emotion recognition ]; Deep learning ]; Sentiment analysis ] | 2009–2023 | 40 || 25 | 404 | Data mining | Sentiment analysis [251]; Feature extraction ]; Machine learning ] | 2006–2023 | 22 |Table\\\\xa010. Frequency analysis of top 25 keywords in affective computingThe Thomson Data Analyzer was used to automatically and manually clean the keywords assigned by the authors of papers in the dataset. Subsequently, VOSviewer was used to cluster the core (high-frequency) subject words and set a certain co-occurrence frequency and co-occurrence intensity according to the size of the dataset to cluster the keywords. Combined with expert interpretation, each cluster was named and interpreted, and the topics of the journal articles were identified and analyzed. After keyword cleaning, 613 keywords appearing more than 20 times were selected as analysis objects for cluster calculation. Five clusters were obtained by clustering the core subject words with the highest co-occurrence intensity, as shown in Table  and Fig. .| No. | Research theme | Number of core subject words | Average number of citations | Average correlation strength || --- | --- | --- | --- | --- || 1 | Natural language processing techniques used for affective computing and opinion mining | 153 | 10.41 | 197.80 || 2 | Facial expression and micro-expression recognition and analysis | 134 | 15.89 | 178.77 || 3 | Affective computing studies in human–computer interaction | 121 | 18.69 | 110.38 || 4 | Applied research of affective computing in affective disorder analysis | 30 | 33.5 | 165.59 || 5 | Multimodal sentiment analysis based on deep learning | 81 | 9.8 | 260.95 |Table\\\\xa011. Five research themes in affective computing![](\\\\xa06. Five research themes in affective computing.The average number of citations of a research theme is the average number of times that a paper containing these subject words has been cited since publication, and the average correlation strength of a research theme indicates the closeness of the connection between the core subject words contained in this theme concept. The greater the correlation strength, the greater the co-occurrence intensity between the core subject words and the more concentrated the research. In contrast, relatively lower correlation is associated with more scattered research. Research on the application of affective computing in the analysis of affective disorders has the highest average citation frequency, which shows that interdisciplinary research involving affective computing and medicine, especially research on affective disorders and depression recognition, has a greater influence. The average correlation strength of multimodal sentiment analysis based on deep learning is the largest, which shows that the research on this topic is the most concentrated.## DiscussionThis paper presents a comprehensive analysis and review of systematically collected data on papers and major intellectual property rights in the field of affective computing. The results reveal that over the past 25 years, affective computing has experienced rapid growth in the number of published papers, representing a vibrant academic ecology and an interdisciplinary character with a wide range of disciplines. Additionally, scholars worldwide actively participate in a relatively close cooperation network. In particular, Chinese scholars have led the world in terms of the number of publications, scholars, and collaborative papers in this field. Among important research institutions, Tsinghua University and the Chinese Academy of Sciences stand out, with CNCI values indicating that the average number of citations of their papers was more than twice the global average. Citation network analysis showed that Chinese scholars are representative and have become essential nodes in the citation network, indicating that China is constructing a large-scale talent team for affective computing and progressing in both the quantity and quality of research. However, China also faces disadvantages in academic journals, international conferences, and other aspects, leading to weak dominance, which restricts China’s academic discourse improvement in this field. Notably, in recent years, India’s publication volume has exceeded that of the United States for the first time, revealing a robust development potential linked to its advantages in computing. Nonetheless, India still has room for growth in terms of research quality and paper impact as it lacks representative scholars in the field of affective computing.### Challenges and technology development trends#### Modeling of cultural contextsThis study found that affective computing researchers are distributed across various countries globally and have a wide range of cultural backgrounds. While emotional expression has a degree of consistency across humanity, it is considerably influenced by cultural background. Cultural norms and values determine the different emotional experiences of individuals and how others perceive these emotions. Therefore, affective computing systems developed using a single cultural group may fail in other cultural contexts. For example, Chinese, Germans, and Japanese express emotions relatively implicitly, whereas Americans, British, and Brazilians express emotions more overtly. This indicates that emotion agents must match emotion calculation rules with the cultural context. Many Western cultural standards may not necessarily apply in Eastern contexts. For example, Japanese researchers tend to develop robots that can express emotions implicitly because overly direct expressions of emotions may cause user dissatisfaction ]. Therefore, cultural characteristics must be considered in developing universal cross-cultural emotional agents for people from different cultural backgrounds. Hofstede defined culture in terms of 5 measures—power distance, identity, gender, uncertainty avoidance, and long-term orientation—which can be used to summarize the typical rules of emotional expression in different cultural contexts ]. When it is challenging to obtain culture-specific empirical affective data, it is more feasible to design affective computational models using cultural theories and rules.#### Emotion generation techniquesThe cluster analysis of topic terms in affective computing revealed 5 important core topics, including “natural language processing techniques for affective computing and opinion mining” and “facial expression and micro-expression recognition and analysis.” Current research focuses more on emotion recognition, with relatively limited attention accorded to emotion generation. Emotion recognition and generation are both essential aspects of affective computing and constitute an important technical basis for the closed loop of human–computer interaction. To enable machines to provide more anthropomorphic and natural feedback, it is crucial to focus on the following 2 research areas. (a) Generation of facial expressions. The fact that human emotions are expressed through visual (55%), voice (38%), and verbal (7%) signals is also known as the “3V rule,” which reflects the importance of human facial expressions in emotion analysis ]. Appropriate use of facial expressions by avatars and robots can enhance human–robot interaction. Thus, current research aims to build a lexicon of facial expressions that can translate communicative intent into associated expressive morphology and dynamic features to express various meanings. Meanwhile, a team of animation experts is required to achieve realistic facial rendering effects, including lighting and muscle textures. (b) Generation of emotional body movement. This requires the design of embodied agents using computer models of body expression. This area involves studying human kinematics; however, researchers have yet to determine how to characterize the organic combination of body parts, movement strength, and posture of specific emotional states.#### Fine-grained sentiment classification modelsEkman’s basic emotion theory model is a widely used classification model for emotion computation ]. However, in real life, people’s emotions often exist in a mixed state. For example, people often simultaneously express surprise and joy, sadness and pain, etc. Du et\\\\xa0al. ] proposed the concept of mixed emotions based on research conducted using the Facial Action Coding System (FACS). They suggested that the combination of 2 basic emotions creates mixed emotions and defined different types using scenario examples. Using a FACS-based face recognition algorithm model, microvariations in facial muscles can be analyzed to accurately discriminate between different types of mixed emotions. Martinez ] assessed whether mixed emotions can be semantically labeled correctly. The test tasks included prioritization and forced selection of mixed emotion labels, and the results showed that subjects performed consistent and accurate categorization. Mixed emotion is an essential research direction for expression-based fine-grained emotion classification. This concept extends the core idea of FACS, aiming to reveal the relationship between mixed and basic emotions. It offers a better solution to the problem of differentiation of emotions and clarifies the relationship between differentiated emotions and their original emotions, providing traceable clues and measurement possibilities for the generation, development, and change of emotions. It summarizes complex emotional changes into a logical dynamic composite form with similar configuration effects, resulting in strong interpretability, logic, and unity.#### Code of ethics and technical standardsRecording an individual’s emotional state has implications for privacy, particularly when it comes to recording video or audio. Subjects may not agree to provide researchers with authentic and naturalistic emotional data and may feel uncomfortable being monitored in daily life. For example, the results of AI emotion monitoring tools may be analyzed alongside employee performance evaluations, predictions of the risk of leaving the job, and patterns of employee–team interactions for predicting behavior. Although the use of such technology reduces employee turnover and saves costs for organizations ], employees may experience constant psychological stress, leading to burnout ]. Additionally, individuals may lose autonomy as they become more hesitant to display emotions in public, instead choosing to use a “poker face.” While there should be openness in the use of affective computing, appropriate regulation is necessary to assess potential risks involving privacy and security, and the technology should be reviewed and documented for each industry to maximize benefits while minimizing harm, risks, and costs. Ethical issues are more likely to be overlooked in computing and engineering than in psychology. The collection of individual data, particularly physiological data, should be regulated by human research ethics committees, which are best suited to managing informed consent and privacy issues.Efforts should be made to strengthen the development of international standards in the field of affective computing to form a universally accepted specification. Currently, the available standard is “Information technology—Affective computing user interface (AUI)” (standard number ISO/IEC 30150-1:2022). The first part, “Model,” was released in June 2022, and the second part, “Affective Characteristics,” is under construction. However, there is a lack of standards for data collection, data security, and personal privacy protection in the field of affective computing. Therefore, the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union (ITU) should improve relevant standards and unify them for global use.#### Cognitive neuroscience-inspired affective computingJust as CNN architectures are inspired by biological visual processing and reinforcement learning methods are inspired by behaviorist theories in psychology, impulse network models are inspired by neuroplasticity. Cognitive neuroscience has also developed theories on affective circuits ], multiple-wave models ], embodied cognition ], and other related areas, providing brain-inspired insights into the design of affective computation models. Studies on the physiological representations of different emotions offer theoretical foundations and guidelines for feature extraction in affective computing based on facial expressions, psychophysiological measurements, and neuroimaging. Further human research in the field of cognitive neuroscience will ultimately affect the development of affective computing and artificial intelligence as a whole. The cognitive process of human brain emotion processing, its neural mechanism, and its anatomical basis provide essential inspiration for the development of affective computing models. However, to ensure that machines have genuine emotions rather than just appearing to have emotions, further research in cognitive neuroscience is required. This research may involve exploring the neural basis for the generation of human consciousness, the neural mechanism for the construction of human values, and other key scientific issues. Based on this neural theoretical foundation, simulation and machine implementation are feasible options for providing machines with authentic emotions.#### Construction of large-scale multimodal datasetsThe development of affective computing is highly dependent on the construction of large-scale open datasets. Three major trends are described below. The first trend predicts that dataset sizes will continue to grow to meet the demands of deep learning algorithm training. Deep-learning models have a substantial number of parameters, and the selection of these parameters requires samples that are typically 100 times the number of parameters. A larger dataset size enables the trained model to avoid overfitting, which improves model learning. However, the challenge lies in labeling these massive datasets. Thus, it is necessary to explore active, weakly supervised, and unsupervised learning methods to label the meaningful data in large unlabeled datasets or train machines for labeling. The second trend highlights the need for the collection of multimodal data, the accumulation of richer modal information, and fine-grained alignment between different modalities. At this stage, machines differ from human beings in 2 critical aspects: First, humans exist in a multimodal social environment, as evidenced by their joint expression of intentions and emotions through language, facial expressions, speech, and actions; second, humans can switch between modalities for emotional reasoning when dealing with emotions. They can also switch between different modalities to search for clues, eliminate ambiguities, and conduct emotional reasoning through interconnections. Therefore, creating a large-scale multimodal emotion dataset can contribute to the development of human-like emotion intelligence technology and the realization of more accurate emotion recognition. The third trend focuses on collecting natural-scene data, as emotional data collected in performance or evoked mode may not accurately represent real-life scenarios. However, collecting high-quality labeled emotional-physiological data in daily life remains a challenge due to the lack of hardware collection devices that are sufficiently comfortable and resistant to interference.#### Multimodal fusion technology innovationMultimodal fusion combines information from multiple modalities using multimodal representations for sentiment classification. It can enhance the performance of sentiment-computing models by playing a complementary and disambiguating role ]. Multimodal fusion methods can be classified as model-independent or model-based. Model-independent fusion methods do not rely on a specific deep-learning method, whereas model-based fusion methods do.There are 3 categories of model-independent fusion methods: early fusion (feature-based fusion), late fusion (decision-based fusion), and hybrid fusion (combination of the 2). Early fusion integrates features immediately after they are extracted and uses multiple signals to create a single feature vector, which is then modeled using machine-learning algorithms. The larger the number of features and the greater the variation in these features, the more challenging feature-level fusion becomes and the easier it is to overfit the training data. In contrast, late fusion performs integration only after each model outputs the results (e.g., classification or regression results). It can better handle overfitting but does not allow the classifier to train on all data simultaneously. The Dempster–Shafer theory of evidence is a generalization of Bayesian theory to subjective probability. It is widely used in late fusion models because of its ability to model uncertain knowledge and combine beliefs from different sources to obtain new beliefs that take into account all available evidence. Hybrid fusion combines the outputs of earlier fusion methods and unimodal predictors. Although it is flexible, careful design is required to determine the timing, modalities, and method of fusion based on the specific application problem and research content. Researchers must select the appropriate approach at their discretion.Model-based fusion methods address the multimodal fusion problem through implementation techniques and models, using 3 common methods: multiple kernel learning (MKL), graphical models (GMs), and neural networks (NNs). As these methods easily exploit the spatial and temporal structure of the data, they are particularly suitable for time-related modeling tasks. Additionally, they allow human expert knowledge to be embedded in the model, thereby enhancing interpretability. However, their disadvantage is that they are computationally expensive and challenging to train.Research has shown that synesthesia is generated not only in the cerebral cortex but also in the subcortical limbic system, including the thalamus, amygdala, and hippocampus, which are closely related to emotional processing ]. Inspired by the multistage fusion phenomenon that integrates multisensory information in the brain, a multistage multimodal emotion fusion method can be developed. This would first involve training a unimodal model, splicing it as an implicit state with another modal feature, training the bimodal model similarly, and continuing with this process until a multimodal model is obtained. In conclusion, multimodal fusion technology effectively utilizes the synergistic complementarity of different modal information ], enhances emotional understanding and expression, and improves model robustness and performance. This represents an important direction for future research.#### Data- and knowledge-driven technological innovationIn its early stages, affective computing research relied heavily on collected data to make inferences. However, this data-driven approach is both inefficient and ineffective at the application level. For humans to understand data fully, they must activate other associated information, such as potential knowledge or common sense. The human brain can seamlessly combine this information to enable more generalized, intelligent, and frugal computation for complex problems. Therefore, affective computing requires not only big data and extensive computing power but also the integration of knowledge. Knowledge guidance and inspiration can compensate for insufficient or uneven data quality while conserving computational power. For instance, in constructing a multidisciplinary and multi-faceted emotional knowledge map, fine-grained emotional knowledge integrated through emotional commonsense associations is used to enable the modeling of hierarchical logical relationships between aspect words and emotional words. This approach facilitates the dynamic correlation, aggregation, and reasoning of domain, aspect, and emotional knowledge. It provides an optimal solution for various applications of affective computing, such as efficient real-time online sentiment analysis, emotion-injected dialogue systems, and emotion-injected story generation. These applications provide dynamic and accurate domain-adaptive sentiment knowledge.#### Group affective computingCurrent research in affective computing primarily focuses on sentiment analysis at the individual level, neglecting the potential value of group-affective computing. For instance, emotions felt by individual employees can aggregate and spread to create “collective emotions” in the workplace. These shared emotions can considerably affect the organization by offering insights into absenteeism, intra-team communication, team cohesion and performance, and organizational citizenship behavior. As such, affective computing research could expand its focus from individual to collective affect analysis and the propagation of affect across people. Furthermore, group affective computing can predict consumer behavior. EEG-based hyperscanning technology, which explores dynamic brain activity between 2 or more interacting customers and their underlying neuroemotional activities, can be used to anticipate shared consumption intentions, panic buying, and group-buying marketing effects. Although group affective computing currently lacks a well-established research methodology, it is a promising direction for future studies.#### Unique emotional carriersEmotions are ubiquitous in human political, economic, and cultural life, and the carriers of emotions are continually increasing in number, making them a popular research topic. Several areas have been identified as key carriers of emotions. (a) Political speeches: CORPS is a corpus that contains political speeches with markers indicating audience reactions such as applause, standing ovations, and boos ]. Researchers can use this information to predict emotion-evoking actions and persuasive content that may induce empathy and sympathy in audiences. (b) Music and drama: Affective computing in music and drama provides a basis for the categorized retrieval of relevant emotional carriers. Advancements in artificial intelligence-generated content (AIGC) technology have made machine-generated music possible, and affective computing can enhance the generation of music to conform to emotional classifications. (c) Oil painting: As a representative art form, oil painting allows creators to express their innermost emotions. Its charm lies not in the degree of realism but in the emotions it conveys. Combining affective computing with oil painting would enable the exploration of artificial intelligence methods for emotional expression, the integration of technology and art, and the establishment of a library of emotion-inducing materials for oil paintings, thereby providing resources for the development of affective computing disciplines.### Outlook for future applications#### Affective brain–computer interfacesAffective brain–computer interfaces (aBCIs) are primarily designed to measure emotional states through neurological measurements and to recognize and/or regulate human emotions. Currently, aBCIs are one of the main methods of realizing emotional intelligence. At this stage, the most commonly used physiological signals for emotional brain–computer interfaces are EEG signals, which map closely to an individual’s emotional state. As in motor brain–computer interfaces, the human brain plays the role of a controller for the entire system. The first step involves decoding an individual’s initial emotional state and then recognizing and understanding their emotions. Subsequently, a control strategy or system is designed to achieve the target emotion using control signals or parameters that provide feedback to the brain, thereby forming a closed-loop system.Unlike facial expressions, physiological signals such as EEG signals are difficult to disguise and provide an accurate reflection of the real emotional state of the individual. As a result, affective brain–computer interfaces play a crucial role in clinical diagnostics and therapy. Their uses include detecting workload and mental state, using neurofeedback for stress relief, aiding in the diagnosis of social anxiety and other disorders ], and enabling objective assessment and intervention in depression. Furthermore, affective brain–computer interfaces have considerable potential for military applications. They can help maximize the physiological capabilities of individual soldiers, enhance their endurance and tolerance to extreme environments, and improve their overall physical and mental fitness. These objectives are achieved by installing electroencephalography electrodes inside combat helmets to detect threats and emotional signals emitted by the brain. The signals are then converted into computer language using computer algorithms, analyzed, and confirmed by combat command. Subsequently, threat warnings and reminders about emotional regulation are sent to the affected soldiers, and signals to cooperate in combat are transmitted to surrounding soldiers. In addition, direct transcranial current stimulation, transcranial electromagnetic stimulation, and deep brain cortex stimulation can act on the brain to eliminate fatigue, reduce stress and anxiety, control pain sensation, and enhance cognitive ability. This system helps improve the situational awareness of soldiers on the battlefield, thereby improving their ability to survive.The primary obstacle to the application of affective brain–computer interfaces is their unstable performance. Cross-modal affective models that rely on heterogeneous transfer learning (HTL) may be necessary for establishing reliable and robust aBCI technology in complex real-world environments. To address the missing-modalities problem, cross-modal emotion models comprehensively analyze signals from multiple modalities and extract correlation characteristics during the training process. In the testing stage, predictions are made based on partial modal information. For example, correlating EEG signals with eye movement enables the use of eye movement alone to assess emotions in scenarios where collecting EEG signals is difficult. The HTL approach ensures that performance degradation in the absence of modalities is acceptable, thereby improving model robustness. In addition, transfer-learning techniques based on deep and generative adversarial networks can solve the problem of individual differences. These techniques enable generalization from the source domain to the target domain, thereby expanding the scope of possible applications of affective brain–computer interfaces.#### Empathic human–computer dialogueThere have been 4 waves of change in the way people interact with machines. The first wave, represented by Microsoft, involved the organic fusion of the user interface, operating system, keyboard, and mouse. This greatly reduced the difficulty of human–computer interaction and contributed to the rapid popularization of the personal computer. The second wave, represented by Google, involved the organic integration of search engine and internet technologies. This integration broke down information silos and considerably expanded the boundaries of interaction. The third wave, represented by Apple, involved the miniaturization of computing represented by the smartphone. This breakthrough removed the physical space limitations of human–computer interaction, enabling interconnectivity anytime, anywhere. Currently, we are in the fourth wave, represented by OpenAI. This wave involves the comprehensive application of a human–computer dialogue system that makes human–computer interaction more anthropomorphic and naturalized.The essence of human–computer dialogue is to make human–computer interaction more human-like. Humans exchange information through natural language and multiple senses, and human–computer interaction can imitate this process through multimodal information for joint analysis and decision-making. Human–computer dialogue involves a diverse range of signals, including speech, text, and images (such as individual facial expressions and body movements), conveying information in both the rational and perceptual dimensions. Linguistic text serves as the ontology of intent understanding, but emotional information conveyed through voice intonation, facial expressions, and body movements plays a crucial role in disambiguation, which is essential for in-depth communication between humans and machines. The use of different emotional colors to express the same sentence results in entirely different connotations. As Nobel Prize winner Simon noted, emotion recognition is crucial for the communication and understanding of information. Therefore, affective computing offers machines the ability to achieve deep contextual understanding.In advanced technology fields, research has expanded to include machine expression and action generation, referred to as “multimodal emotional expression generation.” A current focus area is the development of a “virtual human” interface that not only appears human-like but also simulates human demeanor and behavior. For instance, voice-driven facial-expression animation generation technology can create virtual humans with facial expressions and lip, head, and body movements that closely resemble those of real people. The virtual human no longer has an empty skin but appears more 3-dimensional and vivid. The personalization of human–computer interaction lays the crucial foundations for future applications in areas such as elderly companions, intelligent customer service, and mayor hotlines, revealing important prospects for practical use.#### Emotion-assisted decision-makingHuman–computer interaction involves both shallow and deep levels. At the shallow level, machines are equipped with the ability to read and speak, whereas at the deep level, they are capable of thinking and making decisions like humans. Nobel Prize winner Kahneman described human decision-making as entailing 2 processes: fast (System 1) and slow (System 2). The unconscious “System 1” relies on emotions, experience, and rapid judgments, while the conscious “System 2” relies on rational deliberation. Emotions play an important role in advanced human thinking and decision-making. The book “Descartes’ Error” emphasizes that emotions are crucial for rational decision-making and behavior ]. Numerous studies have indicated that purely rational decision-making may not always be the optimal solution for humans when dealing with problems due to the complexity of the social environment. Incorporating emotional factors into the decision-making process may help individuals identify better solutions. Therefore, inputting emotional variables can enable machines to make decisions in a more human-like manner. In building a harmonious human–machine symbiotic society, it is essential to master this high-level function, which is also an important direction in affective computing research. The modeling of machine agents has begun to incorporate patterns of emotional influence on human rational decision-making and mechanisms for deciding and interrupting behaviors based on goals , ].Emotion-assisted decision-making abilities can be applied widely across various fields of human–machine collaboration. For example, in production tool manipulation, the operator’s emotional state regarding operation specifications, safety awareness, and accurate judgment has an impact. Monitoring and early warning of negative emotions, psychological stress, fatigue, and drowsiness, etc., can help identify potential anthropogenic risks to production safety. Machines can then optimize management decisions, intervene early, and intervene intelligently to avoid major accidents. In assisted driving, negative emotions such as anger and anxiety can seriously affect the driver’s concentration and may lead to traffic accidents. Emotion-assisted decision making can be incorporated into driver monitoring systems (DMS) that use facial-expression recognition technology and wearable devices to provide real-time monitoring of the driver’s emotional state. This approach equips the vehicle with enhanced safety performance and improves the overall driving experience ].#### Affective virtual realityThe metaverse is generating considerable interest in both industrial and academic circles as the next generation of immersive, full-fledged internet. It is considered a theme park for digitized human beings, a virtual complex resulting from the development of cutting-edge technologies, and a utopia where the human body and consciousness can cross physical time and space. As a new type of future living space, the development of the metaverse cannot be limited to creating a virtual space parallel to the real world. It should exist in human life like air, enabling humans to shuttle freely between the virtual and real worlds. Affective virtual reality is crucial for constructing the metaverse because it can considerably enhance an individual’s experience of bodily ownership, sense of agency, and situational awareness. In particular, an individual’s avatar in the metaverse, which is a core element of the metaverse construction, includes voice tone, facial expressions, body movements, and gestures that richly and 3-dimensionally express the individual’s emotions and create scenes and spaces for emotional twins ]. As in movies and literature, complex and emotionally rich avatar characters engage audiences more than simple and stable characters do. This appeal creates the illusion that avatars are alive and pass the Turing test, which enhances the audience’s interest and engagement in the virtual world ]. Affective virtual reality has considerable potential for applications in virtual reality socialization, virtual reality anchors, and virtual reality marketing.### LimitationsThis bibliometric analysis has several limitations that should be acknowledged. First, the basic processing unit of information in this study is the article in its entirety, and the full content of the literature has not been systematically broken down, which may result in incomplete analysis and conclusions. Second, the assumption that the articles contain information of equal quality makes it difficult to consider the objective differences in the value of the literature. In future research, a combination of bibliometrics and content analysis could be used to enhance the reliability and accuracy of the analytical results.## ConclusionAffective computing is a rapidly developing field with broad prospects. Emerging forces such as China and India are injecting strong momentum into the field. However, the field of affective computing also faces challenges and development trends in 10 aspects, including cultural background modeling, ethical and moral norms, and multimodal integration. Affective computing has great potential for application in 4 major fields and requires the joint efforts of researchers and industry practitioners. These efforts can make affective computing beneficial to the progress of human society by building a more anthropomorphic, harmonious, and natural human–computer symbiotic social form.## Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.## Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.## References1Keltner D, Sauter D, Tracy J, Cowen A. Emotional expression: Advances in basic emotion theory. _J Nonverbal Behav_. 2019; **43**(2):133–160.2Soleymani M, Garcia D, Jou B, Schuller B, Chang S-F, Pantic M. A survey of multimodal sentiment analysis. _Image Vis Comput_. 2017; **65**:3–14.3Bach DR, Dayan P. Algorithms for survival: A comparative perspective on emotions. _Nat Rev Neurosci_. 2017; **18**:311–319.4Chen L, Zhou M, Wu M, She J, Liu Z, Dong F, Hirota K. Three-layer weighted fuzzy support vector regression for emotional intention understanding in human–robot interaction. _IEEE Trans Fuzzy Syst_. 2018; **26**(5):2524–2538.5Kahneman D. _Thinking_, _fast and slow_. Macmillan, London, UK: Farrar, Straus and Giroux; 2011.6Fanselow MS. Emotion, motivation and function. _Curr Opin Behav Sci_. 2018; **19**:105–109.7Lopes PN, Salovey P, Coté S, Beers M. Emotion regulation abilities and the quality of social interaction. _Emotion_. 2005; **5**:113–118.8Suvilehto JT, Glerean E, Dunbar RIM, Hari R, Nummenmaa L. Topography of social touching depends on emotional bonds between humans. _Proc Natl Acad Sci U S A_. 2015; **112**:13811–13816.9Picard RW. _Affective computing_. Cambridge (MA): MIT Press; 1997.10Ekman P. Are there basic emotions? _Psychol Rev_. 1992; **99**(3):550–553.11Russell JA. A circumplex model of affect. _J Pers Soc Psychol_. 1980; **39**:1161–1178.12Mehrabian A. Framework for a comprehensive description and measurement of emotional states. _Genet Soc Gen Psychol Monogr_. 1995; **121**(3):339–361.13Bakker I, Van Der Voordt T, Vink P, De Boon J. Pleasure, arousal, dominance: Mehrabian and russell revisited. _Curr Psychol_. 2014; **33**:405–421.14Pozzi FA, Fersini E, Messina E, Liu B. Chapter\\\\xa01—Challenges of sentiment analysis in social networks: An overview. In: Pozzi FA, Fersini E, Messina E, Liu B, editors, _Sentiment analysis in social networks_. Boston: Morgan Kaufmann; 2017. p. 1–11.15Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. Poster presented at: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies; Portland, Oregon, USA; 2011. p. 142–150.16Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing; 2013; Seattle, WA, USA. p. 1631–1642.17Blitzer J, Dredze M, Pereira F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Poster presented at: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics; 2007; Prague, Czech Republic. p. 440–447.18Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of German emotional speech. _Interspeech_. 2005; **5**:1517–1520.19McKeown G, Valstar M, Cowie R, Pantic M, Schroder M. The SEMAINE Database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE Trans Affect Comput_. 2011; **3**(1):5–17.20Xu L, Xu M, Yang D. Chinese emotional speech database for the detection of emotion variations. _J Tsinghua Univ Nat Sci._ 2009;49(S1):1413–1418 _._21Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: From unimodal analysis to multimodal fusion. _Inf Fusion_. 2017; **37**:98–125.22Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, Gao S, Sun Y, Ge W, Zhang W, et al. A systematic review on affective computing: Emotion models, databases, and recent advances. _Inf Fusion_. 2022; **83–84**:19–52.23Zhang Z, Luo P, Loy CC, Tang X. From facial expression recognition to interpersonal relation prediction. _Int J Comput Vis_. 2018; **126**:550–569.24Mollahosseini A, Hasani B, Mahoor MH. AffectNet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Trans Affect Comput_. 2019; **10**:18–31.25Li S, Deng W, Du J. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017; . Honolulu, HI. p. 2584–2593.26Li X, Pfister T, Huang X, Zhao G, Pietikäinen M. A spontaneous micro-expression database: Inducement, collection and baseline. Paper presented at: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG); 2013; Shanghai, China. p. 1–6.27Galvão F, Alarcão SM, Fonseca MJ. Predicting exact valence and arousal values from EEG. _Sensors (Basel)_. 2021; **21**(10):3414.28Shalbaf A, Bagherzadeh S, Maghsoudi A. Transfer learning with deep convolutional neural network for automated detection of schizophrenia from EEG signals. _Phys Eng Sci Med_. 2020; **43**(4):1229–1239.29Shirahama K, Grzegorzek M. Emotion recognition based on physiological sensor data using codebook approach. In: Piętka E, Badura P, Kawa J, Wieclawek W, editors. _Information technologies in medicine_. Cham: Springer International Publishing; 2016. p. 27–39.30Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I. DEAP: A database for emotion analysis\\\\xa0using physiological signals. _IEEE Trans Affect Comput_. 2012; **3**(1):18–31.31Duan R-N, Zhu J-Y, Lu B-L. Differential entropy feature for EEG-based emotion classification. Paper presented at: 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER); 2013; San Diego, CA, USA. p. 81–84.32Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K. Introducing WESAD, a multimodal dataset for wearable stress and affect detection. Paper presented at: Proceedings of the 20th ACM International Conference on Multimodal Interaction; 2018; Boulder, CO, USA. p. 400–408.33Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. _Comput Linguist_. 2011; **37**(2):267–307.34Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. Paper presented at: Proceedings of the International Conference on Web Search and Web Data Mining—WSDM ’08; 2008; Palo Alto, CA, USA. p. 231.35Mullen T, Collier N. Sentiment analysis using support vector machines with diverse information sources. Paper presented at: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing; 2004; Barcelona, Spain. p. 412–418.36Pak A, Paroubek P. Text representation using dependency tree subgraphs for sentiment analysis. In: Xu J, Yu G, Zhou S, Unland R, editors. _Database systems for advanced applications_. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. p. 323–332.37Deng J, Ren F. A survey of textual emotion recognition and its challenges. _IEEE Trans Affect Comput_. 2023; **14**(1):49–67.38Heaton CT, Schwartz DM. Language models as emotional classifiers for textual conversation. Paper presented at: Proceedings of the 28th ACM International Conference on Multimedia; 2020; Seattle, WA, USA. p. 2918–2926.39Mao R, Liu Q, He K, Li W, Cambria E. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. _IEEE Trans Affect Comput_. 2022; **14**(3):1743–1753.40Lee CM, Narayanan SS. Toward detecting emotions in spoken dialogs. _IEEE Trans Audio Speech Lang Process_. 2005; **13**(2):293–303.41Lugger M, Yang B. The relevance of voice quality features in speaker independent emotion recognition. Paper presented at: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing—ICASSP ’07; 2007; Honolulu, HI, USA. p. IV-17–IV–20.42Likitha MS, Gupta SRR, Hasitha K, Raju AU. Speech based human emotion recognition using MFCC.Paper presented at: 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); 2017; Chennai, India. p. 2257–2260.43Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. _Speech Commun_. 2010; **52**(7–8):613–625.44Alisamir S, Ringeval F. On the evolution of speech representations for affective computing: A brief history and critical overview. _IEEE Signal Process. Mag_. 2021; **38**(6):12–21.45Stappen L, Baird A, Schumann L, Schuller B. The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements. _IEEE Trans Affect Comput_. 2023; **14**(2):1334–1350.46Huang Z, Dong M, Mao Q, Zhan Y. Speech emotion recognition using CNN. Paper presented at: Proceedings of the 22nd ACM International Conference on Multimedia; 2014; New York, NY, USA. p. 801–804.47Neumann M, Vu NT. Improving speech emotion recognition with unsupervised representation learning on unlabeled speech. Paper presented at: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2019; Brighton, UK. p. 7390–7394.48Abdelwahab M, Busso C. Domain adversarial for acoustic emotion recognition. _IEEE/ACM Trans Audio Speech Lang Process_. 2018; **26**(12):2423–2435.49Shan C, Gong S, McOwan PW. Facial expression recognition based on Local Binary Patterns: A comprehensive study. _Image Vis Comput_. 2009; **27**(6):803–816.50Chao W-L, Ding J-J, Liu J-Z. Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection. _Signal Process_. 2015; **117**:1–10.51James W. Review of la pathologie des emotions by Ch. Féré. _Philos Rev_. 1893; **2**:333–336.52Cannon WB. The James-Lange theory of emotions: A critical examination and an alternative theory. _Am J Psychol_. 1987; **100**:567–586.53Kim M-K, Kim M, Oh E, Kim S-P. A review on the computational methods for emotional state estimation from the human EEG. _Comput Math Methods Med_. 2013; **2013**: Article e573734.54Craik A, He Y, Contreras-Vidal JL. Deep learning for electroencephalogram (EEG) classification tasks: A review. _J Neural Eng_. 2019; **16**(3): Article 031001.55Maria MA, Akhand MAH, Shimamura T. Emotion recognition from EEG with normalized mutual information and convolutional neural network. Paper presented at: 2022 12th International Conference on Electrical and Computer Engineering (ICECE); 2022; Dhaka, Bangladesh. p. 372–375.56Rahman MM, Sarkar AK, Hossain MA, Hossain MS, Islam MR, Hossain MB, Quinn JMW, Moni MA. Recognition of human emotions using EEG signals: A review. _Comput Biol Med_. 2021; **136**: Article 104696.57D’mello SK, Kory J. A review and meta-analysis of multimodal affect detection systems. _ACM Comput Surv_. 2015; **47**(3):1–36.58He Z, Li Z, Yang F, Wang L, Li J, Zhou C, Pan J. Advances in multimodal emotion recognition based on brain–computer interfaces. _Brain Sci_. 2020; **10**(10):687.59Filippini C, Perpetuini D, Cardone D, Chiarelli AM, Merla A. Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review. _Appl Sci_. 2020; **10**(8):2924.60Spezialetti M, Placidi G, Rossi S. Emotion recognition for human-robot interaction: Recent advances and future perspectives. _Front Robot AI_. 2020; **7**: Article 532279.61Peng Y, Fang Y, Xie Z, Zhou G. Topic-enhanced emotional conversation generation with attention mechanism. _Knowl Based Syst_. 2019; **163**:429–437.62Dybala P, Ptaszynski M, Rzepka R, Araki K, Sayama K. Metaphor, humor and emotion processing in human-computer interaction. _Int J Comput Linguist Res_. 2013.63Goswamy T, Singh I, Barkati A, Modi A. Adapting a language model for controlled affective text generation. Paper presented at: Proceedings of the 28th International Conference on Computational Linguistics; 2020; Barcelona, Spain. p. 2787–2801.64Lei Y, Yang S, Wang X, Xie L. MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. _IEEE/ACM Trans Audio Speech Lang Process_. 2022; **30**:853–864.65Crawford K. Time to regulate AI that interprets human emotions. _Nature_. 2021; **592**(7853):167.66Ho M-T, Mantello P, Nguyen H-KT, Vuong Q-H. Affective computing scholarship and the rise of China: A view from 25 years of bibliometric data. _Humanit Soc Sci Commun_. 2021; **8**: Article 282.67Yadegaridehkordi E, Noor NFBM, Ayub MNB, Affal HB, Hussin NB. Affective computing in education: A systematic review and future research. _Comput Educ_. 2019; **142**: Article 103649.68Wu C-H, Huang Y-M, Hwang J-P. Review of affective computing in education/learning: Trends and challenges. _Br J Educ Technol_. 2016; **47**(6):1304–1323.69Liberati G, Veit R, Kim S, Birbaumer N, von Arnim C, Jenner A, Lulé D, Ludolph AC, Raffone A, Belardinelli MO, da Rocha JD, Sitaram R. Development of a binary fMRI-BCI for Alzheimer patients: A semantic conditioning paradigm using affective unconditioned stimuli. Paper presented at: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction; 2013; Geneva, Switzerland. p. 838–842.70Yuvaraj R, Murugappan M, Mohamed Ibrahim N, Iqbal Omar M, Sundaraj K, Mohamad K, Palaniappan R, Mesquita E, Satiyan M. On the analysis of EEG power, frequency and asymmetry in Parkinson’s disease during emotion processing. _Behav Brain Funct_. 2014; **10**:12.71Baki P, Kaya H, Çiftçi E, Güleç H, Salah AA. A multimodal approach for mania level prediction in bipolar disorder. _IEEE Trans Affect Comput_. 2022; **13**(4):2119–2131.72Mohammadi-Ziabari SS, Treur J. Integrative biological, cognitive and affective modeling of a drug-therapy for a post-traumatic stress disorder. In: Fagan D, Martín-Vide C, O’Neill M, Vega-Rodríguez MA, editors. _Theory and practice of natural computing_. Cham: Springer International Publishing; 2018. p. 292–304.73Tivatansakul S, Ohkura M. Healthcare system focusing on emotional aspects using augmented reality—Implementation of breathing control application in relaxation service. Paper presented at: 2013 International Conference on Biometrics and Kansei Engineering; 2013; Tokyo, Japan. p. 218–222.74Zenonos A, Khan A, Kalogridis G, Vatsikas S, Lewis T, Sooriyabandara M. HealthyOffice: Mood recognition at work using smartphones and wearable sensors. Paper presented at: 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops); 2016; Sydney, NSW, Australia. p. 1–6.75Weziak-Bialowolska D, Bialowolski P, Lee MT, Chen Y, VanderWeele TJ, McNeely E. Psychometric properties of flourishing scales from a comprehensive well-being assessment. _Front Psychol_. 2021; **12**: Article 652209.76Pei G, Xiao Q, Pan Y, Li T, Jin J. Neural evidence of face processing in social anxiety disorder: A systematic review with meta-analysis. _Neurosci Biobehav Rev_. 2023; **152**: Article 105283.77Pei G, Li T. A literature review of EEG-based affective computing in marketing. _Front Psychol_. 2021; **12**: Article 602843.78Valle-Cruz D, Fernandez-Cortez V, López-Chau A, Sandoval-Almazán R. Does twitter affect stock market decisions? Financial sentiment analysis during pandemics: A comparative study of the H1N1 and the COVID-19 periods. _Cognit Comput_. 2022; **14**(1):372–387.79Gómez LM, Cáceres MN. Applying data mining for sentiment analysis in music. In: De la Prieta F, Vale Z, Antunes L, Pinto T, Campbell AT, Julián V, Neves AJR, Moreno MN, editors. _Trends in cyber-physical multi-agent systems_. Cham: Springer International Publishing; 2018. p. 198–205.80Yu L, Zhang W, Wang J, Yu Y. SeqGAN: Sequence generative adversarial nets with policy gradient. Paper presented at: Proceedings of the AAAI Conference on Artificial Intelligence; 2017; San Francisco, CA, USA. p. 31.81Oliveira HG. A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation. Paper presented at: Proceedings of the 10th International Conference on Natural Language Generation; 2017; Santiago de Compostela, Spain. p. 11–20.82Zhang X, Lapata M. Chinese Poetry Generation with Recurrent Neural Networks. Paper presented at: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2014; Doha, Qatar. p. 670–680.83Mao G, Liu X, Du H, Zuo J, Wang L. Way forward for alternative energy research: A bibliometric analysis during 1994–2013. _Renew Sustain Energy Rev_. 2015; **48**:276–286.84Haustein S, Larivière V. The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects. In: Welpe I, Wollersheim J, Ringelhan S, Osterloh M, editors. _Incentives and performance: Governance of research organizations_. Cham: Springer International Publishing; 2014. p. 121–139.85Hammarfelt B, Rushforth AD. Indicators as judgment devices: An empirical study of citizen bibliometrics in research evaluation. _Res Eval_. 2017; **26**(3):169–180.86Wang J, Veugelers R, Stephan P. Bias against novelty in science: A cautionary tale for users of bibliometric indicators. _Res Policy_. 2017; **46**(8):1416–1436.87Van Eck NJ, Waltman L. Software survey: VOSviewer, a computer program for bibliometric mapping. _Scientometrics_. 2010; **84**(2):523–538.88Šabanović S. Robots in society, society in robots. _Int J of Soc Robotics_. 2010; **2**:439–450.89Hofstede G. _Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations_. London, UK: Sage; 2001.90Mehrabian A. _Communication without words. Communication theory. 2nd ed._ London, UK: Routledge; 2008.91Du S, Tao Y, Martinez AM. Compound facial expressions of emotion. _Proc Natl Acad Sci U S A_. 2014; 111(15):E1454–E1462.92Martinez AM. Computational models of face perception. _Curr Dir Psychol Sci_. 2017; **26**(3):263–269.93Dragano N, Lunau T. Technostress at work and mental health: Concepts and research results. _Curr Opin Psychiatry_. 2020; **33**(4):407–413.94LeDoux J. _The emotional brain: The mysterious underpinnings of emotional life_. New York, NY, USA: Simon and Schuster; 1998.95Pessoa L, Adolphs R. Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. _Nat Rev Neurosci_. 2010; **11**(11):773–782.96Price TF, Peterson CK, Harmon-Jones E. The emotive neuroscience of embodiment. _Motiv Emot_. 2012; **36**:27–37.97Cytowic RE. _Synesthesia: A union of the senses_. Cambridge, MA, USA: MIT Press; 2002.98Guerini M, Strapparava C, Stock O. CORPS: A corpus of tagged political speeches for persuasive communication processing. _J Inf Technol Politics_. 2008; **5**(1):19–32.99Damasio AR. _Descartes’ error_. New York, NY, USA: Random House; 2006.100Scheutz M. The inherent dangers of unidirectional emotional bonds between humans and social robots. In: Lin P, Abney K, Bekey GA, editors. _Robot ethics: The ethical and social implications of robotics_. Cambridge (MA): MIT Press; 2011. p. 205.101Scheutz M, Schermerhorn P. Dynamic robot autonomy: Investigating the effects of robot decision-making in a human-robot team task. Paper presented at: Under review for the 4th ACM International Conference on Human-Robot Interaction; 2009; La Jolla, CA, USA.102Gill R, Singh J. A review of neuromarketing techniques and emotion analysis classifiers for visual-emotion mining. Paper presented at: 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART); 2020; Moradabad, India. p. 103–108.103Pei G, Li B, Li T, Xu R, Dong J, Jin J. Decoding emotional valence from EEG in immersive virtual reality. Paper presented at: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC); 2022; Chiang Mai, Thailand. p. 1469–1476.104Ochs M, Sadek D, Pelachaud C. A formal model of emotions for an empathic rational dialog agent. _Auton Agent Multi-Agent Syst_. 2012; **24**:410–440.### Information#### Published In![]( ComputingVolume 3#### CopyrightCopyright © 2024 Guanxiong Pei et\\\\xa0al.Exclusive licensee Zhejiang Lab. No claim to original U.S. Government Works. Distributed under a .#### Article versions#### Submission history**Received**: 15 October 2023**Accepted**: 6 December 2023**Published online**: 5 January 2024#### Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.#### Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.### Authors#### AffiliationsGuanxiongPei Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.HaiyingLi National Science Library, Chinese Academy of Sciences, Beijing, China.YandiLu Center for Psychological Sciences, Zhejiang University, Hangzhou, China.YanleiWangDe.InnoScience, Deloitte, Shanghai, China.ShizhenHuaResearch Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.TaihaoLi  Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.#### Funding Informationthe Zhejiang Provincial Natural Science Foundation of China : LQ22C090007the National Natural Science Foundation of China : T2241018the National Science and Technology Major Project of the Ministry of Science and Technology of China : 2021ZD0114303the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2023KFKT003the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2022KFKT002#### Notes*Address correspondence to: ### Metrics#### Article UsageArticle Metrics- - No data available.05001,0001,500Jan 2024Apr 2024Jul 2024Oct 2024Jan 2025Apr 2025Jul 202520,97629- - - - Total number ofdownloads and citation**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.Citation information is sourced from  service.#### Altmetrics]( more details]( up by **10** news outlets]( by **7** X users]( **1** Facebook pages]( readers on Mendeley#### Dimensions32Total citations32Recent citationsn/aField Citation Ration/aRelative Citation Ratio]( 10 citation in Web of Science- 29 citation in Crossref#### scite![]( Citations22010Citing PublicationsSupportingMentioningContrastingSee how this article has been cited at scite shows how a scientific paper has been cited by providing the context of the citation, a classification describing whether it supports, mentions, or contrasts the cited claim, and a label indicating in which section the citation was made.### Citations#### Cite as- Guanxiong Pei,- Haiying Li,- Yandi Lu,- Yanlei Wang,- Shizhen Hua,- Taihao Li.Affective Computing: Recent Advances, Challenges, and Future Trends._Intell Comput._2024;3:0076.DOI: #### Export citationSelect the format you want to export the citation of this publication.Please select one from the listRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorksDirect import01. - Jonas Fritsch,    Sticking with Affect in HCI and Design: from Interaction to Relation, Proceedings of the sixth decennial Aarhus conference: Computing X Crisis, (202-215), (2025).     02. - Yuan Zhou,    - Siamak Khatibi,Exploring Artificial Personality Grouping Through Decision Making in Feature Spaces, AI, **6**, 8, (184), (2025). 03. - FNU Rakhi,    Modeling Students’ Emotions in Computing Education: A Context-Specific Multi-Modal Approach, Proceedings of the 2025 ACM Conference on International Computing Education Research V.2, (59-60), (2025).     04. - Jialin Sun,    Research on the Design of Intelligent Voice Interaction System Based on Affective Computing, International Scientific Technical and Economic Research, (1-15), (2025).     05. - Susanne Draheim,    - Kai von Luck,    - Larissa Putzar,Working with the Gaps in Understanding: Revisiting an Approach to Affective Computing Research in the EmotionBike Project, Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments, (596-601), (2025). 06. - Ken Nee Chee,    Affectagogy Unleashed, Pedagogical Approaches to Bridging Emotional and Cognitive Learning, (277-292), (2025).     07. - Joaquim C. Reis,    - Luzia Travado,    - Michael H. Antoni,Advancing health psychology research in oncology: Biobehavioral models, stress pathways, and stress-management interventions for cancer patients, International Journal of Clinical and Health Psychology, **25**, 3, (100615), (2025). 08. - Manuel Paniagua-Gómez,    - Manuel Fernandez-Carmona,Trends and Challenges in Real-Time Stress Detection and Modulation: The Role of the IoT and Artificial Intelligence, Electronics, **14**, 13, (2581), (2025). 09. - Thuy Thi Tran,    - Thang Van Le,    - Nam Hoang Le,    - Anh Van Thi Dam,    - Thao Thi Nguyen,    - Anh Thi Tuan Nguyen,    - Hue Thi Nguyen,Emotional attachment to artificial intelligence and perceived social isolation among university students: An application of Sternberg’s triangular theory of love, Multidisciplinary Science Journal, **7**, 12, (2025662), (2025). 10. - Jan Kincl,    - Marc T. P. Adam,    - Tanja Pavleska,The Role of\\\\xa0Human Factors in\\\\xa0Cybersecurity Incident Management, HCI for Cybersecurity, Privacy and Trust, (35-53), (2025). 11. See moreLoading...### View options#### PDF formatDownload this article as a PDF file### Share#### Copy the article linkCopy LinkCopied!Copying failed.#### Share on social media        | Now Reading:- - \")- - - - - -  xrefBack.goToxrefBack.goToRequest permissionsExpand AllCollapseExpand for moreShow all referencesSHOW ALL BOOKSCopy link✓Thanks for sharing!Find any serviceA2A![](\\'}]\\nUnexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 6, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\\n[{\\'title\\': \\'Best Vibe Coding Tools 2025: Top Picks for Better Coding\\', \\'url\\': \\'https://azumo.com/artificial-intelligence/ai-insights/best-vibe-coding-tools\\', \\'markdown\\': \\'- - - # Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunExplore the 10 best vibe coding tools for 2025, designed to make coding a more enjoyable and efficient experience. These tools enhance developer productivity with features like AI assistance, fast performance, intuitive design, and smooth collaboration. Learn what makes a tool have \"good vibes\" and why selecting the right coding environment is essential for long-term success. Whether you\\\\\\'re looking for minimal setup friction or an elegant interface, these tools will improve your coding journey. From Cursor IDE to GitHub Copilot, find the best fit for your workflow and elevate your development process.Written by:September 2, 2025![]( Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunLet\\\\\\'s be real for a second. You can absolutely build amazing software with basic text editors and command-line tools. But why would you want to? The days of suffering through clunky interfaces and frustrating workflows are long gone. Today\\\\\\'s coding tools aren\\\\\\'t just about functionality anymore; they\\\\\\'re about creating an experience that makes you actually excited to write code.Think about it this way: you probably spend 8+ hours a day staring at your development environment. Shouldn\\\\\\'t it feel good? Shouldn\\\\\\'t it make you more productive instead of fighting you every step of the way? That\\\\\\'s exactly what we\\\\\\'re talking about when we say \"vibe\" in coding tools.The numbers back this up, too. According to Stack Overflow\\\\\\'s 2024 Developer , 76% of developers are now using or planning to use AI coding assistants. The entire software development tools market is projected to hit . This isn\\\\\\'t just about new features anymore; it\\\\\\'s about developer happiness.At Azumo, we’ve worked with leading companies like Meta, Wolters Kluwer, and Wine Enthusiast to deliver cutting-edge AI-driven solutions, helping teams optimize their workflows and enhance business outcomes.Our deep expertise in  and our  give us a unique perspective on the best tools available in the coding space. We understand that the right tools can dramatically improve a developer\\\\\\'s experience and productivity, which is why we’re excited to share our insights on the best vibe coding tools for 2025.## What Makes a Coding Tool Have \"Good Vibes\"?Before we jump into the tools themselves, let\\\\\\'s talk about what we actually mean by \"good vibes\" in a development environment. It\\\\\\'s not just about pretty colors or trendy interfaces (though those don\\\\\\'t hurt).**Smooth Performance That Doesn\\\\\\'t Get in Your Way:** You know that feeling when you\\\\\\'re in the zone and your editor starts lagging? Total vibe killer. Great tools respond instantly to your input, handle large files without breaking a sweat, and never make you wait around while they \"think.\"**Intuitive Design That Feels Natural:** The best coding tools feel like they were designed by someone who actually codes. They put the most important features right where you expect them, use shortcuts that make sense, and don\\\\\\'t make you hunt through menus for basic functionality.**Smart Assistance Without Being Pushy:** Modern tools are getting really good at predicting what you want to do next. The good ones offer helpful suggestions and automate tedious tasks. The great ones do it without interrupting your flow or making you feel like the computer is trying to do your job for you.**Beautiful Aesthetics That Reduce Eye Strain:** Look, we\\\\\\'re not being shallow here. When you\\\\\\'re staring at code for hours, having clean typography, thoughtful color schemes, and well-organized layouts actually reduces cognitive load. Your brain doesn\\\\\\'t have to work as hard to parse what you\\\\\\'re looking at.**Strong Community and Ecosystem:** Tools with great vibes usually have passionate communities behind them. That means better documentation, more extensions, helpful tutorials, and people who actually want to help when you run into problems.**Flexibility to Match Your Workflow:** Everyone codes differently. Some people love keyboard shortcuts, others prefer clicking around. Some want everything in one window, others like to spread things across multiple monitors. The best tools adapt to how you work, not the other way around.**Minimal Setup Friction:** Nothing kills enthusiasm faster than spending three hours trying to configure a tool before you can write your first line of code. Great tools either work beautifully out of the box or make customization genuinely enjoyable.This combination of factors creates what developers often describe as \"flow state\"—that magical zone where you\\\\\\'re completely absorbed in solving problems and everything just works. That\\\\\\'s what we\\\\\\'re looking for in our tool selection.## Top 10 Best Vibe Coding Tools### 1. !If we\\\\\\'re talking about tools that completely change how coding feels, Cursor has to top the list. This isn\\\\\\'t just another editor with AI bolted on; it\\\\\\'s been built from the ground up around the idea of AI-human collaboration.What makes Cursor special is how naturally the AI assistance flows into your regular coding process. Instead of feeling like you\\\\\\'re constantly asking a chatbot for help, it\\\\\\'s more like having a really smart pair programming partner who knows exactly what you\\\\\\'re trying to build. Cursor IDE has emerged as particularly noteworthy, making waves as one of the best AI coding tools available due to its superior auto-completion features and integration capabilities.#### Key Features:- Context-aware code completion that actually understands your project- Natural language to code conversion that works surprisingly well- Built-in chat interface for explaining complex code sections- Multi-file editing with AI understanding the relationships between files- Beautiful, clean interface that stays out of your way**Why It Has Great Vibes:** Cursor feels like coding in the future. The AI suggestions are smart enough to be helpful but not so aggressive that they interrupt your thought process. Plus, the interface is clean and modern without being distracting.### 2. !VS Code basically won the editor wars, and for good reason. Visual Studio Code continues to dominate the IDE landscape, establishing itself as the cornerstone development environment regardless of programming language, whether JavaScript, Python, or C++.What\\\\\\'s amazing about VS Code isn\\\\\\'t just that it\\\\\\'s free and powerful; it\\\\\\'s that Microsoft somehow made it feel lightweight and fast despite having more features than most full IDEs. The extension ecosystem is incredible, which means you can customize it to feel exactly right for any type of project.#### Key Features:- Massive extension marketplace covering every language and framework- Integrated terminal that actually works well- Git integration that makes version control painless- IntelliSense that\\\\\\'s smart without being annoying- Cross-platform consistency**Why It Has Great Vibes:** VS Code strikes the perfect balance between power and simplicity. It\\\\\\'s familiar enough that you can jump right in, but deep enough that you\\\\\\'ll keep discovering new features months later. The community support means there\\\\\\'s always someone who\\\\\\'s solved whatever problem you\\\\\\'re facing.### 3. !GitHub Copilot isn\\\\\\'t an editor itself, but it transforms whatever editor you\\\\\\'re using. According to , GitHub Copilot stands as the second most popular AI coding tool, having been tried by 40% of developers and regularly used by 26% of respondents.What\\\\\\'s wild about Copilot is how it actually learns your coding style over time. It\\\\\\'s not just suggesting random code snippets—it\\\\\\'s suggesting code that looks like something you would write, just faster than you could type it.#### Key Features:- Real-time code suggestions as you type- Support for dozens of programming languages- Context awareness that spans multiple files- Integration with the most popular editors- Comment-to-code generation**Why It Has Great Vibes:** Copilot feels like having a coding buddy who\\\\\\'s really good at remembering syntax and common patterns. It handles the tedious stuff so you can focus on the interesting problems. Plus, when it suggests exactly what you were about to type, it\\\\\\'s genuinely satisfying.### 4. !Zed is the new kid on the block, but it\\\\\\'s making waves for all the right reasons. Built in Rust, it\\\\\\'s designed from the ground up for performance and collaboration. If you\\\\\\'re tired of editors that slow down as your projects get bigger, Zed might be exactly what you need.#### Key Features:- Lightning-fast startup and response times- Real-time collaborative editing- Beautiful, minimalist interface- Built-in AI assistance- Memory-efficient architecture**Why It Has Great Vibes:** Zed feels incredibly snappy and responsive. There\\\\\\'s something satisfying about an editor that never makes you wait. The collaborative features are particularly smooth—you can literally watch your teammates code in real-time without any of the lag or sync issues you get with other tools.### 5.  (via Command Line)!According to , the best AI developer tools in 2025 include Aider, Cursor, Zed, Claude Code, Windsurf, and GitHub Copilot, representing the cutting edge of AI-assisted development.Claude Code brings AI assistance directly to your terminal, which is perfect if you\\\\\\'re someone who lives in the command line. It\\\\\\'s particularly good at understanding complex codebases and making suggestions that actually make sense in context.#### Key Features:- Terminal-native AI assistance- Deep understanding of project structure- Excellent at refactoring and code optimization- Natural language query capabilities- Git integration for tracking changes**Why It Has Great Vibes:** If you\\\\\\'re a terminal person, Claude Code feels like having a really smart assistant who speaks your language. It doesn\\\\\\'t try to force you into a GUI—it works within the workflow you already love.### 6. !Windsurf represents the next generation of AI-integrated development environments. It\\\\\\'s designed around the principle that AI and human developers should work together seamlessly, not as separate entities.#### Key Features:- Deep AI integration across all features- Intelligent project understanding- Advanced debugging assistance- Collaborative AI agent capabilities- Modern, intuitive interface**Why It Has Great Vibes:** Windsurf feels like what development environments will look like in five years. The AI integration is so smooth that it doesn\\\\\\'t feel like you\\\\\\'re using an AI tool—it just feels like you\\\\\\'ve gotten really good at coding really fast.### 7. !Don\\\\\\'t sleep on Sublime Text. According to , Sublime Text maintains its reputation for exceptional performance, known for quick response times and the ability to handle large files with ease, featuring innovative multi-caret editing capabilities that allow developers to edit multiple lines simultaneously.While everyone else has been adding AI and collaborative features, Sublime Text has been perfecting the core editing experience. Sometimes you just want an editor that\\\\\\'s incredibly fast, looks beautiful, and never gets in your way.#### Key Features:- Incredibly fast performance, even with huge files- Multi-caret editing for complex refactoring- Clean, distraction-free interface- Powerful search and replace capabilities- Minimal memory usage**Why It Has Great Vibes:** Sublime Text feels like a precision instrument. Every interaction is crisp and immediate. It\\\\\\'s the editor equivalent of a really good mechanical keyboard: simple, reliable, and satisfying to use.### 8. !Aider is a command-line AI coding assistant that\\\\\\'s particularly good at making changes to existing codebases. If you work with large, complex projects and need AI help that actually understands the bigger picture, Aider is worth checking out.#### Key Features:- Git-aware AI assistance- Excellent at understanding large codebases- Command-line focused workflow- Automated testing integration- Smart refactoring capabilities**Why It Has Great Vibes:** Aider respects your existing workflow while making it significantly more powerful. It doesn\\\\\\'t try to change how you work—it just makes you better at what you\\\\\\'re already doing.### 9. !Replit has transformed from a simple online coding playground into a full-featured development environment. What makes it special is how quickly you can go from idea to running code, especially for collaborative projects.#### Key Features:- Zero-setup development environment- Real-time collaboration features- Built-in deployment and hosting- AI-powered code assistance- Educational-friendly features**Why It Has Great Vibes:** Replit removes all the friction from getting started. You can literally go from having an idea to sharing a working prototype in minutes. It\\\\\\'s particularly great for experimenting with new technologies or collaborating with people who might not have development environments set up.### 10. !Tabnine focuses specifically on AI-powered code completion, and it\\\\\\'s really good at it. What sets it apart is the focus on privacy and the ability to train on your own codebase to provide more relevant suggestions.#### Key Features:- Privacy-focused AI completion- Team training capabilities- Multi-language support- Integration with the most popular editors- On-premises deployment options**Why It Has Great Vibes:** Tabnine feels thoughtful and respectful of your work. The privacy focus means you can use it on sensitive projects, and the team training features help it get better at suggesting code that matches your team\\\\\\'s style and standards.## Productivity Enhancement Features That MatterBeyond the core editing experience, the tools that have the best vibes integrate seamlessly with the rest of your development workflow. According to , IDEs substantially boost developer productivity by consolidating all necessary coding tools into one cohesive workspace, providing access to features like built-in error detection and version control integration rather than requiring manual coding in simple text editors.**Version Control Integration:** The best coding tools make Git feel natural instead of scary. They show you what\\\\\\'s changed, make it easy to stage and commit files, and help you resolve merge conflicts without wanting to throw your computer out the window.**Project Management Connections:** According to , popular project and task management tools that enhance the coding experience include Jira, Trello, Asana, and Microsoft Planner, while continuous integration and deployment tools have become essential components of modern software development practices. When your editor can show you which issues you\\\\\\'re working on or automatically update task status based on your commits, it creates a much smoother workflow.**CI/CD Pipeline Integration:** Nothing kills momentum like having to switch between your editor and a web browser to check if your build passed. The best tools give you build status right in your editor and make it easy to see what went wrong when something breaks.**Real-Time Collaboration:** Whether you\\\\\\'re pair programming or just need to show someone a bug, tools that make collaboration feel natural rather than awkward create much better team experiences.## The Future of Vibe-Focused Development ToolsWe\\\\\\'re in the middle of a major shift in how development tools work. According to , product innovation in the software development tools market continues to expand rapidly, with nearly 57% of newly introduced tools now supporting low-code or no-code development approaches.AI integration is becoming table stakes, but the focus is shifting from \"AI that can code\" to \"AI that helps you code better.\" We\\\\\\'re seeing tools that understand not just syntax but intent, that can refactor entire codebases while maintaining your coding style, and that can explain complex code in plain English.The developer community is also driving innovation in ways we haven\\\\\\'t seen before. According to , which received over 49,000 responses from 177 countries, provides crucial insights into the evolving developer landscape. The survey focuses on 314 different technologies, with new emphasis on AI agent tools, LLMs, and community platforms, indicating where the industry is heading.Visual design and user experience are becoming just as important as functionality. Developers are no longer willing to use tools that feel clunky or outdated, even if they\\\\\\'re technically powerful.## Choose Your Coding VibeHere\\\\\\'s the thing about coding tools—the \"best\" one is the one that fits how you work and think. If you love keyboard shortcuts and never want to touch a mouse, Sublime Text or Vim-style editors might be your jam. If you\\\\\\'re working on a team and need seamless collaboration, Cursor or Zed could be game-changers. If you\\\\\\'re just getting started and want something that works well out of the box, VS Code is still hard to beat.The good news is that most of these tools are free or have generous trial periods, so you can actually try them out and see what clicks. Don\\\\\\'t just read reviews—actually use them for a few days on real projects. You\\\\\\'ll know pretty quickly which ones feel right.The software development industry continues to grow at an incredible pace, with the developer community expanding from 27 million to a projected  in the next decade. As this community grows, the demand for tools that not only function well but actually make coding enjoyable will only increase.Remember, you\\\\\\'re going to spend thousands of hours with whatever tools you choose. Pick ones that make those hours feel productive, creative, and maybe even fun. The best coding tool is the one that gets out of your way and lets you focus on building amazing things.Try a few of these tools this week. Set up Cursor for your next AI-assisted project, give Zed a spin if you want something blazingly fast, or explore how GitHub Copilot could speed up your current workflow. The future of coding is here, and it feels pretty good.**Ready to level up your development experience?** At , we understand that great software comes from developers who have access to the best tools and environments. Our nearshore development teams use cutting-edge tools and AI-powered workflows to build intelligent applications that drive real business results. Whether you\\\\\\'re looking to augment your existing team or build something completely new, we\\\\\\'d love to show you how the right combination of talent and tools can transform your development process.\\\\u200d![]( the Author:ML/AI & Backend EngineerGuillermo Germade, Data Science Expert at Azumo, specializes in building machine learning models and AI systems, focusing on consumer tech, entertainment, and big data. reCAPTCHASelect all images with **cars** Click verify once there are none left|     |     |     || --- | --- | --- || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( |Please try again.Please select all matching images.Please also check the new images.Please select around the object, or reload if there are none.Verify\\'}, {\\'title\\': \\'Top 10 Vibe Coding Tools That Feel Like Magic in 2025\\', \\'url\\': \\'https://dev.to/therealmrmumba/top-10-vibe-coding-tools-that-feel-like-magic-in-2025-1md\\', \\'markdown\\': \\'AI is no longer just assisting developers—it’s transforming how we build software. Whether you\\\\\\'re a solo indie hacker, a startup engineer, or part of an enterprise team, there’s now an AI tool that aligns with your workflow. The best ones don’t simply automate tasks; they feel intuitive, context-aware, and remarkably efficient.* * *## Why These Tools Matter in 2025Software development has evolved. We’ve moved past the era of simple code autocomplete and template generators. Today, developers demand tools that integrate deeply with their tech stack, adapt to their workflow, and help them ship faster without compromising quality.Here are three key trends that shape this new landscape:1. **Autonomous Assistance** – Developers now rely on AI tools that can complete multi-step tasks with minimal guidance, from writing modules to fixing bugs.2. **Context-Aware Coding** – Tools that understand your full codebase, API structure, and data flow are no longer a luxury—they’re a necessity.3. **Real-Time Collaboration** – AI is becoming a collaborative partner in the development process, offering suggestions, generating code, and even reviewing pull requests in real-time.]( list focuses on tools that exemplify these traits and offer genuine improvements to the way developers work in 2025.* * *## The Top 10 Vibe Coding Tools in 2025### 1. Apidog MCP Server**The missing link between your API and AI coding assistants** bridges the gap between your backend and tools like Cursor, Cline, and Copilot Workspace. Once your OpenAPI definitions are connected, you can:- Autogenerate API logic and DTOs based on updated schema- Let AI tools access real-time API documentation for better suggestions- Streamline backend/frontend development across large teams]( especially powerful in teams that maintain frequently changing APIs or use domain-driven design.* * *### 2. Cursor]( code editor designed for pair programming with GPT-4** is built on VS Code but reimagines the editing experience with AI at the center. Unlike plugins, Cursor deeply integrates AI into every layer:- Offers context-aware autocomplete that understands your full project- Supports inline debugging and refactoring with natural language prompts- Can explain, rewrite, or extend code based on your cursor positionIt’s ideal for fast iteration, onboarding into unfamiliar codebases, or accelerating solo projects.* * *### 3. Copilot Workspace]( task-oriented Copilot that tackles full development cycles**Copilot Workspace is a major leap from GitHub Copilot. Rather than suggesting code line by line, it can:- Understand a GitHub issue and break it into subtasks- Write full PRs for those subtasks- Summarize what it did and wait for your feedbackThis makes it perfect for junior developers, product engineers, or teams with many open tickets to manage.* * *### 4. Sweep]( that turns GitHub issues into functioning pull requests** is a bot that integrates directly into your GitHub repo. You describe an issue, and it:- Scans the entire repo to understand the current architecture- Writes a PR that resolves the issue- Updates associated files and testsFor lean teams or solo founders managing multiple products, Sweep can save hours of context-switching.* * *### 5.Fine by Fine.dev]( is the AI assistant that startup CTOs have been dreaming of. This AI-powered development platform automates tasks throughout the software development lifecycle, from coding and debugging to testing and code review.What makes Fine special is its ability to work autonomously—it can create and implement code, conduct pull request reviews, generate tests, and handle common issues without constant human input. The GitHub integration and support for asynchronous work make it particularly suitable for distributed teams.* * *### 6. Continue]( with your codebase in any editor** is an open-source extension that works in VS Code and JetBrains. It allows you to:- Chat with your entire codebase- Ask for refactors, explanations, or generation tasks- Use local models or connect with remote LLMs like Claude, GPT-4, or MistralIt’s the go-to for developers who want full AI capabilities without being locked into a closed ecosystem.* * *### 7. The Windsurf Editor by Codeium]( to Keep you in flow state** is revolutionizing the IDE concept by seamlessly integrating AI as a collaborator rather than just a tool. The platform offers a unique blend of AI copilots and autonomous agents that work together with developers to enhance productivity.The Cascade feature, which provides deep contextual awareness across your codebase, has helped me understand and navigate unfamiliar codebases much faster than traditional methods. The multi-file editing and predictive actions keep me in a constant flow state, which has noticeably improved my productivity.* * *### 8. Lazy AI]( app creation with AI-powered workflows** makes building applications almost effortless. Designed for speed and simplicity, it lets users create full-stack apps without writing code. Its standout features include:- Pre-built workflows for common developer and automation tasks- One-click deployment for both frontend and backend projects- Customizable templates for bots, AI tools, dev utilities, and marketing apps- Minimal skill requirement—ideal for rapid prototyping or solo makersIt’s the perfect fit for anyone looking to bring ideas to life without the overhead of traditional development.* * *### 9. Devika**Your open-source AI software engineer** is an open-source autonomous AI agent that takes high-level instructions and breaks them down into actionable development tasks. Designed to behave like a junior developer, it combines:- Advanced planning and reasoning algorithms- Contextual keyword extraction for precision- Dynamic agent tracking and decision-making- Web browsing to supplement gaps in knowledgeDevika stands out as a community-driven alternative to commercial AI coding platforms. It’s ideal for developers looking to delegate complex coding work while retaining full control and transparency over the process.* * *### 10. Cody by Sourcegraph]( code understanding with full repo context** is designed to:- Answer questions across massive codebases- Perform intelligent refactors based on architecture- Search, navigate, and explain with precisionIts integration with Sourcegraph gives it superpowers in monorepos, enterprise codebases, and multi-language projects.### AI Coding Tools in Practice: Use Case ScenariosTo better understand how these AI coding tools can be applied in real-world settings, here are a few practical scenarios that demonstrate their potential across different developer environments:#### **Solo Developer: Boosting Productivity with Cursor + Continue**Independent developers often juggle multiple roles—from writing code to debugging and managing small deployments. Tools like **Cursor** (an AI-native code editor) combined with **Continue** (a Copilot-like extension that integrates into IDEs) offer a significant productivity boost. Together, they streamline code suggestions, automate refactoring, and provide quick answers without leaving the editor, allowing solo devs to maintain momentum and ship faster.#### **Startup Team: Automating Ticket Resolutions with Sweep + Copilot Workspace**Early-stage startups typically operate with lean engineering teams that need to move fast. Tools like **Sweep**, which auto-generates pull requests from GitHub issues, and **Copilot Workspace**, which can create and manage entire features autonomously, allow small teams to maintain high output. These tools handle boilerplate tasks and bug fixes, freeing up developers to focus on high-impact product features.#### **Enterprise: Using Cody + Tabnine for Large, Secure Codebases**For enterprises with massive codebases and stricter security requirements, **Cody** (from Sourcegraph) and **Tabnine** offer scalable AI integration. Cody excels at understanding large repositories and referencing documentation or historical code changes. Tabnine, known for its privacy-first model, ensures that proprietary code doesn’t leave company boundaries, making it ideal for secure enterprise environments.#### **Data Scientist: Speeding Up EDA with Cogram + CodeSquire**Data scientists benefit greatly from tools that can reduce time spent on repetitive analysis. **Cogram** helps generate SQL queries and Jupyter code based on natural language, while **CodeSquire** personalizes code suggestions based on the user’s unique style and workflows. These tools accelerate exploratory data analysis (EDA) and modeling, letting scientists focus more on insights and less on syntax.* * *## Final Thoughts2025 isn’t about whether you use AI as a developer—it’s about how. The tools listed above aren’t just trendy—they’re reshaping the development experience by embedding intelligence into your daily workflow. Some extend your capabilities. Others remove tedious work. The best ones do both.Choose the ones that match your stack, integrate with your workflow, and make you feel like a better developer—not just a faster one.* * *Sentry]( PromotedDropdown menu- - * * *- ]( Read More!Templates let you quickly answer FAQs or store snippets for re-use.SubmitPreview CollapseExpand]( David]( David]( David![]( of Postiz, an open-source social media scheduling tool.Running Gitroom, the best place to learn how to grow open-source tools.- EducationDidn\\\\\\'t finish high school :(- PronounsNev/Nevo- WorkOSS Chief @ Gitroom- JoinedFeb 23, 2022• Dropdown menu- - Hide- Neat how coding is getting easier, like having a little helper doing chores for you. But if AI keeps taking on bigger parts of the job, what skills should people still work on to stay sharp in tech?CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Totally! As AI handles more, staying sharp in problem-solving, system design, and critical thinking is key. Knowing why things work still sets you apart.CollapseExpand]( Sečkár]( Sečkár]( Sečkár![]( am from Czechia, born in 1988. Studied computer science at KIT VŠE, Prague. Currently working as senior Java & JS developer for Eviden.- Email- LocationPrague, Czechia- EducationKIT VŠE, Prague- PronounsHe- WorkSenior application developer @ Eviden- JoinedOct 21, 2023• Dropdown menu- - Hide- business analysis, software architecture, observability, scalability, security...to name a fewCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- And to continue on the reply of , convert abstract requirements to good software requirements is still key, AI can only hallucinate solutions that most likely be wrong or wrongly translated.Software architecture and applying best practices is also still important, AI only generate some code which might not scale very well. Or doesn\\\\\\'t fit the overal design, or worse it\\\\\\'s using deprecated functionality. So a human developer should be really keen on it and not just accept all the code. How well it even might look like. It\\\\\\'s still just a tool.CollapseExpand]( Lopez]( LopezAudrey Lopez]( JoinedMar 6, 2025• Dropdown menu- - Hide- Nice post, Emmanuel Mumba!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Thanks Lopez! Glad you enjoyed it.CollapseExpand]( Sebastian]( SebastianRalph Sebastian]( JoinedAug 10, 2023• Dropdown menu- - Hide- Nice list. By the way, Windsurf has free GPT 4.1 access for now (it will not last long), worth trying out.CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Oh nice, thanks for the heads-up! I’ll definitely check Windsurf out before the window closes.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Openai is indeed delivering free gpt 4.1 while in preview.But also o4-mini models I believe. Mini-high, Etc.CollapseExpand]( Lee]( LeeJennie Lee]( JoinedDec 18, 2024• Dropdown menu- - Hide- OK, vibe coding is getting hot this time!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Haha, for real! 🔥 Coding’s definitely having its moment, More like the vibe is leveling up every week.CollapseExpand]( the Coder]( the CoderJosh the Coder]( Coder that is Interested in AI, No Code. Learning forever!- JoinedNov 27, 2023• Dropdown menu- - Hide- OK, this is a good list. I am using Cursor but also willing to explore some alternatives. Good work Emmanuel!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Appreciate that! 🙌 Cursor’s solid, always cool to explore what else is out there too. Let me know if you find any gems.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Windsurf is really good. Since they at least providea decent deb file setup, causing less issues on my linux setup than the stupid appimages from cursor.CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- I\\\\\\'ve spent more time trying to fix our juniors vibe coded problems than I can imagine. And what\\\\\\'s worse is having to explain why. They have no idea what the code they are pumping out does. It\\\\\\'s a horrifying trend. Hopefully this doesn\\\\\\'t spread to all industries and we end up with vibe built airplanes LOLCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Haha I was already afraid this will happen 2 years ago. And now we are at this stage where agents are commonly available and used in editors.It\\\\\\'s powerful for experienced devs. But juniors should really not just using it and blindly accept it. That is not how you grow in your knowledge or carrière. Instead of you fixing their issues, they should take full responsibility of the code they put out for review.CollapseExpand]( Orion]( OrionAshe Orion]( JoinedAug 12, 2024• Dropdown menu- - Hide- Okay so as a developer in the making, at what point in my learning process will these tools stop being a hindrance to my education and start being actual real usable tools and not like Nintendo 64 cheat codes because right now, having nothing but HTML CSS and some very intermediate vanilla JavaScript at my disposal (and only as it relates to the front-end. Getting at back end soon) even just the Codium Auto-complete in VS was detracting from my ability to build a solid foundation by typing through my full lines of code. I noticed that when I had auto complete enabled, pretty much immediately anytime I was working in other environments without an autocomplete buddy by my side my syntax absolutely suffered because my monkey brain wasn\\\\\\'t getting the muscle memory of typing out the proper syntax and to be honest it was a little embarrassing in Zoom meetings with my tutors to just sit there and fumble multiple times on things like spacing or back ticks or, dare I out myself, neglected line termination in ; and }However I definitely see the value of not having to type out very obvious return statements or whatever other repetitive this.thing = thething; -esque scenarios that when you add them all up really do burn giant holes your productive work hours. I\\\\\\'m just so terrified of using it too early, leaving the safety of being an \\\\\\'aspiring engineer\\\\\\' for the reality of the job market, finally landing an interview only to realize that while I thought I was learning to code I\\\\\\'d just been surfing the vibe.CollapseExpand]( Enlightened]( EnlightenedAaron Enlightened]( Developer using AI tools for programming but not depending on them completely!- JoinedApr 18, 2025• Dropdown menu- - Hide- LOL ... I know exactly how you feel! I am at a similar junction in my programming journey and this is what I do to not feel like an imposter later on - Open a plain .txt file and just type some HTML and CSS code (along with vanilla JavaScript if you want to) into it, then save the file as \"index.html\", and then open this file in Google Chrome browser and see the result. To modify or edit any code that you feel is giving unexpected results, just use the Chrome Developer tools. This will give you confidence in YOUR coding prowess with ZERO autocomplete using any type of AI. Do this for just 15-20 minutes every day for 1 month and you will have solved the imposter syndrome to a large extent.For all the rest of your tutorials use the autocomplete feature on VSCode Editor, but try to ask ChatGPT \"why\" was some particular code snippet suggested by the AI autocomplete. This will help you in understanding the WHY of AI coding and will help make you a better programmer.BTW, I signed up on DEV just to post this comment. So, thank you for that. ;-)CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- It\\\\\\'s a tough gig for new devs, you are entering a world where tools exist that actually hinder your understanding of what\\\\\\'s going on.At the risk of showing my age, id say it\\\\\\'s akin to why the monk will make the initiate carry pales of water to then monastery before he learns the king fu. To generate an understanding of the fundamentals of movement, patience and discipline.Some people a lot older than me would say using an IDE is not true development and that real devs can code in notepad.Tools evolve and as long as they fill the purpose of making coding easier, but still allow you to learn and gain the knowledge and understanding I wouldn\\\\\\'t see a problem with using them. But as you pointed out, it can be a hindrance, and you actually realizing that and accepting it means you will succeed where many of your peers won\\\\\\'t.AI is just another tool, it\\\\\\'s great when you already have an understanding of how systems should work and just want some snippets. But the problem we are seeing is a misuse of these tools. The blind copy paste vibe coding is not creating developers it\\\\\\'s creating slop.Here\\\\\\'s my ultimate advice though, AI doesn\\\\\\'t have to be just a tool, it can be an amazing teacher. It can explain things to you in anyway you want , if you ask it to simplify and breakdown topics it will. It can explain things to you as if you were a toddler. USE THIS.Vibe code your heart out but every single time it gives you code you do not understand, ask it to explain it you. You will be amazed how quickly it goes from hindrance to game changing.CollapseExpand]( jones]( jonessam jones]( JoinedApr 16, 2025• Dropdown menu- - Hide- Very informative article. Although not at all what I expected from title \"Vibe Coding\". In industry, Vibe Coding is being used to signify low code/no code or citizen development. Basically, the same holy grail that the industry has been searching for since Oracle Forms or earlier.BUt putting the Title aside, looking forward to trying out some of your suggestions.CollapseExpand]( Mottley]( MottleyDaley Mottley]( Consultant | Full Stack Web Developer- Email- LocationBarbados- JoinedOct 8, 2024• Dropdown menu- - Hide- Nice post, Emmanuel!Have you tried ?It\\\\\\'s a pretty decent vibe coding tool. I love it... 😅Thanks for sharing 👍CollapseExpand]( Green]( GreenStuart Green]( JoinedApr 18, 2025• Dropdown menu- - Hide- Crazy times ahead. Can only imagine the downstream impact these tools will have (personalized software, rise of individual creators, etc).I also love platforms like  and  for \"vibe coding\" without the actual code. Though harder to build more advanced features than with Cursor, etc.Some comments may only be visible to logged-in visitors.  to view all comments.Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\\\\\'s .Hide child comments as wellConfirmFor further actions, you may consider blocking this person and/or Embeddable]( PromotedDropdown menu- - * * *- ]( Embed in minutes, load in milliseconds, extend infinitely. Import any chart, connect to any database, embed anywhere. Scale elegantly, monitor effortlessly, CI/CD & version control.👋 Kindness is contagiousDropdown menu- - * * *- CloseDive into this thoughtful piece, beloved in the supportive DEV Community. **Coders of every background** are invited to share and elevate our collective know-how.A sincere \"thank you\" can brighten someone\\\\\\'s day—leave your appreciation below!On DEV, **sharing knowledge smooths our journey** and tightens our community bonds. Enjoyed this? A quick thank you to the author is hugely appreciated.### !We\\\\\\'re a place where coders share, stay up-to-date and grow their careers. ![](\\'}]\\nUnexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 8, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_URZ5D9aGNTS35obDjthzg375', arguments='{\"query\": \"emotion-aware programming education IEEE 2022\"}', name='web_search_tool'), FunctionCall(id='call_0QfXVfV9DNu7IVPtgrFNajy7', arguments='{\"query\": \"affective computing programming software engineering mood-based development 2023 2024\"}', name='web_search_tool'), FunctionCall(id='call_d74ZAGK7DVpc4VJFOMsgev51', arguments='{\"query\": \"AI-assisted coding impact study 2024 2025 Stack Overflow Developer Survey\"}', name='web_search_tool'), FunctionCall(id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', arguments='{\"query\": \"emotion-based coding tools 2024 2025\"}', name='web_search_tool'), FunctionCall(id='call_S9VoFTZt8NefRPNRbUH4tZQg', arguments='{\"query\": \"cursor windsour codeium vibe coding 2025 article\"}', name='web_search_tool')], results=[FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 7, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_URZ5D9aGNTS35obDjthzg375', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Affective Software Engineering\\', \\'url\\': \\'https://affectivese.org/\\', \\'markdown\\': \"!We are a group of computer scientists whose research focuses on using Affective Computing methods in the field of Software Engineering. Our interests include, but are not limited to, the role of emotions in the software development process, reusing datasets from Affective Computing experiments as well as improving the teaching process of future software engineers.We are open to any kind of cooperation. Do you want to do research together? Prepare a paper? Submit a project proposal? If you are conducting research at the intersection of the fields of affective computing and software engineering feel free to contact us via . Take a look at our current research and projects and get in touch!### DevEmo datasetThe DevEmo dataset consists of 217 video clips showing students\\' facial expressions as they solve programming tasks. The recordings were collected in the participants\\' actual work environment. The video clips are annotated with five labels, including four emotions (anger, confusion, happiness and surprise) and a neutral state.In order to facilitate the annotation of a large number of video recordings, we have developed AffecTube. It is a comprehensive solution that leverages crowdsourcing to annotate videos directly on the YouTube platform, resulting in ready-to-use emotion-annotated datasets. AffecTube provides a low-resource environment with an intuitive interface and customizable options, making it a versatile tool applicable not only to emotion annotation, but also to various video-based behavioral annotation processes.If you are interested in using the DevEmo dataset or AffecTube check out the projects\\' websites:Journal papers:### Affective data reausabilityModern research in Affective Computing, often rely on datasets containing experiments-originated data such as biosignals, video clips, or images. Moreover, conducting experiments with a vast number of participants to build datasets for Affective Computing research is time-consuming and expensive. Therefore, it is extremely important to provide solutions allowing one to (re)use data from a variety of sources, which usually demands data integration.In the research conducted so far, we have proposed the ROAD (Recording Ontology for Affective-related Datasets) ontology and its implementation with the graph knowledge base in the form of the GRISERA framework.If you are interested in our approach take a look at:Journal papers:### Emotions in the software development processEmotions play an important role in any human being\\'s life, including software developers. The aim of our research is to determine what emotions are optimal in the software development process, how to recognize them and how (if at all) to influence them.Publications:### Affect-aware software engineering educationWith the Covid-19, the role of distance learning has increased. The aim of the research is to develop methods that, using emotion recognition techniques, will allow for more effective remote learning of programming.Publications:### Affective loop in Socially Assistive Robotics as an intervention tool for children with autismEMBOA project entitled “Affective loop in Socially Assistive Robotics as an intervention tool for children with autism” is a research and didactic project that aims at enhancement of social robot intervention in children with autism with affective computing technologies. The project is implemented in years 2019-2022 under EU Erasmus Plus Strategic Partnership for Higher Education Programme.More information on project website: Wrobel MR., Landowska A., Makuch K., , Proceedings of the 32nd International Conference on Information Systems Development (ISD 2024), 2024Barkana DE., Bartl-Pokorny KD., Kose H, Landowska A., Milling M., Robins B., Schuller BW., Uluer P., Wrobel MR., Zorcec T., , International Journal of Social Robotics, 2024Kulas D., Wrobel MR., , SoftwareX, 2023Manikowska M., Sadowski D., Sowinski A., Wrobel MR, , Applied Sciences, 2023Zawadzka T, Wierciński T, Waloszek W, Wróbel MR, , IEEE Access, 2023Zawadzka T, Wierciński T, Meller G, Rock M, Zwierzycki R, Wróbel MR, , Sensors, 2021Zawadzka T, Waloszek W, Karpus A, Zapałowska S, Wróbel MR, , IEEE Access , 2021Bartl-Pokorny KD, Pykala M, Uluer P, Barkana DE, Baird A, Kose H, Zorcec T, Robins B, Schuller BW, Landowska A, , IEEE Access , 2021Wróbel MR, , IEEE Access , 2020Blincoe K, Springer O, Wrobel MR, , IEEE Software, 2019Wrobel MR, Zielke AW, , 2018 Federated Conference on Computer Science and Information Systems (FedCSIS), 2018Wrobel MR, , Applied Sciences, 2018Landowska A, Brodny G, , IEEE Access, 2018Landowska A , , Applied Sciences , 2018Wrobel MR, , 2016 Federated Conference on Computer Science and Information Systems (FedCSIS), 2016Brodny G, Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 2016 9th International Conference on Human System Interactions (HSI), 2016Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Information Systems Development and Applications, 2015Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , Human-Computer Systems Interaction: Backgrounds and Applications 3, 2014Kolakowska A, Landowska A, Szwoch M, Szwoch W, Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013Wrobel MR, , 6th International Conference on Human System Interactions (HSI), 2013\"}, {\\'title\\': \\'Affective Computing: Recent Advances, Challenges, and Future ...\\', \\'url\\': \\'https://spj.science.org/doi/10.34133/icomputing.0076\\', \\'markdown\\': \\'AdvertisementMain content starts hereContents## AbstractAffective computing is a rapidly growing multidisciplinary field that encompasses computer science, engineering, psychology, neuroscience, and other related disciplines. Although the literature in this field has progressively grown and matured, the lack of a comprehensive bibliometric analysis limits the overall understanding of the theory, technical methods, and applications of affective computing. This review presents a quantitative analysis of 33,448 articles published in the period from 1997 to 2023, identifying challenges, calling attention to 10 technology trends, and outlining a blueprint for future applications. The findings reveal that the emerging forces represented by China and India are transforming the global research landscape in affective computing, injecting transformative power and fostering extensive collaborations, while emphasizing the need for more consensus regarding standard setting and ethical norms. The 5 core research themes identified via cluster analysis not only represent key areas of international interest but also indicate new research frontiers. Important trends in affective computing include the establishment of large-scale datasets, the use of both data and knowledge to drive innovation, fine-grained sentiment classification, and multimodal fusion, among others. Amid rapid iteration and technology upgrades, affective computing has great application prospects in fields such as brain–computer interfaces, empathic human–computer dialogue, assisted decision-making, and virtual reality.#### SIGN UP FOR _Intelligent Computing_ eTOC ALERTS!Get the latest SPJ table of contents delivered right to you!GET OUR E-ALERTS## IntroductionAccording to basic emotion theory, emotion is the grammar of social living and serves as a crucial means of exchanging information, maintaining relationships, and communicating ideas between individuals. Moreover, it is a fundamental psychological element that ensures basic human survival while shaping social habits and supporting advanced thinking , ]. Given its central role in numerous human intellectual activities such as perception, learning, decision-making, reasoning, and socializing, emotion is an important force driving the continuous and diverse prosperity of human civilization.The importance of emotions to human beings can be summarized in 5 crucial aspects. First, the survival function is a learned physiological response that allows individuals to adapt positively to their environment ]. Emotions play a pivotal role in strengthening the capacity to adapt to the environment by regulating attention, memory, perception, and other cognitive processes. This ensures a greater chance of survival and development during the evolutionary process. Second, the communication function highlights the importance of emotions for the accurate expression and understanding of human intentions ]. The same words spoken with different emotions carry different connotations. Thus, emotions are inseparable from natural language and are critical for semantic disambiguation. Third, emotions have a decision-making function that manifests in both fast and slow modes of thinking. The commonly used unconscious “System 1” mainly relies on emotions and experiences, while the conscious “System 2” depends on rational deliberation ]. Therefore, emotions are widely involved in higher-level thinking and decision-making processes that profoundly affect the results and efficiency of decisions. Fourth, emotions serve a motivational function in stimulating and sustaining individuals’ behaviors, thereby affecting the degree of resource input, behavioral persistence, and evaluation of outcomes ]. Finally, emotions perform a maintenance function as bonds between members of ethnic groups, families, social circles, social classes, and other groups. During human socialization, emotions serve as the core of low-cost maintenance of social relations, forming potential social interaction contracts, and are closely tied to individual moral constraints and codes of conduct , ]. Hence, the nature and functions of emotions ensure that they are inseparable from human survival and development.As the era of a human–machine symbiotic society approaches, endowing machines with emotional intelligence becomes increasingly crucial. Emotional intelligence represents a fundamental technology and an essential prerequisite for realizing naturalized and anthropomorphic human–computer interaction. It is of great value for opening up the era of intelligence and digitization. Picard is credited with being the first to propose a comprehensive definition of affective computing. In her 1997 book, _Affective computing_, she defined it as “computing that relates to, arises from, or deliberately influences emotions” ]. The goal of affective computing is to create a computing system capable of perceiving, recognizing, and understanding human emotions and responding intelligently, sensitively, and naturally, thus making human–computer interaction more natural. The epochal importance of affective computing lies in its impact on changing how emotions are perceived as abstractions within psychology, making it possible for emotions to be measured, computed, and machine-learned.Affective computing encompasses various disciplines, including computer science, engineering science, brain and psychological science, and social sciences. Computer science and engineering science focus on providing various information technology tools and engineering capabilities to enable digital reconstruction and computational realization of emotion perception, recognition, understanding, and feedback, allowing machines to possess human-like emotional and cognitive functions. The psychological and consciousness aspects of the brain and psychological sciences provide theories on the basic definition of human emotions and the structure of related elements, laying the foundation for modeling emotion theories. Cognitive neuroscience, another branch of the brain and psychological sciences, examines the emotion-processing mechanism of the human brain and establishes a functional network of psychological elements associated with emotions, providing important inspiration and strategic guidance for developing affective computing models. Social and medical sciences offer numerous opportunities for the application of affective computing and serve as a resource for designing application scenarios for such technologies.### Research in affective computingThe research content of affective computing primarily covers 5 aspects. The first aspect is the fundamental theory of emotion, which currently relies on the discrete emotion model and the dimensional emotion model from the field of psychology to define various types of emotions, ranging from basic to compound. The second aspect involves collecting emotional signals, such as text, speech, facial expressions, gestures, and physiological signals, to establish corresponding datasets. The third aspect is sentiment analysis, which utilizes machine-learning and deep-learning algorithms to model and identify emotional signals. The fourth aspect is multimodal fusion, which leverages multimodal emotional features and fusion algorithms to enhance the accuracy of emotional classification. Finally, the fifth aspect is generating and expressing emotions, processes that enable robots to express emotional states through facial expressions, voice intonation, body movements, etc., and facilitates natural, anthropomorphic, and personified human–robot interaction. Figure  illustrates the specific content and development status of these 5 aspects.![](\\\\xa01. Research content of affective computing.![](\\\\xa02. Annual scientific production on “affective computing” from 1997 January 1 to 2023 September 25.#### Basic theory of emotionThe field of affective psychology has numerous grounded theories of emotion and serves as an important source of inspiration for the development of computable emotion models. The discrete emotion model and the dimensional emotion model are the most commonly used theoretical models for artificial intelligence emotion modeling. The discrete emotion model categorizes emotions individually rather than in correlated groups, as does Ekman’s basic emotion classification model, which is based on facial expression analysis ] and comprises happiness, sadness, anger, disgust, surprise, fear, and contempt. Although the discrete emotion model is clearly defined, interpretable, easy to understand, and capable of semantically integrating vocabulary and concepts, it lacks granularity and provides a limited quantitative description of emotions. In contrast, dimensional affective models represent different emotions through multidimensional vectors in affective space. Such models include the valence–arousal affective model ] and the 3-dimensional pleasure–arousal–dominance model , ]. These models are highly quantitative, abstract, and inductive and have continuous emotional value vectors. They are suitable for handling changes in emotional states over time but are not intuitively interpretable; thus, it is difficult for machines to use them to develop rich coping strategies for emotional interactions. The selection of the model depends on the actual application tasks and scene requirements, as both discrete and dimensional emotion models have advantages and disadvantages.#### Collection of emotional signalsTo support data acquisition and the comparison of algorithms in affective computing, numerous open-source databases have been established. They contain datasets that can be categorized as textual, speech/audio, visual, physiological, or multimodal. The characteristics of these databases considerably impact model design and network architecture in affective computing.Text-based resources on various communication carriers serve as massive datasets for emotional text mining ]. Representative datasets include the internet movie database (IMDb) ], the Stanford sentiment treebank, which contains sentences from movie reviews ], and the Multi-Domain Sentiment Dataset, which contains Amazon.com product reviews ]. Speech is another crucial modality for decoding emotions in human intercommunication. Speech signals comprise both the emotional content of the speech and the emotional characteristics of the sound itself. Representative datasets include EmoDB ], the SEMAINE database ], and CSED ]. Visual-emotional signals such as body movements and facial expressions are now more convenient to gather because of low-cost sensors such as cameras and camcorders, and they do not require direct contact with the user ]. This field has vast amounts of data and many related research papers with considerable data collected directly from real-world scenarios, making it more conducive to grounded applications ]. Representative datasets include the Expression-in-the-Wild (ExpW) dataset ], AffectNet ], the Real-world Affective Faces Database (RAF-DB) ], and SMIC, a database of spontaneous microexpressions ].Physiological data have an advantage over signal data such as text, speech, and facial expressions in that they can more directly, objectively, and accurately reflect an individual’s emotional state while being less influenced by subjective consciousness , ]. Consequently, physiological data have become a research hotspot in affective computing. Commonly used physiological data in this field include electroencephalograms (EEGs), skin electricity, cardiac electricity, electromyography (EMG), eye electricity, respiration, skin temperature, and blood volume pulse. However, obtaining physiological data requires the use of complex sensors. Thus, such data are expensive and challenging to collect for use in practical applications. Consequently, the scale of physiological data used in laboratory research is generally small ]. Representative datasets include the Database for Emotion Analysis using Physiological Signals (DEAP) ], the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) ], and WESAD, a dataset for wearable stress and affect detection ].#### Sentiment analysis**Text analysis.** This method focuses on extracting, analyzing, understanding, and generating emotional information in natural language. Early text affective recognition relied mainly on manually constructed affective dictionaries and rules for affective analysis. These methods judge sentiment polarity by matching sentiment words with grammatical rules in a text , ]. However, this approach is limited by emotional lexicon coverage and rules, making it challenging to support multidomain sentiment analysis. With the advancement of machine learning, text emotion recognition methods based on statistical and machine learning algorithms have emerged. By training on large-scale text datasets, machine learning models can automatically learn emotional expression and semantic features, enhancing the accuracy and generalization ability of sentiment classification , ]. In recent years, deep-learning technology has considerably impacted text emotion recognition. Neural network-based models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, bidirectional encoder representation from transformers (BERT), and generative pre-trained transformers (GPT), have been successful in various sentiment analysis tasks – ]. They can capture contextual information and semantic relationships to better understand and analyze sentiments.**Speech analysis.** Speech emotion recognition is the process by which a computer automatically recognizes the emotional state signaled by speech. Speech contains emotional information, such as speech rate and intonation, in addition to semantic information. Speech emotion analysis combines linguistic and acoustics-related technologies to analyze the syntax, semantics, and acoustic feature information related to the speaker’s emotional state ]. This analysis mainly revolves around rhyme, spectrum, and sound quality features. The numerous acoustic features related to affective states include fundamental frequency, duration, speech rate, resonance peaks, pitch, mel-filter bank (MFB), log-frequency power coefficients (LFPC), linear predictive cepstral coefficients (LPCC), and mel-frequency cepstral coefficients (MFCC) – ]. These features are represented as fixed dimensional feature vectors, with each component representing the statistical value of each acoustic parameter, including the mean, variance, maximum or minimum value, and range of variation. Recently, the ability of neural networks to extract suitable feature parameters has received increasing attention. Deep speech emotion features are learned from speech signals or spectrograms through tasks related to speech emotion recognition. Deep speech features learned from large-scale training data are widely used as speech emotion features in speech event detection and speech emotion recognition tasks, as in the VGGish and wav2vec projects , ], for example. In recent years, algorithms such as ConvNet learning ], ConvNet-RNN ], and adversarial learning ] have considerably improved speech emotion recognition performance.**Visual analysis.** Visual emotion recognition research primarily focuses on facial expression recognition (FER) and emotional body gesture recognition. The conventional method involves feature extraction followed by classification. Typically, handcrafted features for static image analysis include local binary pattern (LBP), histogram of oriented gradients (HOG), local phase quantization (LPQ), and Gabor features , ]. Some scholars have proposed dynamic feature extraction methods, such as LBP on three orthogonal planes (LBP-TOP) ]. Features are usually classified using pattern recognition classification methods such as K-nearest neighbors, support vector machines (SVMs), or multi-layer perceptrons (MLPs). Another approach is the feature learning approach, which combines the end-to-end training of feature representations and classifiers on a given task target, typically a combination of the entire connection layer and softmax. The feature-learning method employs features learned from big data through layer-by-layer feature transformation and can describe the intrinsic information of data better than handcrafted features. However, supervised training methods such as deep CNNs are not universal and rely on large amounts of sample data. Therefore, it is too early to abandon traditional feature-extraction methods. In visual emotion analysis, automatic training features can be extracted and integrated with traditional features, which may further improve system performance.**Physiological signal analysis.** Physiological changes that occur with emotions, including brain electrical activity, heart rate changes, electrical skin response, muscle tension, and respiration rate, are supported by mainstream theories, such as the physiological theory of emotion ] and Lange’s theory of emotion ]. By detecting changes in these physiological signals, patterns associated with emotions can be recognized and then used to develop computer systems that can automatically recognize emotions. Physiological signals are more challenging to recognize than text, speech, and facial expression signals mentioned above, and they have unique properties. For example, computing EEG data requires more complex preprocessing, including electrode position localization, bandpass filtering, reference conversion, segment analysis interception, artifact removal, and bad electrode interpolation. Researchers must have cross-field knowledge to apply machine learning or deep learning methods to recognize emotions from physiological signals ].Affective computing mainly employs peripheral nervous system (PNS) features, such as facial EMG, galvanic skin potential (GSP), photoplethysmography (PPG), heart rate variability (HRV), respiratory rate, and electrocardiogram (ECG), whereas central nervous system (CNS) features include EEG, near-infrared, and brain-imaging features. EEG features have dominated the studies published on this topic. For instance, manual feature extraction involves multidimensional feature extraction from EEG signals in the time, frequency, time–frequency, and nonlinear domains for emotion recognition and classification. Recent studies have emphasized the integrity and relevance of these features. To construct functional brain networks, many studies have started defining a channel as a node and quantifying the relationship between individual nodes using phase synchronization, inter-correlation, and mutual information, treating strength as the functional connectivity between the brain regions of the corresponding channel. Complex network measures, including efficiency, clustering coefficients, degree distribution, small-world features, and average shortest distance, are then used to extract functional brain network features. Since 2018, deep learning methods such as CNNs, RNNs, deep belief networks (DBNs), and stacked autoencoders (SAEs) – ] are being increasingly used for emotional computation of EEG data, generalizing sentiment analysis to various physiological signals.#### Multimodal fusionEarly affective computing primarily involved unimodal data analysis and emotion recognition, focusing on a single modality, such as text, speech, facial expression, body movement, or physiological signals. However, this approach fails to conform to the human perception and expression patterns of emotions and has limitations in terms of the information obtained for emotion recognition ]. Humans communicate their emotions through multiple channels, including language, tone of voice, facial expressions, and body movements. Textual, auditory, and visual information together provide more comprehensive emotional information than they do individually, just as the brain relies on multiple sensory input sources to validate events. Moreover, unimodal information is insufficient and can be easily affected by various external factors ]. Emotional signals can be disguised or affected by other signals from a single channel, for example, when facial expressions are obscured or when noise interferes with speech, resulting in a considerable reduction in emotion analysis performance. Multimodal emotion analysis considers the complementarity of emotion expression among modalities and is thus more robust and aligned with natural human behavior expression. Therefore, research on multimodal fusion of affective computation has received increasing attention. Multimodal fusion algorithms integrate information from different modalities into a stable multimodal representation, enabling comprehensive processing and coordinated optimization to identify human emotions as accurately as possible ]. Common multimodal fusion methods can be categorized into feature-, model-, and decision-layer-based fusion depending on the fusion stage ].#### Generation and expression of emotionsAffective computing enables machines to provide empathic feedback based on deep contextual understanding. Robots and other agents can deliver expressions and responses, conveying the emotional temperature to the user through facial expressions, emotional text responses, and body movements , ] by building on the results of sentiment analysis and recognition. Emotional text generation and speech synthesis are the most-studied areas of research. Emotional text generation involves the automatic generation of emotional response content that matches the message of the dialogue and is consistent with the machine’s strategy, which is chosen according to the context ]. For instance, a traffic enforcement robot may exhibit a fundamental difference in the language used for persuasion and the language used for enforcement, a difference that is crucial to obtaining effective practical traffic management results. The goal of emotional text generation is for the model to generate text that conforms to a specified sentiment category, as expressed by emotion-related keywords or techniques such as metaphors ]. Pretrained models such as GPTs are increasingly being utilized as a base for emotionally controllable text generation and achieving powerful results ]. Responding to text content with emotional color is only the first step. The generated text needs to be expressed using a related emotional voice. Emotional coding information is integrated into the speech synthesis model to make human–machine dialogue less cold and mechanical, thereby allowing individuals to perceive “machine empathy” and feel warmth and affinity. Emotional speech synthesis uses a specific voice style and combines text content with emotional tags to provide a robot or agent with a voice that expresses a particular emotion ]. This process inputs textual content and a specific voice style into a neural network that synthesizes an output voice in that style by utilizing the spectral, rhythmic, and linguistic features of human voices that express emotion.### Applications of affective computingAffective computing is a technology that advances according to the actual needs of the industry, which drives progress and iteration. To build up reliability, general applications initially focused on recreation, leisure, or serving people with urgent needs, then gradually expanded to more fields, transforming the technology and contributing to productive endeavors. In 2021, the value of affective computing reached $21.6 billion, and it is expected to double by 2024 , ]. As the industry grows, the creative applications of affective computing technologies will flourish, yielding satisfactory results in various fields.#### EducationIn the field of education, affective computing is primarily used to recognize the emotional state of learners and provide corresponding feedback and adjustment ]. For example, teachers can utilize intelligent emotional teaching systems to better understand students’ engagement levels and adjust the pace and content of their teaching to improve the learning experience. An intelligent system can recommend customized learning content based on the sentiment analysis of students’ interests. Students can provide authentic teaching feedback through intelligent systems to improve the comprehensiveness and accuracy of teaching evaluations. One advantage of an intelligent system is that it can be used in both traditional and online classrooms to strengthen the contextualization of online teaching, enhance emotional interaction between teachers and students, and improve teaching quality. Affective computing techniques are also conducive to the research and development of educational games and robots ], providing improved human–computer interaction and achieving educational objectives more effectively.#### HealthcareAffective computing research has expanded into various psychiatric disorders in the affective disorders category, such as Alzheimer’s ], Parkinson’s ], bipolar disorder ], and post-traumatic stress ], and into healthcare areas including relaxation service healthcare ] and health office systems ]. Affective computing enables the scientific and objective identification and judgment of patients’ emotions, particularly in psychological disorder treatments, providing a useful complement to more subjective traditional diagnostic tools such as behavioral observation and scale filling. Objective data collection can improve personalized and precise medical treatment ]. In addition, affective computing can be used for the initial screening and efficacy assessment of diseases. For instance, patients with social anxiety disorder exhibit important differences in emotional facial processing compared to the normal population, differences that can be identified by automated monitoring of differential features ].#### Business servicesIn marketing, where the consumer experience is highly correlated with emotions, affective computing is widely used to understand and recognize the user’s emotional state. The application of affective computing can reveal the user’s true preferences and improve and streamline the buying process ]. In the field of financial credit, affective computing technologies can be used to analyze the emotional state and moral level of a customer based on voice and tone, determine the probability of the customer lying, and provide a guide for lending decisions. In the field of stock investment, investor decisions are influenced by irrational judgments. The price trend of a stock is determined not only by a company’s fundamentals but also to a large extent by fluctuations in investor emotions. The study of investor sentiment from social media data (e.g., data from X, formerly known as Twitter) can help identify investors’ emotional preferences and cognitive biases for the purpose of predicting the direction of the stock market ].#### Integration of science and artIn the current digital era, image, audio, and video data have become plentiful and important. Extracting useful information from them and retrieving and mining them effectively are crucial. For example, in recommending music to users, resource management and audio search efficiency are essential. Traditional music search methods match content using text (e.g., song title, artist name, or lyrics). Including sentiment, a high-level semantic feature of music, improves the match between user preferences and music, thus aiding in the primary task in music sentiment analysis ]. Affective computing also empowers automated poetry generation, where deep learning methods such as RNNPG, an RNN-based poem generator, and SeqGAN, a sequence generative adversarial network, are gradually replacing Word Salada, genetic algorithms, and statistical machine translation methods – ]. Expressing emotions more richly is key in making generated poetry spiritual, i.e., in moving beyond resemblance of form to resemblance in spirit.### Importance of this studyThe field of affective computing has grown considerably and exploded in popularity in the last decade for 2 reasons: technological developments providing tools for affective computing and the growth and expansion of demand. In the era of human–machine symbiosis, the deepened human understanding of emotional connotation and the improvement of the “double quotient” (i.e., IQ + EQ) of intelligent machines will become a vital innovative force promoting the affective computing discipline, technological evolution, and industrial progress. Despite the rapid development in affective computing, a comprehensive review of research and systematic analysis of hotspots and trends is lacking. Continuous innovation in algorithmic technology, broadening application requirements, and increasing research efforts necessitate that existing research be summarized and future technological directions be identified. Doing so will enable academia and industry to better understand the development of affective computing technology, thus will facilitate affective computing research, empower applications, and benefit society.This study aims to fill the gaps in existing research through a comprehensive review of affective computing from 1997, when Picard formally proposed the concept, up to 2023. We adopted a bibliometric analysis method to accurately portray the current status of the development of the field and provide insights into present challenges and future trends. The main contributions of this study are as follows. (a) Facing the academic frontier, we list the research hotspots and trends that we identified by analyzing full-scale papers. This allows readers to quickly and comprehensively grasp the development dynamics of the field and understand key common and frontier-leading technologies. (b) Facing major needs and the main battlefield of the economy, we provide blueprints for technological development and insights into current applications. This promotes the application and transformation of affective computing, facilitating high-quality economic development and digital transformation. (c) Facing future trends, we introduce challenges and developments in the field of affective computing, along with predictions for future technology and industry application directions. This serves as a forward-looking guide to the field.## Materials and Methods### Data collectionThis study searched for papers published in affective computing from January 1997 to September 2023 in the Web of Science Core Collection (WoSCC), which includes the Science Citation Index Expanded, Social Sciences Citation Index, Arts & Humanities Citation Index, Emerging Sources Citation Index, Conference Proceedings Citation Index—Science (CPCI-S), and Conference Proceedings Citation Index—Social Sciences & Humanities (CPCI-SSH). The search strategy is summarized in Table .| Index field | Search strategy || --- | --- || Theme keywords | “affective recognition” or “mood recognition” or “affective computing” or “artificial emotional intelligence” or “emotion AI” or “expression recognition” or “emotion recognition” or “emotion learning” or “sentiment analysis” or “sentiment recognition” || Literature types | proceedings papers, articles, review articles, early access |Table\\\\xa01. Search strategy for this studyThe reason this study uses 1997 as the starting point of the timeline is that the book _Affective computing_ ], which was published in that year, is regarded as the work that established affective computing as an independent academic research field. Papers outside this time range were not included in the calculation of citation statistics. In the statistics of Chinese papers, Hong Kong, Macau, and Taiwan are included. The results show that 33,448 papers were published worldwide. Among them, 16,097 (48.13%) were conference papers and 17,351 (51.87%) were journal papers. It should be noted that the names of institutions were standardized using machine and manual methods. However, when scientists publish papers, the writing of the names of institutions is not standardized, which may have caused the omission of papers in the statistics and a deviation in the index calculation results.In addition, this study combined the following 3 databases for data acquisition: (a) Incites: This database is based on the publication date of all document types in the major index databases of the WoSCC. It performs publication count and index calculations to provide research performance analysis. (b) Essential Science Indicators (ESI): This is an in-depth analytical research tool based on the Web of Science. ESI can identify influential countries, institutions, papers, and publications, as well as the cutting-edge in a research field. (c) Journal Citation Reports (JCRs): This is a multidisciplinary journal evaluation tool that provides journal evaluation resources based on citation data statistics. By citing and counting references, the JCR can measure the influence of research at the journal level, revealing the relationships between citing and cited journals.### Data analysisStatistical analysis was performed using a bibliometric method. Bibliometrics applies quantitative methods such as mathematics and statistics to the literature of a scientific or other field and processes statistical data based on information science theory. This widely accepted approach provides quantitative analysis pathways and innovative insights into the assessment of research trends based on previous literature , ]. Unlike peer review and expert judgment, bibliometrics can provide quantitative indicators to ensure objectivity through statistical analysis of academic achievements ]. Bibliometric analysis enables monitoring and summarizes the status, hotspots, and trends of a particular topic, helping researchers identify future research directions ]. In this study, we first cleaned and analyzed the data using the Derwent Data Analyzer (DDA, version 10, Clarivate, London, UK), which is well integrated with the source data from the Web of Science platform. DDA was used for multidimensional data mining, preprocessing, standardization, and statistical analysis. Subsequently, the bibliometric analysis and knowledge visualization software tool VOSviewer (version 1.6.15, Leiden University, Leiden, Netherlands) was employed. This analysis tool provides valuable insights into the structure, advancement, and collaboration in the field of affective computing. Notably, its distinctive feature lies in the graphical representation of bibliometric maps, which is particularly suitable for large-scale data analysis ]. VOSviewer was used to visualize the data in this study.## Results### Publication trendsFrom 1997 to 2009, the number of articles published in this field steadily increased, exhibiting an overall growth trend despite occasional fluctuations (Fig.\\\\xa02). From 2010 to 2019, with the rise of deep learning, a rapid development was observed in the field of affective computing, and the number of articles published in the field rose rapidly, indicating an explosive growth stage of research. After 2019, because of a plateau in the innovation of deep learning methods and the impact of the coronavirus disease 2019 (COVID-19) pandemic on academia, research in the field of affective computing also reached a plateau, and the rising trend slowed down.### Comparison of countriesTo analyze the main research positions in the field of affective computing, the country/region fields of all the authors and the first author of the paper were counted. As shown in Table , among the top 20 countries with publications in the field of affective computing, China is the country with the largest number of publications, accounting for 26.2% of all authors and 24.6% of first authors. China, the United States, India, the United Kingdom, and Germany rank among the top 5 in the number of papers published counting all authors or first author, and are the most important in terms of research in the field of affective computing. The United States ranks second in the number of papers published counting all authors, but third in the number of papers published counting only first author, after India.| No. | Country | Number of papers | Country | Number of papers || --- | --- | --- | --- | --- || (All authors) | (All authors) | (First author) | (First author) || --- | --- | --- | --- || 1 | China | 8,780 | China | 8,223 || 2 | USA | 4,715 | India | 3,632 || 3 | India | 3,829 | USA | 3,274 || 4 | UK | 2,535 | UK | 1,432 || 5 | Germany | 1,706 | Germany | 1,253 || 6 | Japan | 1,321 | Italy | 1,022 || 7 | Italy | 1,302 | Japan | 977 || 8 | Australia | 1,234 | South Korea | 931 || 9 | Spain | 1,178 | Spain | 862 || 10 | South Korea | 1,121 | Australia | 788 || 11 | Canada | 1,100 | Canada | 720 || 12 | France | 943 | France | 587 || 13 | Netherlands | 778 | Turkey | 581 || 14 | Saudi Arabia | 765 | Netherlands | 484 || 15 | Turkey | 691 | Malaysia | 479 || 16 | Singapore | 640 | Pakistan | 454 || 17 | Malaysia | 609 | Brazil | 443 || 18 | Pakistan | 595 | Greece | 413 || 19 | Brazil | 522 | Iran | 398 || 20 | Greece | 483 | Singapore | 394 |Table\\\\xa02. The top 20 countries in the field of affective computingIn addition to the 2-year step in 2021–2022, a 4-year step was used to count the publication volume of the top 10 countries in the field of affective computing. The results are shown in Fig. . Given that the concept of “affective computing” originated in the United States, which has been a major research force in this field, we chose the United States as the benchmark. During the entire period, the relative volume of publications by China and the United States changed considerably, as shown in Fig. . From 1997 to 2004, the number of papers published by the United States far exceeded that of China. From 1997 to 2000, the total number of papers published by China was 20% of that of the United States. From 2001 to 2004, the total number of papers published by China rose to 31% of that of the United States. In the period from 2005 to 2008, the number of papers published by China surpassed that of the United States, and the number of papers published by China in 2021–2022 is about 3 times that of the United States. It can be seen that in recent years, China’s research in the field of affective computing has accumulated rapidly, and its large volume of research has certain advantages compared with that of the United States. In addition, in 2021–2022, the number of papers published by India surpassed that of the United States for the first time. India has gradually become a major research center in the field of affective computing because of its advantages in computer science, engineering, and other disciplines.![](\\\\xa03. Comparison between the top 10 countries and the United States in the number of publications.### Main journalsThis section analyzes basic data on journal papers. The 17,351 published papers were distributed in 1,300 journals, among which _IEEE Access_ [impact factor (IF) 3.9] had the most (875), as shown in Table . Across all journals, 1,209 had an IF listed in the 2022 JCRs. The distribution of the IFs of the 1,209 journals is shown in Table . Among them, 54 journals have IFs greater than 10, and the 5 journals with the highest IFs are _World Psychiatry_ (73.3), _Lancet Psychiatry_ (64.3), _Nature Reviews Neuroscience_ (34.7), _Nature Human Behaviour_ (29.2), and _JAMA Psychiatry_ (25.8). The IFs of most journals are distributed in the 2 intervals of 2 ≤ IF < 4 and 4 ≤ IF < 7. It is worth noting that _IEEE Transactions on Affective Computing_ (IF 11.2) is a high-level journal focusing on the field of affective computing. It is a cross-disciplinary and international archive journal aimed at disseminating the results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. In addition, _Expert Systems with Applications_, _Knowledge-Based Systems_, _Information Processing & Management_, _IEEE Transactions on Multimedia_, _Neurocomputing_, _Information Sciences_, _Pattern Recognition_, _Applied Soft Computing_, _Decision Support Systems_, and _Future Generation Computer Systems_ are also high-level journals favored by scholars in the field of affective computing.| No. | Journal | Number of papers || --- | --- | --- || 1 | _IEEE Access_ | 875 || 2 | _Multimedia Tools and Applications_ | 474 || 3 | _IEEE Transactions on Affective Computing_ | 419 || 4 | _Sensors_ | 378 || 5 | _Frontiers in Psychology_ | 362 || 6 | _Applied Sciences-Basel_ | 349 || 7 | _Expert Systems with Applications_ | 290 || 8 | _International Journal of Advanced Computer Science and Applications_ | 272 || 9 | _Neurocomputing_ | 248 || 10 | _Knowledge-Based Systems_ | 226 || 11 | _Psychiatry Research_ | 191 || 12 | _Electronics_ | 167 || 13 | _Journal of Intelligent & Fuzzy Systems_ | 151 || 14 | _Neural Computing & Applications_ | 144 || 15 | _Neuropsychologia_ | 137 || 16 | _Schizophrenia Research_ | 135 || 17 | _Information Processing & Management_ | 132 || 18 | _Computational Intelligence and Neuroscience_ | 114 || 19 | _Cognitive Computation_ | 112 || 20 | _Information Sciences_ | 110 |Table\\\\xa03. Top 20 journals with the largest number of articles in the field of affective computing| Journal impact factor | Number of journals || --- | --- || IF ≥ 10 | 54 || 7 ≤ IF < 10 | 74 || 4 ≤ IF < 7 | 255 || 2 ≤ IF < 4 | 406 || 1 ≤ IF < 2 | 245 || IF ≤ 1 | 175 |Table\\\\xa04. Journal impact factor distribution### High-level international conferencesCombining ESI’s highly cited and hot papers with the “China Computer Federation Recommended International Academic Conferences” and CORE Computer Science Conference Rankings, we identified the high-level international conferences related to affective computing. These include the ACM International Conference on Multimedia (ACM MM), AAAI Conference on Artificial Intelligence (AAAI), Annual Meeting of the Association for Computational Linguistics (ACL), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE International Conference on Computer Vision (ICCV), International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), and the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).### Discipline distributionThis section analyzes the distribution of research fields based on statistics on the Web of Science categories of papers in the field of affective computing. Studies related to the topic of affective computing involve computer science, communication, engineering, psychology, medicine, and other disciplines, reflecting distinct interdisciplinary characteristics. The top 20 categories with the largest number of publications are listed in Table . The category with the largest proportion is “Computer Science, Artificial Intelligence,” with 12,678 publications (37.93% of the total), followed by “Engineering, Electrical & Electronic,” with 9,820 publications (29.36% of the total).| Web of Science category | Number of papers | Percentage (%) || --- | --- | --- || Computer Science, Artificial Intelligence | 12,687 | 37.90 || Engineering, Electrical & Electronic | 9,820 | 29.36 || Computer Science, Information Systems | 8,714 | 26.05 || Computer Science, Theory & Methods | 8,405 | 25.13 || Computer Science, Interdisciplinary Applications | 3,930 | 11.75 || Telecommunications | 3,133 | 9.37 || Computer Science, Software Engineering | 2,982 | 8.92 || Neurosciences | 2,376 | 7.10 || Psychiatry | 2,100 | 6.28 || Computer Science, Cybernetics | 1,904 | 5.69 || Imaging Science & Photographic Technology | 1,077 | 3.22 || Engineering, Multidisciplinary | 1,045 | 3.12 || Automation & Control Systems | 997 | 2.98 || Computer Science, Hardware & Architecture | 981 | 2.93 || Psychology, Multidisciplinary | 884 | 2.64 || Robotics | 793 | 2.37 || Engineering, Biomedical | 735 | 2.20 || Acoustics | 724 | 2.16 || Linguistics | 637 | 1.90 || Clinical Neurology | 610 | 1.82 |Table\\\\xa05. Top 20 categories with the most papers in the field of affective computing### Technology transfer and conversionThis study searched the Derwent Innovation Index, the world’s most comprehensive database of value-added patent information. Among effective invention patents with transfer records and high value, the transferred patents with an IncoPat patent value of 10 (the highest level) include “Cognitive content display device” (US10902058B2, transferred from IBM to Kyndryl Inc.) and “Signal processing approach to sentiment analysis for entities in documents” (US9436674B2, transferred from Attivio Inc. to Servicenow Inc.). However, the number of patent transfer records related to affective computing is small, indicating that technology transfer activity needs to be improved.### Global distribution of scholarsThis section presents statistical analysis of publications based on the country of the first author to provide a macroscopic understanding of the global distribution of scholars in the field of affective computing. As shown in Table , China has the largest number (4,240), followed by India (2,391) and the United States (2,390). In Fig. , darker shading indicates a larger number of scholars. It can be seen that Asia and North America are the regions with the most concentrated distribution of scholars in the field of affective computing.| No. | Country | Number of scholars | No. | Country | Number of scholars || --- | --- | --- | --- | --- | --- || 1 | China | 4,240 | 11 | Canada | 533 || 2 | India | 2,391 | 12 | France | 425 || 3 | USA | 2,390 | 13 | Turkey | 403 || 4 | UK | 999 | 14 | Netherlands | 349 || 5 | Germany | 825 | 15 | Malaysia | 331 || 6 | Italy | 690 | 16 | Pakistan | 324 || 7 | Japan | 631 | 17 | Brazil | 366 || 8 | South Korea | 514 | 18 | Greece | 248 || 9 | Spain | 545 | 19 | Iran | 270 || 10 | Australia | 496 | 20 | Singapore | 229 |Table\\\\xa06. Number of first authors in the field of affective computing (top 20 countries)![](\\\\xa04. Global distribution of scholars in the field of affective computing.### International collaborationThere is a wide range of international cooperation in the field of affective computing. A count of collaborations between the top 20 countries is shown in Table . The number of articles published by China and the United States is the largest (641), followed by China and the United Kingdom (343). Although cooperation between China and the United States has been challenging in recent years, in the field of affective computing, they remain each other’s largest partners, maintaining a vital and continuous cooperation.|  | C1 | U1 | I1 | U2 | G1 | J | I2 | A | S1 | S2 | C2 | F | N | S3 | T | S4 | M | P | B | G2 || --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- || C1 | / | 641 | 79 | 343 | 79 | 256 | 59 | 218 | 43 | 61 | 137 | 44 | 28 | 61 | 12 | 161 | 38 | 57 | 6 | 4 || U1 | 641 | / | 128 | 232 | 174 | 42 | 120 | 122 | 62 | 83 | 153 | 100 | 105 | 55 | 42 | 71 | 10 | 36 | 53 | 23 || I1 | 79 | 128 | / | 73 | 15 | 15 | 22 | 37 | 19 | 33 | 26 | 26 | 8 | 67 | 10 | 48 | 26 | 11 | 3 | 2 || U2 | 343 | 232 | 73 | / | 294 | 40 | 132 | 119 | 96 | 15 | 60 | 89 | 160 | 77 | 25 | 64 | 30 | 43 | 30 | 48 || G1 | 79 | 174 | 15 | 294 | / | 41 | 69 | 47 | 39 | 11 | 43 | 59 | 99 | 4 | 17 | 17 | 5 | 7 | 13 | 19 || J | 256 | 42 | 15 | 40 | 41 | / | 4 | 20 | 17 | 7 | 27 | 15 | 11 | 6 | 5 | 18 | 16 | 1 | 3 | 1 || I2 | 59 | 120 | 22 | 132 | 69 | 4 | / | 22 | 58 | 13 | 25 | 65 | 53 | 10 | 13 | 37 | 4 | 10 | 6 | 7 || A | 218 | 122 | 37 | 119 | 47 | 20 | 22 | / | 26 | 14 | 30 | 22 | 24 | 27 | 14 | 35 | 25 | 23 | 11 | 3 || S1 | 43 | 62 | 19 | 96 | 39 | 17 | 58 | 26 | / | 15 | 15 | 42 | 46 | 23 | 12 | 11 | 5 | 10 | 24 | 18 || S2 | 61 | 83 | 33 | 15 | 11 | 7 | 13 | 14 | 15 | / | 10 | 16 | 6 | 29 |  | 2 | 9 | 71 | 3 | 3 || C2 | 137 | 153 | 26 | 60 | 43 | 27 | 25 | 30 | 15 | 10 | / | 45 | 19 | 40 | 10 | 9 | 2 | 12 | 17 | 3 || F | 44 | 100 | 26 | 89 | 59 | 15 | 65 | 22 | 42 | 16 | 45 | / | 39 | 11 | 4 | 2 | 8 | 16 | 18 | 10 || N | 28 | 105 | 8 | 160 | 99 | 11 | 53 | 24 | 46 | 6 | 19 | 39 | / |  | 19 | 8 | 3 | 1 | 11 | 14 || S3 | 61 | 55 | 67 | 77 | 4 | 6 | 10 | 27 | 23 | 29 | 40 | 11 |  | / | 8 | 4 | 41 | 120 | 1 | 3 || T | 12 | 42 | 10 | 25 | 17 | 5 | 13 | 14 | 12 |  | 10 | 4 | 19 | 8 | / | 5 | 7 | 3 | 2 | 1 || S4 | 161 | 71 | 48 | 64 | 17 | 18 | 37 | 35 | 11 | 2 | 9 | 2 | 8 | 4 | 5 | / | 4 | 2 |  | 1 || M | 38 | 10 | 26 | 30 | 5 | 16 | 4 | 25 | 5 | 9 | 2 | 8 | 3 | 41 | 7 | 4 | / | 40 |  | 3 || P | 57 | 36 | 11 | 43 | 7 | 1 | 10 | 23 | 10 | 71 | 12 | 16 | 1 | 120 | 3 | 2 | 40 | / | 3 |  || B | 6 | 53 | 3 | 30 | 13 | 3 | 6 | 11 | 24 | 3 | 17 | 18 | 11 | 1 | 2 |  |  | 3 | / |  || G2 | 4 | 23 | 2 | 48 | 19 | 1 | 7 | 3 | 18 | 3 | 3 | 10 | 14 | 3 | 1 | 1 | 3 |  |  | / |Table\\\\xa07. Collaborations between the top 20 countries in the field of affective computingNote: C1, China; U1, USA; I1, India; U2, UK; G1, Germany; J, Japan; I2, Italy; A, Australia; S1, Spain; C2, Canada; S2, South Korea; F, France; N, Netherlands; T, Turkey; S3, Saudi Arabia; S4, Singapore; M, Malaysia; P, Pakistan; B, Brazil; G2, Greece.### Important research institutionsThe top 10 institutions in the world by number of publications (counting all authors) are listed in Table . This study used indicators such as Citation Impact, Category Normalized Citation Impact (CNCI), and Highly Cited Papers to further evaluate the influence of various institutions in the field of affective computing. Among them, CNCI is a valuable and unbiased impact indicator that excludes the influence of publication year, subject field, and document type. A CNCI value of 1 indicates that the cited performance of a group of papers is equivalent to the global average level, a value greater than 1 indicates higher performance, and a value less than 1 indicates lower performance; a value of 2 indicates performance twice as high as the global average. The top 5 institutions according to CNCI rankings were Nanyang Technological University (5.06), Imperial College London (3.58), Tsinghua University (3.23), the Chinese Academy of Sciences (3.15), and the University of California System (2.77).| No. | Institution | Number of papers | Citation impact | Category Normalized Citation Impact | H-index | Percentage in Q1 journals | Country || --- | --- | --- | --- | --- | --- | --- | --- || 1 | Chinese Academy of Sciences | 699 | 20.97 | 3.15 | 60 | 59.87 | China || 2 | University of London | 443 | 50.26 | 2.29 | 77 | 69.45 | UK || 3 | UDICE-French Research Universities | 388 | 18.86 | 1.37 | 42 | 50.43 | France || 4 | Centre National de la Recherche Scientifique (CNRS) | 377 | 19.33 | 1.36 | 42 | 51.56 | France || 5 | University of California System | 371 | 40.83 | 2.77 | 64 | 58.72 | USA || 6 | National Institute of Technology (NIT System) | 364 | 9.68 | 1.46 | 29 | 26.43 | India || 7 | Indian Institute of Technology System (IIT System) | 360 | 13.51 | 1.99 | 36 | 44.7 | India || 8 | Nanyang Technological University | 350 | 46.35 | 5.06 | 69 | 68.99 | Singapore || 9 | Tsinghua University | 302 | 24.87 | 3.23 | 44 | 62.93 | China || 10 | Imperial College London | 300 | 41.00 | 3.58 | 49 | 70.75 | UK |Table\\\\xa08. Institutions with a top 10 publication in affective computingNotes: 1. Citation impact: The citation impact of a set of documents is calculated by dividing the total number of citations of the set of documents by the number of documents. Citation impact shows the average number of citations received by a document in the group. 2. Category Normalized Citation Impact (CNCI): The CNCI of a document is obtained by dividing the actual number of citations by the expected number of citations of documents of the same type, publication year, and subject. When a document is classified into multiple subject areas, the average value of the ratio of actual citations to expected citations is used. The CNCI of a country is the average of the CNCIs of the publications of that country.### Citation network analysisThis section analyzes the direct citations of all authors in the field of affective computing. To highlight the key authors, 40 authors who had published no fewer than 30 papers were selected for analysis. The results are shown in Fig. . Authors in clusters of the same color have strong correlations and inheritance in research content. Representative scholars from the 5 clusters are listed in Table .![](\\\\xa05. Citation network of scholars.| Scholar | Organization | Research fields || --- | --- | --- || Baoliang Lu | Shanghai Jiaotong University, China | Brain-like computing, neural networks, deep learning, emotion AI, affective brain–computer interface || Bjoern Schuller | Imperial College London, UK | Machine intelligence, signal processing, affective computing, digital health, speech recognition || Erik Cambria | Nanyang Technological University, Singapore | Affective computing, sentiment analysis, commonsense reasoning, natural language understanding || Fuji Ren | The University of Tokushima, Japan the University of Electronic Science and Technology of China, China | Natural language processing, artificial intelligence, affective computing, and emotional robots || Wenming Zheng | Southeast University, China | Multimodal affective computing, neural computation, pattern recognition, machine learning, and computer vision |Table\\\\xa09. Representative scholars in the citation network### Word frequency analysisWord frequency refers to the number of times a word occurs in the document being analyzed. In scientometric research, word frequency dictionaries can be established for specific subject areas to quantify the analysis of scientists’ creative activities. Word frequency analysis is the method of extracting keywords or subject words that express the core content of the articles in the literature, to study the development trends and research hotspots of the field through the frequency distribution of these words. The results of conducting frequency and co-occurrence analysis on keywords assigned to papers by authors in the field of affective computing are shown in Table .| No. | Number of occurrences | Technical keyword | Number of co-occurrences with other keywords | Time period | Proportion of occurrences within last 3 years (%) || --- | --- | --- | --- | --- | --- || 1 | 7,621 | Sentiment analysis | Machine learning [958]; Opinion mining [936]; Natural language processing [829] | 2006–2023 | 21 || 2 | 4,566 | Emotion recognition | Feature extraction [422]; Affective computing [397]; Deep learning [372] | 1997–2023 | 24 || 3 | 2,457 | Affective computing | Emotion recognition [397]; Machine learning [191]; Emotion [137] | 2000–2023 | 15 || 4 | 2,232 | Deep learning | Sentiment analysis [691]; Emotion recognition [372]; Machine learning [268] | 2012–2023 | 40 || 5 | 2,054 | Machine learning | Sentiment analysis [958]; Natural language processing [275]; Deep learning [268] | 2002–2023 | 27 || 6 | 1,816 | Facial expression recognition | Deep learning [182]; Feature extraction [150]; Face recognition [109] | 1997–2023 | 18 || 7 | 1,348 | Natural language processing | Sentiment analysis [829]; Machine learning [275]; Deep learning [209] | 2006–2023 | 30% of 1,348 || 8 | 1,214 | Feature extraction | Emotion recognition [422]; Sentiment analysis [213]; Task analysis [181] | 2003–2023 | 32 || 9 | 1,209 | Opinion mining | Sentiment analysis [936]; Natural language processing [159]; Machine learning [151] | 2006–2023 | 11 || 10 | 1,067 | Emotion | Affective computing [137]; Emotion recognition ]; Facial expression ] | 1999–2023 | 13 || 11 | 1,007 | Twitter | Sentiment analysis [770]; Machine learning [160]; Social media [145] | 2011–2023 | 18 || 12 | 975 | Speech emotion recognition | Deep learning ]; Feature extraction ]; Emotion recognition ] | 2006–2023 | 29 || 13 | 852 | Social media | Sentiment analysis [587]; Twitter [145]; Machine learning [105] | 2009–2023 | 21 || 14 | 732 | Social cognition | Schizophrenia [193]; Emotion recognition [184]; Theory of mind [179] | 2001–2023 | 16 || 15 | 657 | Text mining | Sentiment analysis [486]; Natural language processing ]; Opinion mining ] | 2006–2023 | 15 || 16 | 635 | EEG | Emotion recognition [357]; Affective computing ]; Emotion ] | 2004–2023 | 27 || 17 | 620 | Classification | Sentiment analysis [208]; Machine learning ]; Emotion recognition ] | 2003–2023 | 19 || 18 | 618 | Facial expression | Emotion recognition [175]; Emotion ]; Affective computing ] | 1998–2023 | 15 || 19 | 582 | Convolutional neural network | Deep learning [146]; Facial expression recognition ]; Emotion recognition ] | 2003–2023 | 30 || 20 | 535 | Schizophrenia | Social cognition [193]; Emotion recognition ]; Theory of mind ] | 1998–2023 | 8 || 21 | 478 | Support vector machine | Sentiment analysis [123]; Facial expression recognition ]; Emotion recognition ] | 2002–2023 | 9 || 22 | 470 | Feature selection | Sentiment analysis [119]; Emotion recognition ]; Feature extraction ] | 2001–2023 | 16 || 23 | 423 | Face recognition | Feature extraction [155]; Emotion recognition [124]; Facial expression recognition [109] | 1997–2023 | 29 || 24 | 422 | Transfer learning | Emotion recognition ]; Deep learning ]; Sentiment analysis ] | 2009–2023 | 40 || 25 | 404 | Data mining | Sentiment analysis [251]; Feature extraction ]; Machine learning ] | 2006–2023 | 22 |Table\\\\xa010. Frequency analysis of top 25 keywords in affective computingThe Thomson Data Analyzer was used to automatically and manually clean the keywords assigned by the authors of papers in the dataset. Subsequently, VOSviewer was used to cluster the core (high-frequency) subject words and set a certain co-occurrence frequency and co-occurrence intensity according to the size of the dataset to cluster the keywords. Combined with expert interpretation, each cluster was named and interpreted, and the topics of the journal articles were identified and analyzed. After keyword cleaning, 613 keywords appearing more than 20 times were selected as analysis objects for cluster calculation. Five clusters were obtained by clustering the core subject words with the highest co-occurrence intensity, as shown in Table  and Fig. .| No. | Research theme | Number of core subject words | Average number of citations | Average correlation strength || --- | --- | --- | --- | --- || 1 | Natural language processing techniques used for affective computing and opinion mining | 153 | 10.41 | 197.80 || 2 | Facial expression and micro-expression recognition and analysis | 134 | 15.89 | 178.77 || 3 | Affective computing studies in human–computer interaction | 121 | 18.69 | 110.38 || 4 | Applied research of affective computing in affective disorder analysis | 30 | 33.5 | 165.59 || 5 | Multimodal sentiment analysis based on deep learning | 81 | 9.8 | 260.95 |Table\\\\xa011. Five research themes in affective computing![](\\\\xa06. Five research themes in affective computing.The average number of citations of a research theme is the average number of times that a paper containing these subject words has been cited since publication, and the average correlation strength of a research theme indicates the closeness of the connection between the core subject words contained in this theme concept. The greater the correlation strength, the greater the co-occurrence intensity between the core subject words and the more concentrated the research. In contrast, relatively lower correlation is associated with more scattered research. Research on the application of affective computing in the analysis of affective disorders has the highest average citation frequency, which shows that interdisciplinary research involving affective computing and medicine, especially research on affective disorders and depression recognition, has a greater influence. The average correlation strength of multimodal sentiment analysis based on deep learning is the largest, which shows that the research on this topic is the most concentrated.## DiscussionThis paper presents a comprehensive analysis and review of systematically collected data on papers and major intellectual property rights in the field of affective computing. The results reveal that over the past 25 years, affective computing has experienced rapid growth in the number of published papers, representing a vibrant academic ecology and an interdisciplinary character with a wide range of disciplines. Additionally, scholars worldwide actively participate in a relatively close cooperation network. In particular, Chinese scholars have led the world in terms of the number of publications, scholars, and collaborative papers in this field. Among important research institutions, Tsinghua University and the Chinese Academy of Sciences stand out, with CNCI values indicating that the average number of citations of their papers was more than twice the global average. Citation network analysis showed that Chinese scholars are representative and have become essential nodes in the citation network, indicating that China is constructing a large-scale talent team for affective computing and progressing in both the quantity and quality of research. However, China also faces disadvantages in academic journals, international conferences, and other aspects, leading to weak dominance, which restricts China’s academic discourse improvement in this field. Notably, in recent years, India’s publication volume has exceeded that of the United States for the first time, revealing a robust development potential linked to its advantages in computing. Nonetheless, India still has room for growth in terms of research quality and paper impact as it lacks representative scholars in the field of affective computing.### Challenges and technology development trends#### Modeling of cultural contextsThis study found that affective computing researchers are distributed across various countries globally and have a wide range of cultural backgrounds. While emotional expression has a degree of consistency across humanity, it is considerably influenced by cultural background. Cultural norms and values determine the different emotional experiences of individuals and how others perceive these emotions. Therefore, affective computing systems developed using a single cultural group may fail in other cultural contexts. For example, Chinese, Germans, and Japanese express emotions relatively implicitly, whereas Americans, British, and Brazilians express emotions more overtly. This indicates that emotion agents must match emotion calculation rules with the cultural context. Many Western cultural standards may not necessarily apply in Eastern contexts. For example, Japanese researchers tend to develop robots that can express emotions implicitly because overly direct expressions of emotions may cause user dissatisfaction ]. Therefore, cultural characteristics must be considered in developing universal cross-cultural emotional agents for people from different cultural backgrounds. Hofstede defined culture in terms of 5 measures—power distance, identity, gender, uncertainty avoidance, and long-term orientation—which can be used to summarize the typical rules of emotional expression in different cultural contexts ]. When it is challenging to obtain culture-specific empirical affective data, it is more feasible to design affective computational models using cultural theories and rules.#### Emotion generation techniquesThe cluster analysis of topic terms in affective computing revealed 5 important core topics, including “natural language processing techniques for affective computing and opinion mining” and “facial expression and micro-expression recognition and analysis.” Current research focuses more on emotion recognition, with relatively limited attention accorded to emotion generation. Emotion recognition and generation are both essential aspects of affective computing and constitute an important technical basis for the closed loop of human–computer interaction. To enable machines to provide more anthropomorphic and natural feedback, it is crucial to focus on the following 2 research areas. (a) Generation of facial expressions. The fact that human emotions are expressed through visual (55%), voice (38%), and verbal (7%) signals is also known as the “3V rule,” which reflects the importance of human facial expressions in emotion analysis ]. Appropriate use of facial expressions by avatars and robots can enhance human–robot interaction. Thus, current research aims to build a lexicon of facial expressions that can translate communicative intent into associated expressive morphology and dynamic features to express various meanings. Meanwhile, a team of animation experts is required to achieve realistic facial rendering effects, including lighting and muscle textures. (b) Generation of emotional body movement. This requires the design of embodied agents using computer models of body expression. This area involves studying human kinematics; however, researchers have yet to determine how to characterize the organic combination of body parts, movement strength, and posture of specific emotional states.#### Fine-grained sentiment classification modelsEkman’s basic emotion theory model is a widely used classification model for emotion computation ]. However, in real life, people’s emotions often exist in a mixed state. For example, people often simultaneously express surprise and joy, sadness and pain, etc. Du et\\\\xa0al. ] proposed the concept of mixed emotions based on research conducted using the Facial Action Coding System (FACS). They suggested that the combination of 2 basic emotions creates mixed emotions and defined different types using scenario examples. Using a FACS-based face recognition algorithm model, microvariations in facial muscles can be analyzed to accurately discriminate between different types of mixed emotions. Martinez ] assessed whether mixed emotions can be semantically labeled correctly. The test tasks included prioritization and forced selection of mixed emotion labels, and the results showed that subjects performed consistent and accurate categorization. Mixed emotion is an essential research direction for expression-based fine-grained emotion classification. This concept extends the core idea of FACS, aiming to reveal the relationship between mixed and basic emotions. It offers a better solution to the problem of differentiation of emotions and clarifies the relationship between differentiated emotions and their original emotions, providing traceable clues and measurement possibilities for the generation, development, and change of emotions. It summarizes complex emotional changes into a logical dynamic composite form with similar configuration effects, resulting in strong interpretability, logic, and unity.#### Code of ethics and technical standardsRecording an individual’s emotional state has implications for privacy, particularly when it comes to recording video or audio. Subjects may not agree to provide researchers with authentic and naturalistic emotional data and may feel uncomfortable being monitored in daily life. For example, the results of AI emotion monitoring tools may be analyzed alongside employee performance evaluations, predictions of the risk of leaving the job, and patterns of employee–team interactions for predicting behavior. Although the use of such technology reduces employee turnover and saves costs for organizations ], employees may experience constant psychological stress, leading to burnout ]. Additionally, individuals may lose autonomy as they become more hesitant to display emotions in public, instead choosing to use a “poker face.” While there should be openness in the use of affective computing, appropriate regulation is necessary to assess potential risks involving privacy and security, and the technology should be reviewed and documented for each industry to maximize benefits while minimizing harm, risks, and costs. Ethical issues are more likely to be overlooked in computing and engineering than in psychology. The collection of individual data, particularly physiological data, should be regulated by human research ethics committees, which are best suited to managing informed consent and privacy issues.Efforts should be made to strengthen the development of international standards in the field of affective computing to form a universally accepted specification. Currently, the available standard is “Information technology—Affective computing user interface (AUI)” (standard number ISO/IEC 30150-1:2022). The first part, “Model,” was released in June 2022, and the second part, “Affective Characteristics,” is under construction. However, there is a lack of standards for data collection, data security, and personal privacy protection in the field of affective computing. Therefore, the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union (ITU) should improve relevant standards and unify them for global use.#### Cognitive neuroscience-inspired affective computingJust as CNN architectures are inspired by biological visual processing and reinforcement learning methods are inspired by behaviorist theories in psychology, impulse network models are inspired by neuroplasticity. Cognitive neuroscience has also developed theories on affective circuits ], multiple-wave models ], embodied cognition ], and other related areas, providing brain-inspired insights into the design of affective computation models. Studies on the physiological representations of different emotions offer theoretical foundations and guidelines for feature extraction in affective computing based on facial expressions, psychophysiological measurements, and neuroimaging. Further human research in the field of cognitive neuroscience will ultimately affect the development of affective computing and artificial intelligence as a whole. The cognitive process of human brain emotion processing, its neural mechanism, and its anatomical basis provide essential inspiration for the development of affective computing models. However, to ensure that machines have genuine emotions rather than just appearing to have emotions, further research in cognitive neuroscience is required. This research may involve exploring the neural basis for the generation of human consciousness, the neural mechanism for the construction of human values, and other key scientific issues. Based on this neural theoretical foundation, simulation and machine implementation are feasible options for providing machines with authentic emotions.#### Construction of large-scale multimodal datasetsThe development of affective computing is highly dependent on the construction of large-scale open datasets. Three major trends are described below. The first trend predicts that dataset sizes will continue to grow to meet the demands of deep learning algorithm training. Deep-learning models have a substantial number of parameters, and the selection of these parameters requires samples that are typically 100 times the number of parameters. A larger dataset size enables the trained model to avoid overfitting, which improves model learning. However, the challenge lies in labeling these massive datasets. Thus, it is necessary to explore active, weakly supervised, and unsupervised learning methods to label the meaningful data in large unlabeled datasets or train machines for labeling. The second trend highlights the need for the collection of multimodal data, the accumulation of richer modal information, and fine-grained alignment between different modalities. At this stage, machines differ from human beings in 2 critical aspects: First, humans exist in a multimodal social environment, as evidenced by their joint expression of intentions and emotions through language, facial expressions, speech, and actions; second, humans can switch between modalities for emotional reasoning when dealing with emotions. They can also switch between different modalities to search for clues, eliminate ambiguities, and conduct emotional reasoning through interconnections. Therefore, creating a large-scale multimodal emotion dataset can contribute to the development of human-like emotion intelligence technology and the realization of more accurate emotion recognition. The third trend focuses on collecting natural-scene data, as emotional data collected in performance or evoked mode may not accurately represent real-life scenarios. However, collecting high-quality labeled emotional-physiological data in daily life remains a challenge due to the lack of hardware collection devices that are sufficiently comfortable and resistant to interference.#### Multimodal fusion technology innovationMultimodal fusion combines information from multiple modalities using multimodal representations for sentiment classification. It can enhance the performance of sentiment-computing models by playing a complementary and disambiguating role ]. Multimodal fusion methods can be classified as model-independent or model-based. Model-independent fusion methods do not rely on a specific deep-learning method, whereas model-based fusion methods do.There are 3 categories of model-independent fusion methods: early fusion (feature-based fusion), late fusion (decision-based fusion), and hybrid fusion (combination of the 2). Early fusion integrates features immediately after they are extracted and uses multiple signals to create a single feature vector, which is then modeled using machine-learning algorithms. The larger the number of features and the greater the variation in these features, the more challenging feature-level fusion becomes and the easier it is to overfit the training data. In contrast, late fusion performs integration only after each model outputs the results (e.g., classification or regression results). It can better handle overfitting but does not allow the classifier to train on all data simultaneously. The Dempster–Shafer theory of evidence is a generalization of Bayesian theory to subjective probability. It is widely used in late fusion models because of its ability to model uncertain knowledge and combine beliefs from different sources to obtain new beliefs that take into account all available evidence. Hybrid fusion combines the outputs of earlier fusion methods and unimodal predictors. Although it is flexible, careful design is required to determine the timing, modalities, and method of fusion based on the specific application problem and research content. Researchers must select the appropriate approach at their discretion.Model-based fusion methods address the multimodal fusion problem through implementation techniques and models, using 3 common methods: multiple kernel learning (MKL), graphical models (GMs), and neural networks (NNs). As these methods easily exploit the spatial and temporal structure of the data, they are particularly suitable for time-related modeling tasks. Additionally, they allow human expert knowledge to be embedded in the model, thereby enhancing interpretability. However, their disadvantage is that they are computationally expensive and challenging to train.Research has shown that synesthesia is generated not only in the cerebral cortex but also in the subcortical limbic system, including the thalamus, amygdala, and hippocampus, which are closely related to emotional processing ]. Inspired by the multistage fusion phenomenon that integrates multisensory information in the brain, a multistage multimodal emotion fusion method can be developed. This would first involve training a unimodal model, splicing it as an implicit state with another modal feature, training the bimodal model similarly, and continuing with this process until a multimodal model is obtained. In conclusion, multimodal fusion technology effectively utilizes the synergistic complementarity of different modal information ], enhances emotional understanding and expression, and improves model robustness and performance. This represents an important direction for future research.#### Data- and knowledge-driven technological innovationIn its early stages, affective computing research relied heavily on collected data to make inferences. However, this data-driven approach is both inefficient and ineffective at the application level. For humans to understand data fully, they must activate other associated information, such as potential knowledge or common sense. The human brain can seamlessly combine this information to enable more generalized, intelligent, and frugal computation for complex problems. Therefore, affective computing requires not only big data and extensive computing power but also the integration of knowledge. Knowledge guidance and inspiration can compensate for insufficient or uneven data quality while conserving computational power. For instance, in constructing a multidisciplinary and multi-faceted emotional knowledge map, fine-grained emotional knowledge integrated through emotional commonsense associations is used to enable the modeling of hierarchical logical relationships between aspect words and emotional words. This approach facilitates the dynamic correlation, aggregation, and reasoning of domain, aspect, and emotional knowledge. It provides an optimal solution for various applications of affective computing, such as efficient real-time online sentiment analysis, emotion-injected dialogue systems, and emotion-injected story generation. These applications provide dynamic and accurate domain-adaptive sentiment knowledge.#### Group affective computingCurrent research in affective computing primarily focuses on sentiment analysis at the individual level, neglecting the potential value of group-affective computing. For instance, emotions felt by individual employees can aggregate and spread to create “collective emotions” in the workplace. These shared emotions can considerably affect the organization by offering insights into absenteeism, intra-team communication, team cohesion and performance, and organizational citizenship behavior. As such, affective computing research could expand its focus from individual to collective affect analysis and the propagation of affect across people. Furthermore, group affective computing can predict consumer behavior. EEG-based hyperscanning technology, which explores dynamic brain activity between 2 or more interacting customers and their underlying neuroemotional activities, can be used to anticipate shared consumption intentions, panic buying, and group-buying marketing effects. Although group affective computing currently lacks a well-established research methodology, it is a promising direction for future studies.#### Unique emotional carriersEmotions are ubiquitous in human political, economic, and cultural life, and the carriers of emotions are continually increasing in number, making them a popular research topic. Several areas have been identified as key carriers of emotions. (a) Political speeches: CORPS is a corpus that contains political speeches with markers indicating audience reactions such as applause, standing ovations, and boos ]. Researchers can use this information to predict emotion-evoking actions and persuasive content that may induce empathy and sympathy in audiences. (b) Music and drama: Affective computing in music and drama provides a basis for the categorized retrieval of relevant emotional carriers. Advancements in artificial intelligence-generated content (AIGC) technology have made machine-generated music possible, and affective computing can enhance the generation of music to conform to emotional classifications. (c) Oil painting: As a representative art form, oil painting allows creators to express their innermost emotions. Its charm lies not in the degree of realism but in the emotions it conveys. Combining affective computing with oil painting would enable the exploration of artificial intelligence methods for emotional expression, the integration of technology and art, and the establishment of a library of emotion-inducing materials for oil paintings, thereby providing resources for the development of affective computing disciplines.### Outlook for future applications#### Affective brain–computer interfacesAffective brain–computer interfaces (aBCIs) are primarily designed to measure emotional states through neurological measurements and to recognize and/or regulate human emotions. Currently, aBCIs are one of the main methods of realizing emotional intelligence. At this stage, the most commonly used physiological signals for emotional brain–computer interfaces are EEG signals, which map closely to an individual’s emotional state. As in motor brain–computer interfaces, the human brain plays the role of a controller for the entire system. The first step involves decoding an individual’s initial emotional state and then recognizing and understanding their emotions. Subsequently, a control strategy or system is designed to achieve the target emotion using control signals or parameters that provide feedback to the brain, thereby forming a closed-loop system.Unlike facial expressions, physiological signals such as EEG signals are difficult to disguise and provide an accurate reflection of the real emotional state of the individual. As a result, affective brain–computer interfaces play a crucial role in clinical diagnostics and therapy. Their uses include detecting workload and mental state, using neurofeedback for stress relief, aiding in the diagnosis of social anxiety and other disorders ], and enabling objective assessment and intervention in depression. Furthermore, affective brain–computer interfaces have considerable potential for military applications. They can help maximize the physiological capabilities of individual soldiers, enhance their endurance and tolerance to extreme environments, and improve their overall physical and mental fitness. These objectives are achieved by installing electroencephalography electrodes inside combat helmets to detect threats and emotional signals emitted by the brain. The signals are then converted into computer language using computer algorithms, analyzed, and confirmed by combat command. Subsequently, threat warnings and reminders about emotional regulation are sent to the affected soldiers, and signals to cooperate in combat are transmitted to surrounding soldiers. In addition, direct transcranial current stimulation, transcranial electromagnetic stimulation, and deep brain cortex stimulation can act on the brain to eliminate fatigue, reduce stress and anxiety, control pain sensation, and enhance cognitive ability. This system helps improve the situational awareness of soldiers on the battlefield, thereby improving their ability to survive.The primary obstacle to the application of affective brain–computer interfaces is their unstable performance. Cross-modal affective models that rely on heterogeneous transfer learning (HTL) may be necessary for establishing reliable and robust aBCI technology in complex real-world environments. To address the missing-modalities problem, cross-modal emotion models comprehensively analyze signals from multiple modalities and extract correlation characteristics during the training process. In the testing stage, predictions are made based on partial modal information. For example, correlating EEG signals with eye movement enables the use of eye movement alone to assess emotions in scenarios where collecting EEG signals is difficult. The HTL approach ensures that performance degradation in the absence of modalities is acceptable, thereby improving model robustness. In addition, transfer-learning techniques based on deep and generative adversarial networks can solve the problem of individual differences. These techniques enable generalization from the source domain to the target domain, thereby expanding the scope of possible applications of affective brain–computer interfaces.#### Empathic human–computer dialogueThere have been 4 waves of change in the way people interact with machines. The first wave, represented by Microsoft, involved the organic fusion of the user interface, operating system, keyboard, and mouse. This greatly reduced the difficulty of human–computer interaction and contributed to the rapid popularization of the personal computer. The second wave, represented by Google, involved the organic integration of search engine and internet technologies. This integration broke down information silos and considerably expanded the boundaries of interaction. The third wave, represented by Apple, involved the miniaturization of computing represented by the smartphone. This breakthrough removed the physical space limitations of human–computer interaction, enabling interconnectivity anytime, anywhere. Currently, we are in the fourth wave, represented by OpenAI. This wave involves the comprehensive application of a human–computer dialogue system that makes human–computer interaction more anthropomorphic and naturalized.The essence of human–computer dialogue is to make human–computer interaction more human-like. Humans exchange information through natural language and multiple senses, and human–computer interaction can imitate this process through multimodal information for joint analysis and decision-making. Human–computer dialogue involves a diverse range of signals, including speech, text, and images (such as individual facial expressions and body movements), conveying information in both the rational and perceptual dimensions. Linguistic text serves as the ontology of intent understanding, but emotional information conveyed through voice intonation, facial expressions, and body movements plays a crucial role in disambiguation, which is essential for in-depth communication between humans and machines. The use of different emotional colors to express the same sentence results in entirely different connotations. As Nobel Prize winner Simon noted, emotion recognition is crucial for the communication and understanding of information. Therefore, affective computing offers machines the ability to achieve deep contextual understanding.In advanced technology fields, research has expanded to include machine expression and action generation, referred to as “multimodal emotional expression generation.” A current focus area is the development of a “virtual human” interface that not only appears human-like but also simulates human demeanor and behavior. For instance, voice-driven facial-expression animation generation technology can create virtual humans with facial expressions and lip, head, and body movements that closely resemble those of real people. The virtual human no longer has an empty skin but appears more 3-dimensional and vivid. The personalization of human–computer interaction lays the crucial foundations for future applications in areas such as elderly companions, intelligent customer service, and mayor hotlines, revealing important prospects for practical use.#### Emotion-assisted decision-makingHuman–computer interaction involves both shallow and deep levels. At the shallow level, machines are equipped with the ability to read and speak, whereas at the deep level, they are capable of thinking and making decisions like humans. Nobel Prize winner Kahneman described human decision-making as entailing 2 processes: fast (System 1) and slow (System 2). The unconscious “System 1” relies on emotions, experience, and rapid judgments, while the conscious “System 2” relies on rational deliberation. Emotions play an important role in advanced human thinking and decision-making. The book “Descartes’ Error” emphasizes that emotions are crucial for rational decision-making and behavior ]. Numerous studies have indicated that purely rational decision-making may not always be the optimal solution for humans when dealing with problems due to the complexity of the social environment. Incorporating emotional factors into the decision-making process may help individuals identify better solutions. Therefore, inputting emotional variables can enable machines to make decisions in a more human-like manner. In building a harmonious human–machine symbiotic society, it is essential to master this high-level function, which is also an important direction in affective computing research. The modeling of machine agents has begun to incorporate patterns of emotional influence on human rational decision-making and mechanisms for deciding and interrupting behaviors based on goals , ].Emotion-assisted decision-making abilities can be applied widely across various fields of human–machine collaboration. For example, in production tool manipulation, the operator’s emotional state regarding operation specifications, safety awareness, and accurate judgment has an impact. Monitoring and early warning of negative emotions, psychological stress, fatigue, and drowsiness, etc., can help identify potential anthropogenic risks to production safety. Machines can then optimize management decisions, intervene early, and intervene intelligently to avoid major accidents. In assisted driving, negative emotions such as anger and anxiety can seriously affect the driver’s concentration and may lead to traffic accidents. Emotion-assisted decision making can be incorporated into driver monitoring systems (DMS) that use facial-expression recognition technology and wearable devices to provide real-time monitoring of the driver’s emotional state. This approach equips the vehicle with enhanced safety performance and improves the overall driving experience ].#### Affective virtual realityThe metaverse is generating considerable interest in both industrial and academic circles as the next generation of immersive, full-fledged internet. It is considered a theme park for digitized human beings, a virtual complex resulting from the development of cutting-edge technologies, and a utopia where the human body and consciousness can cross physical time and space. As a new type of future living space, the development of the metaverse cannot be limited to creating a virtual space parallel to the real world. It should exist in human life like air, enabling humans to shuttle freely between the virtual and real worlds. Affective virtual reality is crucial for constructing the metaverse because it can considerably enhance an individual’s experience of bodily ownership, sense of agency, and situational awareness. In particular, an individual’s avatar in the metaverse, which is a core element of the metaverse construction, includes voice tone, facial expressions, body movements, and gestures that richly and 3-dimensionally express the individual’s emotions and create scenes and spaces for emotional twins ]. As in movies and literature, complex and emotionally rich avatar characters engage audiences more than simple and stable characters do. This appeal creates the illusion that avatars are alive and pass the Turing test, which enhances the audience’s interest and engagement in the virtual world ]. Affective virtual reality has considerable potential for applications in virtual reality socialization, virtual reality anchors, and virtual reality marketing.### LimitationsThis bibliometric analysis has several limitations that should be acknowledged. First, the basic processing unit of information in this study is the article in its entirety, and the full content of the literature has not been systematically broken down, which may result in incomplete analysis and conclusions. Second, the assumption that the articles contain information of equal quality makes it difficult to consider the objective differences in the value of the literature. In future research, a combination of bibliometrics and content analysis could be used to enhance the reliability and accuracy of the analytical results.## ConclusionAffective computing is a rapidly developing field with broad prospects. Emerging forces such as China and India are injecting strong momentum into the field. However, the field of affective computing also faces challenges and development trends in 10 aspects, including cultural background modeling, ethical and moral norms, and multimodal integration. Affective computing has great potential for application in 4 major fields and requires the joint efforts of researchers and industry practitioners. These efforts can make affective computing beneficial to the progress of human society by building a more anthropomorphic, harmonious, and natural human–computer symbiotic social form.## Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.## Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.## References1Keltner D, Sauter D, Tracy J, Cowen A. Emotional expression: Advances in basic emotion theory. _J Nonverbal Behav_. 2019; **43**(2):133–160.2Soleymani M, Garcia D, Jou B, Schuller B, Chang S-F, Pantic M. A survey of multimodal sentiment analysis. _Image Vis Comput_. 2017; **65**:3–14.3Bach DR, Dayan P. Algorithms for survival: A comparative perspective on emotions. _Nat Rev Neurosci_. 2017; **18**:311–319.4Chen L, Zhou M, Wu M, She J, Liu Z, Dong F, Hirota K. Three-layer weighted fuzzy support vector regression for emotional intention understanding in human–robot interaction. _IEEE Trans Fuzzy Syst_. 2018; **26**(5):2524–2538.5Kahneman D. _Thinking_, _fast and slow_. Macmillan, London, UK: Farrar, Straus and Giroux; 2011.6Fanselow MS. Emotion, motivation and function. _Curr Opin Behav Sci_. 2018; **19**:105–109.7Lopes PN, Salovey P, Coté S, Beers M. Emotion regulation abilities and the quality of social interaction. _Emotion_. 2005; **5**:113–118.8Suvilehto JT, Glerean E, Dunbar RIM, Hari R, Nummenmaa L. Topography of social touching depends on emotional bonds between humans. _Proc Natl Acad Sci U S A_. 2015; **112**:13811–13816.9Picard RW. _Affective computing_. Cambridge (MA): MIT Press; 1997.10Ekman P. Are there basic emotions? _Psychol Rev_. 1992; **99**(3):550–553.11Russell JA. A circumplex model of affect. _J Pers Soc Psychol_. 1980; **39**:1161–1178.12Mehrabian A. Framework for a comprehensive description and measurement of emotional states. _Genet Soc Gen Psychol Monogr_. 1995; **121**(3):339–361.13Bakker I, Van Der Voordt T, Vink P, De Boon J. Pleasure, arousal, dominance: Mehrabian and russell revisited. _Curr Psychol_. 2014; **33**:405–421.14Pozzi FA, Fersini E, Messina E, Liu B. Chapter\\\\xa01—Challenges of sentiment analysis in social networks: An overview. In: Pozzi FA, Fersini E, Messina E, Liu B, editors, _Sentiment analysis in social networks_. Boston: Morgan Kaufmann; 2017. p. 1–11.15Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. Poster presented at: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies; Portland, Oregon, USA; 2011. p. 142–150.16Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing; 2013; Seattle, WA, USA. p. 1631–1642.17Blitzer J, Dredze M, Pereira F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Poster presented at: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics; 2007; Prague, Czech Republic. p. 440–447.18Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of German emotional speech. _Interspeech_. 2005; **5**:1517–1520.19McKeown G, Valstar M, Cowie R, Pantic M, Schroder M. The SEMAINE Database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE Trans Affect Comput_. 2011; **3**(1):5–17.20Xu L, Xu M, Yang D. Chinese emotional speech database for the detection of emotion variations. _J Tsinghua Univ Nat Sci._ 2009;49(S1):1413–1418 _._21Poria S, Cambria E, Bajpai R, Hussain A. A review of affective computing: From unimodal analysis to multimodal fusion. _Inf Fusion_. 2017; **37**:98–125.22Wang Y, Song W, Tao W, Liotta A, Yang D, Li X, Gao S, Sun Y, Ge W, Zhang W, et al. A systematic review on affective computing: Emotion models, databases, and recent advances. _Inf Fusion_. 2022; **83–84**:19–52.23Zhang Z, Luo P, Loy CC, Tang X. From facial expression recognition to interpersonal relation prediction. _Int J Comput Vis_. 2018; **126**:550–569.24Mollahosseini A, Hasani B, Mahoor MH. AffectNet: A database for facial expression, valence, and arousal computing in the wild. _IEEE Trans Affect Comput_. 2019; **10**:18–31.25Li S, Deng W, Du J. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Paper presented at: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017; . Honolulu, HI. p. 2584–2593.26Li X, Pfister T, Huang X, Zhao G, Pietikäinen M. A spontaneous micro-expression database: Inducement, collection and baseline. Paper presented at: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG); 2013; Shanghai, China. p. 1–6.27Galvão F, Alarcão SM, Fonseca MJ. Predicting exact valence and arousal values from EEG. _Sensors (Basel)_. 2021; **21**(10):3414.28Shalbaf A, Bagherzadeh S, Maghsoudi A. Transfer learning with deep convolutional neural network for automated detection of schizophrenia from EEG signals. _Phys Eng Sci Med_. 2020; **43**(4):1229–1239.29Shirahama K, Grzegorzek M. Emotion recognition based on physiological sensor data using codebook approach. In: Piętka E, Badura P, Kawa J, Wieclawek W, editors. _Information technologies in medicine_. Cham: Springer International Publishing; 2016. p. 27–39.30Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I. DEAP: A database for emotion analysis\\\\xa0using physiological signals. _IEEE Trans Affect Comput_. 2012; **3**(1):18–31.31Duan R-N, Zhu J-Y, Lu B-L. Differential entropy feature for EEG-based emotion classification. Paper presented at: 2013 6th International IEEE/EMBS Conference on Neural Engineering (NER); 2013; San Diego, CA, USA. p. 81–84.32Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K. Introducing WESAD, a multimodal dataset for wearable stress and affect detection. Paper presented at: Proceedings of the 20th ACM International Conference on Multimodal Interaction; 2018; Boulder, CO, USA. p. 400–408.33Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. _Comput Linguist_. 2011; **37**(2):267–307.34Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. Paper presented at: Proceedings of the International Conference on Web Search and Web Data Mining—WSDM ’08; 2008; Palo Alto, CA, USA. p. 231.35Mullen T, Collier N. Sentiment analysis using support vector machines with diverse information sources. Paper presented at: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing; 2004; Barcelona, Spain. p. 412–418.36Pak A, Paroubek P. Text representation using dependency tree subgraphs for sentiment analysis. In: Xu J, Yu G, Zhou S, Unland R, editors. _Database systems for advanced applications_. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. p. 323–332.37Deng J, Ren F. A survey of textual emotion recognition and its challenges. _IEEE Trans Affect Comput_. 2023; **14**(1):49–67.38Heaton CT, Schwartz DM. Language models as emotional classifiers for textual conversation. Paper presented at: Proceedings of the 28th ACM International Conference on Multimedia; 2020; Seattle, WA, USA. p. 2918–2926.39Mao R, Liu Q, He K, Li W, Cambria E. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. _IEEE Trans Affect Comput_. 2022; **14**(3):1743–1753.40Lee CM, Narayanan SS. Toward detecting emotions in spoken dialogs. _IEEE Trans Audio Speech Lang Process_. 2005; **13**(2):293–303.41Lugger M, Yang B. The relevance of voice quality features in speaker independent emotion recognition. Paper presented at: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing—ICASSP ’07; 2007; Honolulu, HI, USA. p. IV-17–IV–20.42Likitha MS, Gupta SRR, Hasitha K, Raju AU. Speech based human emotion recognition using MFCC.Paper presented at: 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); 2017; Chennai, India. p. 2257–2260.43Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. _Speech Commun_. 2010; **52**(7–8):613–625.44Alisamir S, Ringeval F. On the evolution of speech representations for affective computing: A brief history and critical overview. _IEEE Signal Process. Mag_. 2021; **38**(6):12–21.45Stappen L, Baird A, Schumann L, Schuller B. The multimodal sentiment analysis in car reviews (MuSe-CaR) dataset: Collection, insights and improvements. _IEEE Trans Affect Comput_. 2023; **14**(2):1334–1350.46Huang Z, Dong M, Mao Q, Zhan Y. Speech emotion recognition using CNN. Paper presented at: Proceedings of the 22nd ACM International Conference on Multimedia; 2014; New York, NY, USA. p. 801–804.47Neumann M, Vu NT. Improving speech emotion recognition with unsupervised representation learning on unlabeled speech. Paper presented at: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2019; Brighton, UK. p. 7390–7394.48Abdelwahab M, Busso C. Domain adversarial for acoustic emotion recognition. _IEEE/ACM Trans Audio Speech Lang Process_. 2018; **26**(12):2423–2435.49Shan C, Gong S, McOwan PW. Facial expression recognition based on Local Binary Patterns: A comprehensive study. _Image Vis Comput_. 2009; **27**(6):803–816.50Chao W-L, Ding J-J, Liu J-Z. Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection. _Signal Process_. 2015; **117**:1–10.51James W. Review of la pathologie des emotions by Ch. Féré. _Philos Rev_. 1893; **2**:333–336.52Cannon WB. The James-Lange theory of emotions: A critical examination and an alternative theory. _Am J Psychol_. 1987; **100**:567–586.53Kim M-K, Kim M, Oh E, Kim S-P. A review on the computational methods for emotional state estimation from the human EEG. _Comput Math Methods Med_. 2013; **2013**: Article e573734.54Craik A, He Y, Contreras-Vidal JL. Deep learning for electroencephalogram (EEG) classification tasks: A review. _J Neural Eng_. 2019; **16**(3): Article 031001.55Maria MA, Akhand MAH, Shimamura T. Emotion recognition from EEG with normalized mutual information and convolutional neural network. Paper presented at: 2022 12th International Conference on Electrical and Computer Engineering (ICECE); 2022; Dhaka, Bangladesh. p. 372–375.56Rahman MM, Sarkar AK, Hossain MA, Hossain MS, Islam MR, Hossain MB, Quinn JMW, Moni MA. Recognition of human emotions using EEG signals: A review. _Comput Biol Med_. 2021; **136**: Article 104696.57D’mello SK, Kory J. A review and meta-analysis of multimodal affect detection systems. _ACM Comput Surv_. 2015; **47**(3):1–36.58He Z, Li Z, Yang F, Wang L, Li J, Zhou C, Pan J. Advances in multimodal emotion recognition based on brain–computer interfaces. _Brain Sci_. 2020; **10**(10):687.59Filippini C, Perpetuini D, Cardone D, Chiarelli AM, Merla A. Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review. _Appl Sci_. 2020; **10**(8):2924.60Spezialetti M, Placidi G, Rossi S. Emotion recognition for human-robot interaction: Recent advances and future perspectives. _Front Robot AI_. 2020; **7**: Article 532279.61Peng Y, Fang Y, Xie Z, Zhou G. Topic-enhanced emotional conversation generation with attention mechanism. _Knowl Based Syst_. 2019; **163**:429–437.62Dybala P, Ptaszynski M, Rzepka R, Araki K, Sayama K. Metaphor, humor and emotion processing in human-computer interaction. _Int J Comput Linguist Res_. 2013.63Goswamy T, Singh I, Barkati A, Modi A. Adapting a language model for controlled affective text generation. Paper presented at: Proceedings of the 28th International Conference on Computational Linguistics; 2020; Barcelona, Spain. p. 2787–2801.64Lei Y, Yang S, Wang X, Xie L. MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. _IEEE/ACM Trans Audio Speech Lang Process_. 2022; **30**:853–864.65Crawford K. Time to regulate AI that interprets human emotions. _Nature_. 2021; **592**(7853):167.66Ho M-T, Mantello P, Nguyen H-KT, Vuong Q-H. Affective computing scholarship and the rise of China: A view from 25 years of bibliometric data. _Humanit Soc Sci Commun_. 2021; **8**: Article 282.67Yadegaridehkordi E, Noor NFBM, Ayub MNB, Affal HB, Hussin NB. Affective computing in education: A systematic review and future research. _Comput Educ_. 2019; **142**: Article 103649.68Wu C-H, Huang Y-M, Hwang J-P. Review of affective computing in education/learning: Trends and challenges. _Br J Educ Technol_. 2016; **47**(6):1304–1323.69Liberati G, Veit R, Kim S, Birbaumer N, von Arnim C, Jenner A, Lulé D, Ludolph AC, Raffone A, Belardinelli MO, da Rocha JD, Sitaram R. Development of a binary fMRI-BCI for Alzheimer patients: A semantic conditioning paradigm using affective unconditioned stimuli. Paper presented at: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction; 2013; Geneva, Switzerland. p. 838–842.70Yuvaraj R, Murugappan M, Mohamed Ibrahim N, Iqbal Omar M, Sundaraj K, Mohamad K, Palaniappan R, Mesquita E, Satiyan M. On the analysis of EEG power, frequency and asymmetry in Parkinson’s disease during emotion processing. _Behav Brain Funct_. 2014; **10**:12.71Baki P, Kaya H, Çiftçi E, Güleç H, Salah AA. A multimodal approach for mania level prediction in bipolar disorder. _IEEE Trans Affect Comput_. 2022; **13**(4):2119–2131.72Mohammadi-Ziabari SS, Treur J. Integrative biological, cognitive and affective modeling of a drug-therapy for a post-traumatic stress disorder. In: Fagan D, Martín-Vide C, O’Neill M, Vega-Rodríguez MA, editors. _Theory and practice of natural computing_. Cham: Springer International Publishing; 2018. p. 292–304.73Tivatansakul S, Ohkura M. Healthcare system focusing on emotional aspects using augmented reality—Implementation of breathing control application in relaxation service. Paper presented at: 2013 International Conference on Biometrics and Kansei Engineering; 2013; Tokyo, Japan. p. 218–222.74Zenonos A, Khan A, Kalogridis G, Vatsikas S, Lewis T, Sooriyabandara M. HealthyOffice: Mood recognition at work using smartphones and wearable sensors. Paper presented at: 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops); 2016; Sydney, NSW, Australia. p. 1–6.75Weziak-Bialowolska D, Bialowolski P, Lee MT, Chen Y, VanderWeele TJ, McNeely E. Psychometric properties of flourishing scales from a comprehensive well-being assessment. _Front Psychol_. 2021; **12**: Article 652209.76Pei G, Xiao Q, Pan Y, Li T, Jin J. Neural evidence of face processing in social anxiety disorder: A systematic review with meta-analysis. _Neurosci Biobehav Rev_. 2023; **152**: Article 105283.77Pei G, Li T. A literature review of EEG-based affective computing in marketing. _Front Psychol_. 2021; **12**: Article 602843.78Valle-Cruz D, Fernandez-Cortez V, López-Chau A, Sandoval-Almazán R. Does twitter affect stock market decisions? Financial sentiment analysis during pandemics: A comparative study of the H1N1 and the COVID-19 periods. _Cognit Comput_. 2022; **14**(1):372–387.79Gómez LM, Cáceres MN. Applying data mining for sentiment analysis in music. In: De la Prieta F, Vale Z, Antunes L, Pinto T, Campbell AT, Julián V, Neves AJR, Moreno MN, editors. _Trends in cyber-physical multi-agent systems_. Cham: Springer International Publishing; 2018. p. 198–205.80Yu L, Zhang W, Wang J, Yu Y. SeqGAN: Sequence generative adversarial nets with policy gradient. Paper presented at: Proceedings of the AAAI Conference on Artificial Intelligence; 2017; San Francisco, CA, USA. p. 31.81Oliveira HG. A survey on intelligent poetry generation: Languages, features, techniques, reutilisation and evaluation. Paper presented at: Proceedings of the 10th International Conference on Natural Language Generation; 2017; Santiago de Compostela, Spain. p. 11–20.82Zhang X, Lapata M. Chinese Poetry Generation with Recurrent Neural Networks. Paper presented at: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); 2014; Doha, Qatar. p. 670–680.83Mao G, Liu X, Du H, Zuo J, Wang L. Way forward for alternative energy research: A bibliometric analysis during 1994–2013. _Renew Sustain Energy Rev_. 2015; **48**:276–286.84Haustein S, Larivière V. The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects. In: Welpe I, Wollersheim J, Ringelhan S, Osterloh M, editors. _Incentives and performance: Governance of research organizations_. Cham: Springer International Publishing; 2014. p. 121–139.85Hammarfelt B, Rushforth AD. Indicators as judgment devices: An empirical study of citizen bibliometrics in research evaluation. _Res Eval_. 2017; **26**(3):169–180.86Wang J, Veugelers R, Stephan P. Bias against novelty in science: A cautionary tale for users of bibliometric indicators. _Res Policy_. 2017; **46**(8):1416–1436.87Van Eck NJ, Waltman L. Software survey: VOSviewer, a computer program for bibliometric mapping. _Scientometrics_. 2010; **84**(2):523–538.88Šabanović S. Robots in society, society in robots. _Int J of Soc Robotics_. 2010; **2**:439–450.89Hofstede G. _Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations_. London, UK: Sage; 2001.90Mehrabian A. _Communication without words. Communication theory. 2nd ed._ London, UK: Routledge; 2008.91Du S, Tao Y, Martinez AM. Compound facial expressions of emotion. _Proc Natl Acad Sci U S A_. 2014; 111(15):E1454–E1462.92Martinez AM. Computational models of face perception. _Curr Dir Psychol Sci_. 2017; **26**(3):263–269.93Dragano N, Lunau T. Technostress at work and mental health: Concepts and research results. _Curr Opin Psychiatry_. 2020; **33**(4):407–413.94LeDoux J. _The emotional brain: The mysterious underpinnings of emotional life_. New York, NY, USA: Simon and Schuster; 1998.95Pessoa L, Adolphs R. Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance. _Nat Rev Neurosci_. 2010; **11**(11):773–782.96Price TF, Peterson CK, Harmon-Jones E. The emotive neuroscience of embodiment. _Motiv Emot_. 2012; **36**:27–37.97Cytowic RE. _Synesthesia: A union of the senses_. Cambridge, MA, USA: MIT Press; 2002.98Guerini M, Strapparava C, Stock O. CORPS: A corpus of tagged political speeches for persuasive communication processing. _J Inf Technol Politics_. 2008; **5**(1):19–32.99Damasio AR. _Descartes’ error_. New York, NY, USA: Random House; 2006.100Scheutz M. The inherent dangers of unidirectional emotional bonds between humans and social robots. In: Lin P, Abney K, Bekey GA, editors. _Robot ethics: The ethical and social implications of robotics_. Cambridge (MA): MIT Press; 2011. p. 205.101Scheutz M, Schermerhorn P. Dynamic robot autonomy: Investigating the effects of robot decision-making in a human-robot team task. Paper presented at: Under review for the 4th ACM International Conference on Human-Robot Interaction; 2009; La Jolla, CA, USA.102Gill R, Singh J. A review of neuromarketing techniques and emotion analysis classifiers for visual-emotion mining. Paper presented at: 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART); 2020; Moradabad, India. p. 103–108.103Pei G, Li B, Li T, Xu R, Dong J, Jin J. Decoding emotional valence from EEG in immersive virtual reality. Paper presented at: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC); 2022; Chiang Mai, Thailand. p. 1469–1476.104Ochs M, Sadek D, Pelachaud C. A formal model of emotions for an empathic rational dialog agent. _Auton Agent Multi-Agent Syst_. 2012; **24**:410–440.### Information#### Published In![]( ComputingVolume 3#### CopyrightCopyright © 2024 Guanxiong Pei et\\\\xa0al.Exclusive licensee Zhejiang Lab. No claim to original U.S. Government Works. Distributed under a .#### Article versions#### Submission history**Received**: 15 October 2023**Accepted**: 6 December 2023**Published online**: 5 January 2024#### Acknowledgments**Funding:** This work was supported by the National Natural Science Foundation of China (grant number T2241018), the Zhejiang Provincial Natural Science Foundation of China (grant number LQ22C090007), the National Science and Technology Major Project of the Ministry of Science and Technology of China (grant number 2021ZD0114303), and the Open Research Project of the Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education of Shanghai) (grant numbers 2023KFKT003 and 2022KFKT002).**Author contributions:** G.P.: Conceptualization, methodology, writing (original draft), and funding acquisition. H.L.: Methodology, data curation, formal analysis, and visualization. Y.L.: Writing (review and editing). Y.W.: Data curation, formal analysis, and visualization. S.H.: Writing (original draft). T.L.: Resources, supervision, validation, and funding acquisition.**Competing interests:** The authors declare that they have no competing interests.#### Data AvailabilityThe data and code used in this study are available from the corresponding author upon request.### Authors#### AffiliationsGuanxiongPei Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.HaiyingLi National Science Library, Chinese Academy of Sciences, Beijing, China.YandiLu Center for Psychological Sciences, Zhejiang University, Hangzhou, China.YanleiWangDe.InnoScience, Deloitte, Shanghai, China.ShizhenHuaResearch Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.TaihaoLi  Research Center for Multi-Modal Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, Hangzhou, China.#### Funding Informationthe Zhejiang Provincial Natural Science Foundation of China : LQ22C090007the National Natural Science Foundation of China : T2241018the National Science and Technology Major Project of the Ministry of Science and Technology of China : 2021ZD0114303the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2023KFKT003the Open Research Project of Key Laboratory of Brain-Machine Intelligence for Information Behavior (Ministry of Education and Shanghai) : 2022KFKT002#### Notes*Address correspondence to: ### Metrics#### Article UsageArticle Metrics- - No data available.05001,0001,500Jan 2024Apr 2024Jul 2024Oct 2024Jan 2025Apr 2025Jul 202520,97629- - - - Total number ofdownloads and citation**Note:** The article usage is presented with a three- to four-day delay and will update daily once available. Due to this delay, usage data will not appear immediately following publication.Citation information is sourced from  service.#### Altmetrics]( more details]( up by **10** news outlets]( by **7** X users]( **1** Facebook pages]( readers on Mendeley#### Dimensions32Total citations32Recent citationsn/aField Citation Ration/aRelative Citation Ratio]( 10 citation in Web of Science- 29 citation in Crossref#### scite![]( Citations22010Citing PublicationsSupportingMentioningContrastingSee how this article has been cited at scite shows how a scientific paper has been cited by providing the context of the citation, a classification describing whether it supports, mentions, or contrasts the cited claim, and a label indicating in which section the citation was made.### Citations#### Cite as- Guanxiong Pei,- Haiying Li,- Yandi Lu,- Yanlei Wang,- Shizhen Hua,- Taihao Li.Affective Computing: Recent Advances, Challenges, and Future Trends._Intell Comput._2024;3:0076.DOI: #### Export citationSelect the format you want to export the citation of this publication.Please select one from the listRIS (ProCite, Reference Manager)EndNoteBibTexMedlarsRefWorksDirect import01. - Jonas Fritsch,    Sticking with Affect in HCI and Design: from Interaction to Relation, Proceedings of the sixth decennial Aarhus conference: Computing X Crisis, (202-215), (2025).     02. - Yuan Zhou,    - Siamak Khatibi,Exploring Artificial Personality Grouping Through Decision Making in Feature Spaces, AI, **6**, 8, (184), (2025). 03. - FNU Rakhi,    Modeling Students’ Emotions in Computing Education: A Context-Specific Multi-Modal Approach, Proceedings of the 2025 ACM Conference on International Computing Education Research V.2, (59-60), (2025).     04. - Jialin Sun,    Research on the Design of Intelligent Voice Interaction System Based on Affective Computing, International Scientific Technical and Economic Research, (1-15), (2025).     05. - Susanne Draheim,    - Kai von Luck,    - Larissa Putzar,Working with the Gaps in Understanding: Revisiting an Approach to Affective Computing Research in the EmotionBike Project, Proceedings of the 18th ACM International Conference on PErvasive Technologies Related to Assistive Environments, (596-601), (2025). 06. - Ken Nee Chee,    Affectagogy Unleashed, Pedagogical Approaches to Bridging Emotional and Cognitive Learning, (277-292), (2025).     07. - Joaquim C. Reis,    - Luzia Travado,    - Michael H. Antoni,Advancing health psychology research in oncology: Biobehavioral models, stress pathways, and stress-management interventions for cancer patients, International Journal of Clinical and Health Psychology, **25**, 3, (100615), (2025). 08. - Manuel Paniagua-Gómez,    - Manuel Fernandez-Carmona,Trends and Challenges in Real-Time Stress Detection and Modulation: The Role of the IoT and Artificial Intelligence, Electronics, **14**, 13, (2581), (2025). 09. - Thuy Thi Tran,    - Thang Van Le,    - Nam Hoang Le,    - Anh Van Thi Dam,    - Thao Thi Nguyen,    - Anh Thi Tuan Nguyen,    - Hue Thi Nguyen,Emotional attachment to artificial intelligence and perceived social isolation among university students: An application of Sternberg’s triangular theory of love, Multidisciplinary Science Journal, **7**, 12, (2025662), (2025). 10. - Jan Kincl,    - Marc T. P. Adam,    - Tanja Pavleska,The Role of\\\\xa0Human Factors in\\\\xa0Cybersecurity Incident Management, HCI for Cybersecurity, Privacy and Trust, (35-53), (2025). 11. See moreLoading...### View options#### PDF formatDownload this article as a PDF file### Share#### Copy the article linkCopy LinkCopied!Copying failed.#### Share on social media        | Now Reading:- - \")- - - - - -  xrefBack.goToxrefBack.goToRequest permissionsExpand AllCollapseExpand for moreShow all referencesSHOW ALL BOOKSCopy link✓Thanks for sharing!Find any serviceA2A![](\\'}]', name='web_search_tool', call_id='call_0QfXVfV9DNu7IVPtgrFNajy7', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 6, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_d74ZAGK7DVpc4VJFOMsgev51', is_error=True), FunctionExecutionResult(content='[{\\'title\\': \\'Best Vibe Coding Tools 2025: Top Picks for Better Coding\\', \\'url\\': \\'https://azumo.com/artificial-intelligence/ai-insights/best-vibe-coding-tools\\', \\'markdown\\': \\'- - - # Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunExplore the 10 best vibe coding tools for 2025, designed to make coding a more enjoyable and efficient experience. These tools enhance developer productivity with features like AI assistance, fast performance, intuitive design, and smooth collaboration. Learn what makes a tool have \"good vibes\" and why selecting the right coding environment is essential for long-term success. Whether you\\\\\\'re looking for minimal setup friction or an elegant interface, these tools will improve your coding journey. From Cursor IDE to GitHub Copilot, find the best fit for your workflow and elevate your development process.Written by:September 2, 2025![]( Best Vibe Coding Tools 2025: 10 Champions That Actually Make Coding FunLet\\\\\\'s be real for a second. You can absolutely build amazing software with basic text editors and command-line tools. But why would you want to? The days of suffering through clunky interfaces and frustrating workflows are long gone. Today\\\\\\'s coding tools aren\\\\\\'t just about functionality anymore; they\\\\\\'re about creating an experience that makes you actually excited to write code.Think about it this way: you probably spend 8+ hours a day staring at your development environment. Shouldn\\\\\\'t it feel good? Shouldn\\\\\\'t it make you more productive instead of fighting you every step of the way? That\\\\\\'s exactly what we\\\\\\'re talking about when we say \"vibe\" in coding tools.The numbers back this up, too. According to Stack Overflow\\\\\\'s 2024 Developer , 76% of developers are now using or planning to use AI coding assistants. The entire software development tools market is projected to hit . This isn\\\\\\'t just about new features anymore; it\\\\\\'s about developer happiness.At Azumo, we’ve worked with leading companies like Meta, Wolters Kluwer, and Wine Enthusiast to deliver cutting-edge AI-driven solutions, helping teams optimize their workflows and enhance business outcomes.Our deep expertise in  and our  give us a unique perspective on the best tools available in the coding space. We understand that the right tools can dramatically improve a developer\\\\\\'s experience and productivity, which is why we’re excited to share our insights on the best vibe coding tools for 2025.## What Makes a Coding Tool Have \"Good Vibes\"?Before we jump into the tools themselves, let\\\\\\'s talk about what we actually mean by \"good vibes\" in a development environment. It\\\\\\'s not just about pretty colors or trendy interfaces (though those don\\\\\\'t hurt).**Smooth Performance That Doesn\\\\\\'t Get in Your Way:** You know that feeling when you\\\\\\'re in the zone and your editor starts lagging? Total vibe killer. Great tools respond instantly to your input, handle large files without breaking a sweat, and never make you wait around while they \"think.\"**Intuitive Design That Feels Natural:** The best coding tools feel like they were designed by someone who actually codes. They put the most important features right where you expect them, use shortcuts that make sense, and don\\\\\\'t make you hunt through menus for basic functionality.**Smart Assistance Without Being Pushy:** Modern tools are getting really good at predicting what you want to do next. The good ones offer helpful suggestions and automate tedious tasks. The great ones do it without interrupting your flow or making you feel like the computer is trying to do your job for you.**Beautiful Aesthetics That Reduce Eye Strain:** Look, we\\\\\\'re not being shallow here. When you\\\\\\'re staring at code for hours, having clean typography, thoughtful color schemes, and well-organized layouts actually reduces cognitive load. Your brain doesn\\\\\\'t have to work as hard to parse what you\\\\\\'re looking at.**Strong Community and Ecosystem:** Tools with great vibes usually have passionate communities behind them. That means better documentation, more extensions, helpful tutorials, and people who actually want to help when you run into problems.**Flexibility to Match Your Workflow:** Everyone codes differently. Some people love keyboard shortcuts, others prefer clicking around. Some want everything in one window, others like to spread things across multiple monitors. The best tools adapt to how you work, not the other way around.**Minimal Setup Friction:** Nothing kills enthusiasm faster than spending three hours trying to configure a tool before you can write your first line of code. Great tools either work beautifully out of the box or make customization genuinely enjoyable.This combination of factors creates what developers often describe as \"flow state\"—that magical zone where you\\\\\\'re completely absorbed in solving problems and everything just works. That\\\\\\'s what we\\\\\\'re looking for in our tool selection.## Top 10 Best Vibe Coding Tools### 1. !If we\\\\\\'re talking about tools that completely change how coding feels, Cursor has to top the list. This isn\\\\\\'t just another editor with AI bolted on; it\\\\\\'s been built from the ground up around the idea of AI-human collaboration.What makes Cursor special is how naturally the AI assistance flows into your regular coding process. Instead of feeling like you\\\\\\'re constantly asking a chatbot for help, it\\\\\\'s more like having a really smart pair programming partner who knows exactly what you\\\\\\'re trying to build. Cursor IDE has emerged as particularly noteworthy, making waves as one of the best AI coding tools available due to its superior auto-completion features and integration capabilities.#### Key Features:- Context-aware code completion that actually understands your project- Natural language to code conversion that works surprisingly well- Built-in chat interface for explaining complex code sections- Multi-file editing with AI understanding the relationships between files- Beautiful, clean interface that stays out of your way**Why It Has Great Vibes:** Cursor feels like coding in the future. The AI suggestions are smart enough to be helpful but not so aggressive that they interrupt your thought process. Plus, the interface is clean and modern without being distracting.### 2. !VS Code basically won the editor wars, and for good reason. Visual Studio Code continues to dominate the IDE landscape, establishing itself as the cornerstone development environment regardless of programming language, whether JavaScript, Python, or C++.What\\\\\\'s amazing about VS Code isn\\\\\\'t just that it\\\\\\'s free and powerful; it\\\\\\'s that Microsoft somehow made it feel lightweight and fast despite having more features than most full IDEs. The extension ecosystem is incredible, which means you can customize it to feel exactly right for any type of project.#### Key Features:- Massive extension marketplace covering every language and framework- Integrated terminal that actually works well- Git integration that makes version control painless- IntelliSense that\\\\\\'s smart without being annoying- Cross-platform consistency**Why It Has Great Vibes:** VS Code strikes the perfect balance between power and simplicity. It\\\\\\'s familiar enough that you can jump right in, but deep enough that you\\\\\\'ll keep discovering new features months later. The community support means there\\\\\\'s always someone who\\\\\\'s solved whatever problem you\\\\\\'re facing.### 3. !GitHub Copilot isn\\\\\\'t an editor itself, but it transforms whatever editor you\\\\\\'re using. According to , GitHub Copilot stands as the second most popular AI coding tool, having been tried by 40% of developers and regularly used by 26% of respondents.What\\\\\\'s wild about Copilot is how it actually learns your coding style over time. It\\\\\\'s not just suggesting random code snippets—it\\\\\\'s suggesting code that looks like something you would write, just faster than you could type it.#### Key Features:- Real-time code suggestions as you type- Support for dozens of programming languages- Context awareness that spans multiple files- Integration with the most popular editors- Comment-to-code generation**Why It Has Great Vibes:** Copilot feels like having a coding buddy who\\\\\\'s really good at remembering syntax and common patterns. It handles the tedious stuff so you can focus on the interesting problems. Plus, when it suggests exactly what you were about to type, it\\\\\\'s genuinely satisfying.### 4. !Zed is the new kid on the block, but it\\\\\\'s making waves for all the right reasons. Built in Rust, it\\\\\\'s designed from the ground up for performance and collaboration. If you\\\\\\'re tired of editors that slow down as your projects get bigger, Zed might be exactly what you need.#### Key Features:- Lightning-fast startup and response times- Real-time collaborative editing- Beautiful, minimalist interface- Built-in AI assistance- Memory-efficient architecture**Why It Has Great Vibes:** Zed feels incredibly snappy and responsive. There\\\\\\'s something satisfying about an editor that never makes you wait. The collaborative features are particularly smooth—you can literally watch your teammates code in real-time without any of the lag or sync issues you get with other tools.### 5.  (via Command Line)!According to , the best AI developer tools in 2025 include Aider, Cursor, Zed, Claude Code, Windsurf, and GitHub Copilot, representing the cutting edge of AI-assisted development.Claude Code brings AI assistance directly to your terminal, which is perfect if you\\\\\\'re someone who lives in the command line. It\\\\\\'s particularly good at understanding complex codebases and making suggestions that actually make sense in context.#### Key Features:- Terminal-native AI assistance- Deep understanding of project structure- Excellent at refactoring and code optimization- Natural language query capabilities- Git integration for tracking changes**Why It Has Great Vibes:** If you\\\\\\'re a terminal person, Claude Code feels like having a really smart assistant who speaks your language. It doesn\\\\\\'t try to force you into a GUI—it works within the workflow you already love.### 6. !Windsurf represents the next generation of AI-integrated development environments. It\\\\\\'s designed around the principle that AI and human developers should work together seamlessly, not as separate entities.#### Key Features:- Deep AI integration across all features- Intelligent project understanding- Advanced debugging assistance- Collaborative AI agent capabilities- Modern, intuitive interface**Why It Has Great Vibes:** Windsurf feels like what development environments will look like in five years. The AI integration is so smooth that it doesn\\\\\\'t feel like you\\\\\\'re using an AI tool—it just feels like you\\\\\\'ve gotten really good at coding really fast.### 7. !Don\\\\\\'t sleep on Sublime Text. According to , Sublime Text maintains its reputation for exceptional performance, known for quick response times and the ability to handle large files with ease, featuring innovative multi-caret editing capabilities that allow developers to edit multiple lines simultaneously.While everyone else has been adding AI and collaborative features, Sublime Text has been perfecting the core editing experience. Sometimes you just want an editor that\\\\\\'s incredibly fast, looks beautiful, and never gets in your way.#### Key Features:- Incredibly fast performance, even with huge files- Multi-caret editing for complex refactoring- Clean, distraction-free interface- Powerful search and replace capabilities- Minimal memory usage**Why It Has Great Vibes:** Sublime Text feels like a precision instrument. Every interaction is crisp and immediate. It\\\\\\'s the editor equivalent of a really good mechanical keyboard: simple, reliable, and satisfying to use.### 8. !Aider is a command-line AI coding assistant that\\\\\\'s particularly good at making changes to existing codebases. If you work with large, complex projects and need AI help that actually understands the bigger picture, Aider is worth checking out.#### Key Features:- Git-aware AI assistance- Excellent at understanding large codebases- Command-line focused workflow- Automated testing integration- Smart refactoring capabilities**Why It Has Great Vibes:** Aider respects your existing workflow while making it significantly more powerful. It doesn\\\\\\'t try to change how you work—it just makes you better at what you\\\\\\'re already doing.### 9. !Replit has transformed from a simple online coding playground into a full-featured development environment. What makes it special is how quickly you can go from idea to running code, especially for collaborative projects.#### Key Features:- Zero-setup development environment- Real-time collaboration features- Built-in deployment and hosting- AI-powered code assistance- Educational-friendly features**Why It Has Great Vibes:** Replit removes all the friction from getting started. You can literally go from having an idea to sharing a working prototype in minutes. It\\\\\\'s particularly great for experimenting with new technologies or collaborating with people who might not have development environments set up.### 10. !Tabnine focuses specifically on AI-powered code completion, and it\\\\\\'s really good at it. What sets it apart is the focus on privacy and the ability to train on your own codebase to provide more relevant suggestions.#### Key Features:- Privacy-focused AI completion- Team training capabilities- Multi-language support- Integration with the most popular editors- On-premises deployment options**Why It Has Great Vibes:** Tabnine feels thoughtful and respectful of your work. The privacy focus means you can use it on sensitive projects, and the team training features help it get better at suggesting code that matches your team\\\\\\'s style and standards.## Productivity Enhancement Features That MatterBeyond the core editing experience, the tools that have the best vibes integrate seamlessly with the rest of your development workflow. According to , IDEs substantially boost developer productivity by consolidating all necessary coding tools into one cohesive workspace, providing access to features like built-in error detection and version control integration rather than requiring manual coding in simple text editors.**Version Control Integration:** The best coding tools make Git feel natural instead of scary. They show you what\\\\\\'s changed, make it easy to stage and commit files, and help you resolve merge conflicts without wanting to throw your computer out the window.**Project Management Connections:** According to , popular project and task management tools that enhance the coding experience include Jira, Trello, Asana, and Microsoft Planner, while continuous integration and deployment tools have become essential components of modern software development practices. When your editor can show you which issues you\\\\\\'re working on or automatically update task status based on your commits, it creates a much smoother workflow.**CI/CD Pipeline Integration:** Nothing kills momentum like having to switch between your editor and a web browser to check if your build passed. The best tools give you build status right in your editor and make it easy to see what went wrong when something breaks.**Real-Time Collaboration:** Whether you\\\\\\'re pair programming or just need to show someone a bug, tools that make collaboration feel natural rather than awkward create much better team experiences.## The Future of Vibe-Focused Development ToolsWe\\\\\\'re in the middle of a major shift in how development tools work. According to , product innovation in the software development tools market continues to expand rapidly, with nearly 57% of newly introduced tools now supporting low-code or no-code development approaches.AI integration is becoming table stakes, but the focus is shifting from \"AI that can code\" to \"AI that helps you code better.\" We\\\\\\'re seeing tools that understand not just syntax but intent, that can refactor entire codebases while maintaining your coding style, and that can explain complex code in plain English.The developer community is also driving innovation in ways we haven\\\\\\'t seen before. According to , which received over 49,000 responses from 177 countries, provides crucial insights into the evolving developer landscape. The survey focuses on 314 different technologies, with new emphasis on AI agent tools, LLMs, and community platforms, indicating where the industry is heading.Visual design and user experience are becoming just as important as functionality. Developers are no longer willing to use tools that feel clunky or outdated, even if they\\\\\\'re technically powerful.## Choose Your Coding VibeHere\\\\\\'s the thing about coding tools—the \"best\" one is the one that fits how you work and think. If you love keyboard shortcuts and never want to touch a mouse, Sublime Text or Vim-style editors might be your jam. If you\\\\\\'re working on a team and need seamless collaboration, Cursor or Zed could be game-changers. If you\\\\\\'re just getting started and want something that works well out of the box, VS Code is still hard to beat.The good news is that most of these tools are free or have generous trial periods, so you can actually try them out and see what clicks. Don\\\\\\'t just read reviews—actually use them for a few days on real projects. You\\\\\\'ll know pretty quickly which ones feel right.The software development industry continues to grow at an incredible pace, with the developer community expanding from 27 million to a projected  in the next decade. As this community grows, the demand for tools that not only function well but actually make coding enjoyable will only increase.Remember, you\\\\\\'re going to spend thousands of hours with whatever tools you choose. Pick ones that make those hours feel productive, creative, and maybe even fun. The best coding tool is the one that gets out of your way and lets you focus on building amazing things.Try a few of these tools this week. Set up Cursor for your next AI-assisted project, give Zed a spin if you want something blazingly fast, or explore how GitHub Copilot could speed up your current workflow. The future of coding is here, and it feels pretty good.**Ready to level up your development experience?** At , we understand that great software comes from developers who have access to the best tools and environments. Our nearshore development teams use cutting-edge tools and AI-powered workflows to build intelligent applications that drive real business results. Whether you\\\\\\'re looking to augment your existing team or build something completely new, we\\\\\\'d love to show you how the right combination of talent and tools can transform your development process.\\\\u200d![]( the Author:ML/AI & Backend EngineerGuillermo Germade, Data Science Expert at Azumo, specializes in building machine learning models and AI systems, focusing on consumer tech, entertainment, and big data. reCAPTCHASelect all images with **cars** Click verify once there are none left|     |     |     || --- | --- | --- || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( || ![]( | ![]( | ![]( |Please try again.Please select all matching images.Please also check the new images.Please select around the object, or reload if there are none.Verify\\'}, {\\'title\\': \\'Top 10 Vibe Coding Tools That Feel Like Magic in 2025\\', \\'url\\': \\'https://dev.to/therealmrmumba/top-10-vibe-coding-tools-that-feel-like-magic-in-2025-1md\\', \\'markdown\\': \\'AI is no longer just assisting developers—it’s transforming how we build software. Whether you\\\\\\'re a solo indie hacker, a startup engineer, or part of an enterprise team, there’s now an AI tool that aligns with your workflow. The best ones don’t simply automate tasks; they feel intuitive, context-aware, and remarkably efficient.* * *## Why These Tools Matter in 2025Software development has evolved. We’ve moved past the era of simple code autocomplete and template generators. Today, developers demand tools that integrate deeply with their tech stack, adapt to their workflow, and help them ship faster without compromising quality.Here are three key trends that shape this new landscape:1. **Autonomous Assistance** – Developers now rely on AI tools that can complete multi-step tasks with minimal guidance, from writing modules to fixing bugs.2. **Context-Aware Coding** – Tools that understand your full codebase, API structure, and data flow are no longer a luxury—they’re a necessity.3. **Real-Time Collaboration** – AI is becoming a collaborative partner in the development process, offering suggestions, generating code, and even reviewing pull requests in real-time.]( list focuses on tools that exemplify these traits and offer genuine improvements to the way developers work in 2025.* * *## The Top 10 Vibe Coding Tools in 2025### 1. Apidog MCP Server**The missing link between your API and AI coding assistants** bridges the gap between your backend and tools like Cursor, Cline, and Copilot Workspace. Once your OpenAPI definitions are connected, you can:- Autogenerate API logic and DTOs based on updated schema- Let AI tools access real-time API documentation for better suggestions- Streamline backend/frontend development across large teams]( especially powerful in teams that maintain frequently changing APIs or use domain-driven design.* * *### 2. Cursor]( code editor designed for pair programming with GPT-4** is built on VS Code but reimagines the editing experience with AI at the center. Unlike plugins, Cursor deeply integrates AI into every layer:- Offers context-aware autocomplete that understands your full project- Supports inline debugging and refactoring with natural language prompts- Can explain, rewrite, or extend code based on your cursor positionIt’s ideal for fast iteration, onboarding into unfamiliar codebases, or accelerating solo projects.* * *### 3. Copilot Workspace]( task-oriented Copilot that tackles full development cycles**Copilot Workspace is a major leap from GitHub Copilot. Rather than suggesting code line by line, it can:- Understand a GitHub issue and break it into subtasks- Write full PRs for those subtasks- Summarize what it did and wait for your feedbackThis makes it perfect for junior developers, product engineers, or teams with many open tickets to manage.* * *### 4. Sweep]( that turns GitHub issues into functioning pull requests** is a bot that integrates directly into your GitHub repo. You describe an issue, and it:- Scans the entire repo to understand the current architecture- Writes a PR that resolves the issue- Updates associated files and testsFor lean teams or solo founders managing multiple products, Sweep can save hours of context-switching.* * *### 5.Fine by Fine.dev]( is the AI assistant that startup CTOs have been dreaming of. This AI-powered development platform automates tasks throughout the software development lifecycle, from coding and debugging to testing and code review.What makes Fine special is its ability to work autonomously—it can create and implement code, conduct pull request reviews, generate tests, and handle common issues without constant human input. The GitHub integration and support for asynchronous work make it particularly suitable for distributed teams.* * *### 6. Continue]( with your codebase in any editor** is an open-source extension that works in VS Code and JetBrains. It allows you to:- Chat with your entire codebase- Ask for refactors, explanations, or generation tasks- Use local models or connect with remote LLMs like Claude, GPT-4, or MistralIt’s the go-to for developers who want full AI capabilities without being locked into a closed ecosystem.* * *### 7. The Windsurf Editor by Codeium]( to Keep you in flow state** is revolutionizing the IDE concept by seamlessly integrating AI as a collaborator rather than just a tool. The platform offers a unique blend of AI copilots and autonomous agents that work together with developers to enhance productivity.The Cascade feature, which provides deep contextual awareness across your codebase, has helped me understand and navigate unfamiliar codebases much faster than traditional methods. The multi-file editing and predictive actions keep me in a constant flow state, which has noticeably improved my productivity.* * *### 8. Lazy AI]( app creation with AI-powered workflows** makes building applications almost effortless. Designed for speed and simplicity, it lets users create full-stack apps without writing code. Its standout features include:- Pre-built workflows for common developer and automation tasks- One-click deployment for both frontend and backend projects- Customizable templates for bots, AI tools, dev utilities, and marketing apps- Minimal skill requirement—ideal for rapid prototyping or solo makersIt’s the perfect fit for anyone looking to bring ideas to life without the overhead of traditional development.* * *### 9. Devika**Your open-source AI software engineer** is an open-source autonomous AI agent that takes high-level instructions and breaks them down into actionable development tasks. Designed to behave like a junior developer, it combines:- Advanced planning and reasoning algorithms- Contextual keyword extraction for precision- Dynamic agent tracking and decision-making- Web browsing to supplement gaps in knowledgeDevika stands out as a community-driven alternative to commercial AI coding platforms. It’s ideal for developers looking to delegate complex coding work while retaining full control and transparency over the process.* * *### 10. Cody by Sourcegraph]( code understanding with full repo context** is designed to:- Answer questions across massive codebases- Perform intelligent refactors based on architecture- Search, navigate, and explain with precisionIts integration with Sourcegraph gives it superpowers in monorepos, enterprise codebases, and multi-language projects.### AI Coding Tools in Practice: Use Case ScenariosTo better understand how these AI coding tools can be applied in real-world settings, here are a few practical scenarios that demonstrate their potential across different developer environments:#### **Solo Developer: Boosting Productivity with Cursor + Continue**Independent developers often juggle multiple roles—from writing code to debugging and managing small deployments. Tools like **Cursor** (an AI-native code editor) combined with **Continue** (a Copilot-like extension that integrates into IDEs) offer a significant productivity boost. Together, they streamline code suggestions, automate refactoring, and provide quick answers without leaving the editor, allowing solo devs to maintain momentum and ship faster.#### **Startup Team: Automating Ticket Resolutions with Sweep + Copilot Workspace**Early-stage startups typically operate with lean engineering teams that need to move fast. Tools like **Sweep**, which auto-generates pull requests from GitHub issues, and **Copilot Workspace**, which can create and manage entire features autonomously, allow small teams to maintain high output. These tools handle boilerplate tasks and bug fixes, freeing up developers to focus on high-impact product features.#### **Enterprise: Using Cody + Tabnine for Large, Secure Codebases**For enterprises with massive codebases and stricter security requirements, **Cody** (from Sourcegraph) and **Tabnine** offer scalable AI integration. Cody excels at understanding large repositories and referencing documentation or historical code changes. Tabnine, known for its privacy-first model, ensures that proprietary code doesn’t leave company boundaries, making it ideal for secure enterprise environments.#### **Data Scientist: Speeding Up EDA with Cogram + CodeSquire**Data scientists benefit greatly from tools that can reduce time spent on repetitive analysis. **Cogram** helps generate SQL queries and Jupyter code based on natural language, while **CodeSquire** personalizes code suggestions based on the user’s unique style and workflows. These tools accelerate exploratory data analysis (EDA) and modeling, letting scientists focus more on insights and less on syntax.* * *## Final Thoughts2025 isn’t about whether you use AI as a developer—it’s about how. The tools listed above aren’t just trendy—they’re reshaping the development experience by embedding intelligence into your daily workflow. Some extend your capabilities. Others remove tedious work. The best ones do both.Choose the ones that match your stack, integrate with your workflow, and make you feel like a better developer—not just a faster one.* * *Sentry]( PromotedDropdown menu- - * * *- ]( Read More!Templates let you quickly answer FAQs or store snippets for re-use.SubmitPreview CollapseExpand]( David]( David]( David![]( of Postiz, an open-source social media scheduling tool.Running Gitroom, the best place to learn how to grow open-source tools.- EducationDidn\\\\\\'t finish high school :(- PronounsNev/Nevo- WorkOSS Chief @ Gitroom- JoinedFeb 23, 2022• Dropdown menu- - Hide- Neat how coding is getting easier, like having a little helper doing chores for you. But if AI keeps taking on bigger parts of the job, what skills should people still work on to stay sharp in tech?CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Totally! As AI handles more, staying sharp in problem-solving, system design, and critical thinking is key. Knowing why things work still sets you apart.CollapseExpand]( Sečkár]( Sečkár]( Sečkár![]( am from Czechia, born in 1988. Studied computer science at KIT VŠE, Prague. Currently working as senior Java & JS developer for Eviden.- Email- LocationPrague, Czechia- EducationKIT VŠE, Prague- PronounsHe- WorkSenior application developer @ Eviden- JoinedOct 21, 2023• Dropdown menu- - Hide- business analysis, software architecture, observability, scalability, security...to name a fewCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- And to continue on the reply of , convert abstract requirements to good software requirements is still key, AI can only hallucinate solutions that most likely be wrong or wrongly translated.Software architecture and applying best practices is also still important, AI only generate some code which might not scale very well. Or doesn\\\\\\'t fit the overal design, or worse it\\\\\\'s using deprecated functionality. So a human developer should be really keen on it and not just accept all the code. How well it even might look like. It\\\\\\'s still just a tool.CollapseExpand]( Lopez]( LopezAudrey Lopez]( JoinedMar 6, 2025• Dropdown menu- - Hide- Nice post, Emmanuel Mumba!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Thanks Lopez! Glad you enjoyed it.CollapseExpand]( Sebastian]( SebastianRalph Sebastian]( JoinedAug 10, 2023• Dropdown menu- - Hide- Nice list. By the way, Windsurf has free GPT 4.1 access for now (it will not last long), worth trying out.CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Oh nice, thanks for the heads-up! I’ll definitely check Windsurf out before the window closes.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Openai is indeed delivering free gpt 4.1 while in preview.But also o4-mini models I believe. Mini-high, Etc.CollapseExpand]( Lee]( LeeJennie Lee]( JoinedDec 18, 2024• Dropdown menu- - Hide- OK, vibe coding is getting hot this time!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Haha, for real! 🔥 Coding’s definitely having its moment, More like the vibe is leveling up every week.CollapseExpand]( the Coder]( the CoderJosh the Coder]( Coder that is Interested in AI, No Code. Learning forever!- JoinedNov 27, 2023• Dropdown menu- - Hide- OK, this is a good list. I am using Cursor but also willing to explore some alternatives. Good work Emmanuel!CollapseExpand]( Mumba]( MumbaEmmanuel Mumba]( Emmanuel Mumba | Tech Innovator | SEO Specialist | Expert in web development, graphic design, and tech-driven innovations.- JoinedSep 19, 2024• Dropdown menu- - Hide- Appreciate that! 🙌 Cursor’s solid, always cool to explore what else is out there too. Let me know if you find any gems.CollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Windsurf is really good. Since they at least providea decent deb file setup, causing less issues on my linux setup than the stupid appimages from cursor.CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- I\\\\\\'ve spent more time trying to fix our juniors vibe coded problems than I can imagine. And what\\\\\\'s worse is having to explain why. They have no idea what the code they are pumping out does. It\\\\\\'s a horrifying trend. Hopefully this doesn\\\\\\'t spread to all industries and we end up with vibe built airplanes LOLCollapseExpand]( van den Berg]( van den BergMelroy van den Berg]( Engineer & DevOps Architect. Open-source lover. Blogger.- LocationNetherlands- JoinedJun 9, 2019• Dropdown menu- - Hide- Haha I was already afraid this will happen 2 years ago. And now we are at this stage where agents are commonly available and used in editors.It\\\\\\'s powerful for experienced devs. But juniors should really not just using it and blindly accept it. That is not how you grow in your knowledge or carrière. Instead of you fixing their issues, they should take full responsibility of the code they put out for review.CollapseExpand]( Orion]( OrionAshe Orion]( JoinedAug 12, 2024• Dropdown menu- - Hide- Okay so as a developer in the making, at what point in my learning process will these tools stop being a hindrance to my education and start being actual real usable tools and not like Nintendo 64 cheat codes because right now, having nothing but HTML CSS and some very intermediate vanilla JavaScript at my disposal (and only as it relates to the front-end. Getting at back end soon) even just the Codium Auto-complete in VS was detracting from my ability to build a solid foundation by typing through my full lines of code. I noticed that when I had auto complete enabled, pretty much immediately anytime I was working in other environments without an autocomplete buddy by my side my syntax absolutely suffered because my monkey brain wasn\\\\\\'t getting the muscle memory of typing out the proper syntax and to be honest it was a little embarrassing in Zoom meetings with my tutors to just sit there and fumble multiple times on things like spacing or back ticks or, dare I out myself, neglected line termination in ; and }However I definitely see the value of not having to type out very obvious return statements or whatever other repetitive this.thing = thething; -esque scenarios that when you add them all up really do burn giant holes your productive work hours. I\\\\\\'m just so terrified of using it too early, leaving the safety of being an \\\\\\'aspiring engineer\\\\\\' for the reality of the job market, finally landing an interview only to realize that while I thought I was learning to code I\\\\\\'d just been surfing the vibe.CollapseExpand]( Enlightened]( EnlightenedAaron Enlightened]( Developer using AI tools for programming but not depending on them completely!- JoinedApr 18, 2025• Dropdown menu- - Hide- LOL ... I know exactly how you feel! I am at a similar junction in my programming journey and this is what I do to not feel like an imposter later on - Open a plain .txt file and just type some HTML and CSS code (along with vanilla JavaScript if you want to) into it, then save the file as \"index.html\", and then open this file in Google Chrome browser and see the result. To modify or edit any code that you feel is giving unexpected results, just use the Chrome Developer tools. This will give you confidence in YOUR coding prowess with ZERO autocomplete using any type of AI. Do this for just 15-20 minutes every day for 1 month and you will have solved the imposter syndrome to a large extent.For all the rest of your tutorials use the autocomplete feature on VSCode Editor, but try to ask ChatGPT \"why\" was some particular code snippet suggested by the AI autocomplete. This will help you in understanding the WHY of AI coding and will help make you a better programmer.BTW, I signed up on DEV just to post this comment. So, thank you for that. ;-)CollapseExpand]( Lawson]( LawsonAsh Lawson]( JoinedApr 11, 2025• Dropdown menu- - Hide- It\\\\\\'s a tough gig for new devs, you are entering a world where tools exist that actually hinder your understanding of what\\\\\\'s going on.At the risk of showing my age, id say it\\\\\\'s akin to why the monk will make the initiate carry pales of water to then monastery before he learns the king fu. To generate an understanding of the fundamentals of movement, patience and discipline.Some people a lot older than me would say using an IDE is not true development and that real devs can code in notepad.Tools evolve and as long as they fill the purpose of making coding easier, but still allow you to learn and gain the knowledge and understanding I wouldn\\\\\\'t see a problem with using them. But as you pointed out, it can be a hindrance, and you actually realizing that and accepting it means you will succeed where many of your peers won\\\\\\'t.AI is just another tool, it\\\\\\'s great when you already have an understanding of how systems should work and just want some snippets. But the problem we are seeing is a misuse of these tools. The blind copy paste vibe coding is not creating developers it\\\\\\'s creating slop.Here\\\\\\'s my ultimate advice though, AI doesn\\\\\\'t have to be just a tool, it can be an amazing teacher. It can explain things to you in anyway you want , if you ask it to simplify and breakdown topics it will. It can explain things to you as if you were a toddler. USE THIS.Vibe code your heart out but every single time it gives you code you do not understand, ask it to explain it you. You will be amazed how quickly it goes from hindrance to game changing.CollapseExpand]( jones]( jonessam jones]( JoinedApr 16, 2025• Dropdown menu- - Hide- Very informative article. Although not at all what I expected from title \"Vibe Coding\". In industry, Vibe Coding is being used to signify low code/no code or citizen development. Basically, the same holy grail that the industry has been searching for since Oracle Forms or earlier.BUt putting the Title aside, looking forward to trying out some of your suggestions.CollapseExpand]( Mottley]( MottleyDaley Mottley]( Consultant | Full Stack Web Developer- Email- LocationBarbados- JoinedOct 8, 2024• Dropdown menu- - Hide- Nice post, Emmanuel!Have you tried ?It\\\\\\'s a pretty decent vibe coding tool. I love it... 😅Thanks for sharing 👍CollapseExpand]( Green]( GreenStuart Green]( JoinedApr 18, 2025• Dropdown menu- - Hide- Crazy times ahead. Can only imagine the downstream impact these tools will have (personalized software, rise of individual creators, etc).I also love platforms like  and  for \"vibe coding\" without the actual code. Though harder to build more advanced features than with Cursor, etc.Some comments may only be visible to logged-in visitors.  to view all comments.Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\\\\\'s .Hide child comments as wellConfirmFor further actions, you may consider blocking this person and/or Embeddable]( PromotedDropdown menu- - * * *- ]( Embed in minutes, load in milliseconds, extend infinitely. Import any chart, connect to any database, embed anywhere. Scale elegantly, monitor effortlessly, CI/CD & version control.👋 Kindness is contagiousDropdown menu- - * * *- CloseDive into this thoughtful piece, beloved in the supportive DEV Community. **Coders of every background** are invited to share and elevate our collective know-how.A sincere \"thank you\" can brighten someone\\\\\\'s day—leave your appreciation below!On DEV, **sharing knowledge smooths our journey** and tightens our community bonds. Enjoyed this? A quick thank you to the author is hugely appreciated.### !We\\\\\\'re a place where coders share, stay up-to-date and grow their careers. ![](\\'}]', name='web_search_tool', call_id='call_gktlSI1Wl1Ht2VVGQXzmTvuU', is_error=False), FunctionExecutionResult(content='Unexpected error during search: Status code 429. Rate limit exceeded. Consumed (req/min): 8, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 24s, resets at Mon Sep 08 2025 12:33:48 GMT+0000 (Coordinated Universal Time) - No additional error details provided.', name='web_search_tool', call_id='call_S9VoFTZt8NefRPNRbUH4tZQg', is_error=True)]), TextMessage(id='a8564d30-d359-40aa-b823-4ae0ac09fbb7', source='research_analyst', models_usage=RequestUsage(prompt_tokens=58513, completion_tokens=4612), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 34, 5, 594104, tzinfo=datetime.timezone.utc), content='보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 요약\\n\\nExecutive Summary\\n- 바이브 코딩(감정/환경 인식 기반 코딩 및 인터랙티브 코딩)은 AI 보조 코딩의 확장된 개념으로 주목받고 있으며, 개발자와 비개발자 간의 협업 방식과 개발 생산성에 새로운 기회를 열고 있습니다.\\n- 현재 주요 트렌드는 AI 코드 생성 도구의 급속한 확산, 코드 작성 보조의 고도화, IDE/에디터의 AI 내재화, 그리고 AI 에이전트의 활용 확대입니다. 다국적 대기업-스타트업 간 경쟁 구도가 심화되고 있습니다.\\n- 개발자의 미래에 대해선 “대체”보다 “보조-확대”의 방향이 주류이며, 개발자의 역할은 시스템 설계, 아키텍처, 보안, 복합 문제 해결 등 고부가 가치 영역으로 재편될 가능성이 큽니다. 다만 초입력 개발자(엔트리 레벨) 일자리의 변화 가능성과 훈련/교육의 적응이 중요한 도전 과제로 남습니다.\\n- 향후 교육·기업 전략 측면에서 감정인식/Affective Computing, 윤리 및 데이터 보안, 문화적 맥락의 고려 등 새로운 연구 주제가 핵심 이슈로 부상할 전망입니다.\\n\\nBackground & Current State\\n- 바이브 코딩의 정의 및 현황\\n  - 바이브 코딩은 자연어/감정 기반의 코드 작성 지원 및 대화형 인터페이스를 통해 프로그래밍을 보조하는 개념으로 확장되고 있습니다. Databricks의 레이크베이스(Lakebase) 출시 및 Agent Bricks/AI 판사 같은 기능들이 데이터-AI 워크플로우를 하나의 플랫폼에서 통합하려는 시도를 보여줍니다.\\n  - 바이브 코딩은 데이터+AI 생태계의 통합, 비즈니스 프로세스 자동화, 그리고 AI 코드 생성 도구의 통합으로 기업 생산성 증대에 기여합니다.\\n- 최근 개발 동향 및 주요 데이터\\n  - 글로벌 개발자 커뮤니티에서 AI 도구의 채택이 빠르게 확산되고 있으며, Stack Overflow의 2025 개발자 서베이 결과에 따르면 84%의 응답자가 AI 도구를 사용 중이거나 향후 사용할 계획이며, 일부 규모의 개발자는 매일 AI 도구를 사용합니다(일부 직군에서 50% 이상이 일상적으로 사용).\\n  - 다수의 기업/스타트업이 AI 코드 생성 도구에 대한 투자를 확대하고 있으며, Cursor, Windsurf, Codeium 등의 스타트업 기업가치가 수십억 달러 규모로 평가되는 사례가 있습니다.\\n  - 엔트리 레벨 개발자 채용시장에선 AI 도입으로 반복 작업이 감소하는 등 일자리 구조의 변화가 나타나고 있으며, 빅테크가 자체 AI 코딩 도구를 확산시키고 있습니다.\\n- 관련 데이터 및 현장 인사이트(발췌)\\n  - 2025 Stack Overflow Developer Survey: AI 도구의 사용 증가, 일상 사용 비중 상승, 도구의 정확성 및 보안에 대한 우려 증가, 복잡한 작업에서의 신뢰성 이슈 지적.\\n  - 데이터코어 및 스타트업 생태계 소식: Cursor(가치평가 약 9B 달러), Windsurf/Codeium(가치평가 약 3B 달러) 등, AI 코드 생성 도구의 상용화 및 투자가 활발.\\n  - 감정 인식/ affective computing 연구의 확산: 교육 현장, 산업 적용, 데이터셋(DevEmo, DEAP 등) 및 국제 표준(IS/IEC 30150-1 등) 이슈가 제시됨.\\n- 한계 및 주의점\\n  - AI 도구의 정확도, 보안, 데이터 프라이버시 이슈는 여전히 큰 걱정거리이며, 인간의 검증이 여전히 필요합니다.\\n  - 바이브 코딩의 문화적/사회적 맥락에서의 차이를 고려한 접근이 필요합니다. 특정 문화권에서의 감정 표현 차이가 시스템 설계에 영향을 줄 수 있습니다.\\n\\nAnalysis & Insights\\n- 주요 트렌드\\n  - AI-보조 코딩에서 AI-주도 코딩으로의 이동이 진행 중이나, 인간 개발자의 역할은 여전히 필요합니다. 대규모 시스템 설계, 보안, 아키텍처 의사결정, 도메인 지식의 적용 등은 인간의 영역으로 남습니다.\\n  - 에이전트 기반 협업의 확산: 에이전트 브릭스/AI 판사 등 기능이 도입되며, 대화형 명령으로 정보 추출, 자동 코드 작성, 품질 관리가 가능해집니다. 다수의 도구가 IDE 내 협업/배포/모니터링까지 연결됩니다.\\n  - 감정인식 기반의 개발환경: 개발 학습/실무 환경에서 감정 데이터를 활용한 적응형 학습, 몰입도 향상, 학습 효과 개선에 대한 연구가 축적되고 있습니다. 다만 프라이버시 및 문화적 차이에 대한 규범이 필요합니다.\\n- 다양한 관점\\n  - 기술-생태계 관점: 대형 플랫폼(예: Databricks)와 스타트업 간의 경쟁+협력 구조가 형성되어 있으며, Lakebase 같은 플랫폼은 데이터+AI 워크플로우의 통합을 촉진합니다.\\n  - 산업-경제 관점: AI 코드 생성 도구에 대한 투자 증가 및 스타트업의 고밸류에이션 현상은 시장의 성장 잠재력을 시사합니다. 엔트리레벨 개발자 일자리가 변동될 가능성과 대규모 기업의 자동화 도입은 고용시장에 직접적인 영향을 줄 수 있습니다.\\n  - 윤리-정책 관점: 데이터 프라이버시, 보안, 문화적 맥락의 반영, 표준화의 필요성 등은 향후 연구/정책의 핵심 이슈가 됩니다.\\n- 전문가 의견 및 현장 견해\\n  - 전문가들은 “AI는 개발자의 보조 도구”로 자리매김하며, 인간은 설계, 아키텍처, 보안, 도메인 전문성 측면에서 주도적 역할을 유지해야 한다는 입장을 공통적으로 제시합니다.\\n  -emotion-aware/감정 인식 연구는 교육 및 협업 도구의 개선에 기여할 수 있으나, 구현의 복잡성과 윤리적 쟁점으로 인해 도입에 신중해야 한다는 시각이 많습니다.\\n  - AI 에이전트의 채택은 생산성 증가 및 학습 가속에 긍정적이지만, 협업의 질이나 팀 내 커뮤니케이션에 미치는 영향에 대한 재평가가 필요합니다.\\n- 도전 요소\\n  - 신뢰성 문제: \"Almost right\" 한 AI 결과를 디버깅하는 데 소요되는 시간과 노력이 여전히 큽니다.\\n  - 데이터 관리 및 보안: 에이전트 기억(memory) 관리, 데이터 접근, 로그 보안 등은 엔터프라이즈 환경에서 핵심 이슈입니다.\\n  - 문화/윤리: 서로 다른 문화적 맥락에서의 감정 표현 차이에 맞춘 UX/대화 전략 필요.\\n  - 교육/재훈련: 개발자의 기술적 재교육 필요성 증가. 특히 고급 아키텍처 설계/보안/데브옵스 관점의 역량 강화가 중요합니다.\\n\\nFuture Outlook\\n- Emerging trends\\n  - 에이전트-주도적 개발: IDE 내 에이전트가 코드 생성, 버그 수정, 테스트 작성까지 자동화하는 흐름이 확산될 가능성.\\n  - 멀티모달 감정 인식의 확장: 텍스트/음성/비주얼/생리 신호를 결합한 감정 인식으로 개발 환경의 적응성과 협업 품질이 개선될 전망.\\n  - 문화적 맥락 반영: 글로벌 확산 속에서 지역별 문화 차이를 고려한 감정 모델과 인터페이스 표준화 필요성이 증가.\\n  - 교육-기업 연결 강화: affective computing/감정인식 기반 학습 도구를 활용한 실무형 교육 커리큘럼 확산.\\n- Predictions\\n  - 2030년대 초반까지 반복적이고 보일러플레이트 수준의 코딩은 AI 도구가 상당 부분 대체하거나 보조하는 수준에 이를 가능성 큼.\\n  - 고급 개발자 역할은 시스템 설계, 보안, 아키텍처 의사결정, 도메인 지식의 적용에 더 집중될 것이고, 개발 생태계의 협업 도구는 더 통합적이고 자동화된 흐름을 제공할 것.\\n  - 교육 체계는 AI 도구 사용법뿐 아니라 윤리, 보안, 문화적 맥락 이해를 포함한 AI-리터러시를 강화하는 방향으로 재편될 가능성.\\n- Implications\\n  - 기업: 데이터/AI 워크플로우의 통합 투자 확대, 에이전트-기반 협업 도구 도입, 보안·규정 준수 체계 강화 필요.\\n  - 교육: 감정 인식·사회적 상호작용의 이해를 포함한 교육과정 설계, 재교육/직무전환 프로그램 강화.\\n  - 정책/윤리: 개인정보보호, AI 투명성, 책임소재, 데이터의 편향성 문제를 다루는 국제 표준 및 기업 내부 정책의 필요성 증가.\\n\\nRecommendations (실무 및 정책 제안)\\n- 기업 전략\\n  - AI 도구 채택 로드맷 수립: 어떤 코드 작성 단계에서 어떤 도구를 언제 사용할지 명시하고, 인간 검증 프로세스를 병행.\\n  - 에이전트 거버넌스 구축: 에이전트의 메모리/데이터 접근 제어, 감사 로그, 보안 규정 준수 확보.\\n  - 문화/윤리 가이드라인: 감정 인식 기반 UX 설계 시 문화 간 차이를 고려한 정책 수립.\\n- 교육-인재 전략\\n  - AI 도구 활용 역량 강화 교육: 프레이밍된 과제에서의 도구 사용법, 리뷰 및 디버깅 전략, 보안/윤리 의식 교육 포함.\\n  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계, 보안, 성능 최적화에 초점을 맞춘 훈련 강화.\\n  - 감정 인식 교육의 도입: 교육 현장에서의 감정 데이터 활용 윤리와 데이터 관리 교육 포함.\\n- 연구 방향\\n  - 감정 인식과 코드 품질 간의 관계 연구: 감정 상태가 개발 생산성과 코드 품질에 미치는 영향 분석.\\n  - 다문화/다언어 환경에서의 감정Affective 모델 비교 연구.\\n  - 에이전트-협업의 효과성 분석: 팀 성과, 학습효과, 협업 품질에 대한 실증 연구.\\n\\nSources\\n- 데이터/동향 및 기술 현황\\n  - Stack Overflow Developer Survey 2025: AI 도구 사용 현황, 일상 사용 비율, 업무 신뢰도 및 복잡도 처리에 대한 인식\\n  URL 예시: https://survey.stackoverflow.co/2025/ai (발췌 요약)\\n- 바이브 코딩 및 스타트업 생태계\\n  - 2025년 바이브 코딩/AI 개발 동향 보고서(예시 기사/리포트): Databricks의 바이브 코딩 개념, Lakebase 출시 및 에이전트 도구\\n  - Cursor, Windsurf, Codeium 등 스타트업 밸류에이션 및 투자현황\\n  - 예시 기사/리포트: TokenPost, Nate News, Unite.AI, Dev.to 등(요지 요약)\\n- AI 코드 생성 도구 및 비교\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 활용 사례\\n  - Repl.it/에이전트 협업 기능, 에이전트-브릭스/AI 판사 등 기능 설명\\n- 감정 인식/ affective computing 연구\\n  - Real-time emotion aware applications: 사례 연구 및 GPU 병렬처리 가속\\n  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing/Computing)\\n  - DevEmo 데이터셋, AffecTube 등 데이터셋 및 교육 연구\\n- 관련 정책 및 표준\\n  - ISO/IEC 30150-1:2022 (Affective computing user interface)\\n  - 감정 인식 기술의 윤리·데이터 프라이버시 이슈 관련 논의\\n- AI와 개발자 직무의 미래에 관한 분석/리뷰\\n  - AI가 개발자 직무에 미치는 영향에 대한 종합 리뷰 및 2025년 트렌드 요약\\n  - AI 에이전트의 생산성 영향 및 협업 효과에 관한 설문/연구\\n- 참고 기사 예시(한국어/영문)\\n  - 2025 바이브 코딩 관련 기사 모음(예: TokenPost, 네이트 뉴스)\\n  - 2025 AI 코드 생성 도구 비교 기사(EliteBrains, Dev.to, Azumo 등)\\n\\n마무리\\n- 본 보고서는 바이브 코딩의 개념적 확장과 AI 코드 생성 도구의 확산이 소프트웨어 개발자의 미래에 미치는 영향을 다각적으로 조망했습니다. 주요 시사점은 기술의 보조적 역할 강화, 인간 고유의 설계/보안/도메인 전문성의 중요성 유지, 그리고 감정/문화적 맥락을 반영한 윤리적이고 책임 있는 도입이 필요하다는 점입니다.\\n- 필요하신 경우, 위의 핵심 주제(2–3개)와 각 주제에 대한 구체적 하위 주제, 그리고 3–5개의 최신 자료 목록으로 구성된 맞춤형 FOCUSED 연구 계획서를 추가로 작성해 드리겠습니다.\\n\\nREPORT_COMPLETE', type='TextMessage'), TextMessage(id='1052795b-6870-43de-b8a6-6cca5d82e558', source='research_analyst', models_usage=RequestUsage(prompt_tokens=61650, completion_tokens=4395), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 34, 43, 798910, tzinfo=datetime.timezone.utc), content='제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\\n\\nExecutive Summary\\n- 정의와 맥락: 바이브 코딩은 분위기/감정 인식 기반의 코딩 방식과 인터랙티브한 개발 환경을 포괄하는 확장 개념으로 이해되며, AI 기반 코드 작성 보조와 데이터-AI 플랫폼의 통합 트렌드를 아우릅니다.\\n- 현황 요약: AI 보조 코딩 도구의 채택이 급격히 확산 중이며, 대기업-스타트업 간의 생태계 경쟁이 심화하고 있습니다. 커뮤니티 기반의 협업 도구, 에이전트 기능, 그리고 플랫폼 차원의 데이터-AI 워크플로우 통합이 두드러집니다.\\n- 소프트웨어 개발자의 미래: AI 보조 도구는 개발자 업무의 생산성과 속도를 높이되, 인간 개발자의 핵심 역할은 시스템 설계, 아키텍처, 보안, 도메인 전문성 등 고부가 가치 영역으로 재편될 가능성이 큽니다. 특히 초보 개발자 진입 및 직무전환 측면에서 교육 체계의 신속한 적응이 중요한 과제로 남습니다.\\n- 시사점: 교육, 기업 거버넌스, 윤리/보안 표준의 정교화가 필요하고, 문화적 맥락을 반영한 감정 기반 UX의 도입과 데이터 프라이버시 관리가 중요합니다.\\n\\nBackground & Current State\\n- 현재 개발 풍경\\n  - 바이브 코딩은 감정/상황 인식 기반의 코드 생성과 업무 자동화를 포함하는 포괄적 모형으로 진화 중이며, 데이터 생태계의 통합과 AI 자동화 도구의 결합이 중심 축입니다. 예를 들어 Databricks의 레이크베이스(Lakebase)와 관련 생태계, 에이전트 빌딩(Agent Bricks) 및 AI 판사(judges) 같은 기능이 데이터-AI 워크플로우의 혁신적 연결고리로 주목됩니다.\\n  - 스타트업 생태계: Cursor, Windsurf, Codeium 같은 AI 코딩 도구 관련 스타트업의 가치 평가가 수십억 달러 수준에 이르는 사례가 늘고 있습니다. 이는 AI 코드 생성 시장의 성장성과 투자 열기를 시사합니다.\\n- 최근 developments 및 데이터 포인트\\n  - 대규모 개발자 커뮤니티의 채택 증가: Stack Overflow의 2025 개발자 서베이에서 AI 도구를 사용하는/사용 계획이 80%대에 이르는 부문이 다수였고, 일부 개발자는 매일 AI 도구를 사용합니다.\\n  - AI 도구에 대한 인식과 신뢰도: 전문 개발자층에서 AI 도구의 긍정적 효과에 대한 인식은 높지만, 도구의 정확성·보안·프라이버시 이슈에 대한 우려도 여전히 큽니다.\\n  - 엔트리 레벨 개발자 고용환경: AI 도입으로 반복적·보일러플레이트 작업이 감소하는 한편, 대형 IT 기업들은 자체 AI 코드 생성 솔루션을 확산시키고 있습니다.\\n- 주요 통계 및 데이터 포인트(요약)\\n  - Stack Overflow 개발자 설문 2025: 약 84%가 AI 도구를 사용 중이거나 곧 사용할 계획. profesionals의 경우 일부에서 매일 사용 비중이 높음.\\n  - AI 도구에 대한 신뢰도: 고급 숙련자일수록 도구의 신뢰도에 대해 신중하며, 완전한 자동화를 기대하기에는 추가 검증이 필요하다는 입장 증가.\\n  - 에이전트 활용: 전체 응답의 상당수가 에이전트를 작업에 도입 중이거나 도입 계획을 가지고 있으나, 에이전트 기억(memory) 관리, 보안, 관측 가능성 등의 이슈가 남아 있습니다.\\n  - 바이브 코딩의 실무성 인식: 다수 응답자가 바이브 코딩을 “주류 작업의 일부”로 보지 않고, 아직은 도입 초기 또는 비주류 흐름으로 간주하는 경향이 높습니다.\\n- 용어 정리\\n  - 바이브 코딩: 분위기/감정 인식 기반의 코딩 또는 Chat/자연어 명령을 바탕으로 한 대화형 개발 도구를 포함하는 개념.\\n  - Lakebase: 데이터+AI 워크플로우를 한 플랫폼에서 처리하도록 설계된 서버리스 OLTP 데이터베이스.\\n  - 에이전트 빅스(Agent Bricks) 및 AI 판사: 대화형 명령으로 AI 에이전트를 생성/관리하고, 생성된 결과의 품질을 모니터링하는 도구.\\n  - 감정 인식(Affective Computing) 연구: 교육, 인터랙션 디자인, 멀티모달 감정 인식 등을 중심으로 한 연구 분야.\\n\\nAnalysis & Insights\\n- 주요 트렌드\\n  - AI-보조 코딩의 강화: 코드 작성의 보조에서 벗어나 다층적 작업(요구 분석, 코드 생성, 테스트, 문서화)을 자동화하는 방향으로 도구가 발전하고 있습니다.\\n  - 에이전트-기반 협업의 확산: IDE 내에서 에이전트가 실시간으로 협업하고, 코드 품질 관리까지 참여하는 사례가 늘고 있습니다.\\n  - 데이터 플랫폼의 통합: 데이터 관리와 AI 개발이 하나의 워크플로우로 결합되는 사례가 증가하고 있으며, Lakebase와 같은 플랫폼의 채택이 확산될 것으로 보입니다.\\n  - 감정/문화적 맥락의 고려 필요성: 감정 인식의 UX 설계에서 문화적 차이를 반영하고, 프라이버시/윤리 이슈를 관리하는 것이 핵심 이슈로 부상합니다.\\n- 다양한 관점\\n  - 기술 관점: 개발 생산성 향상, 코드 품질 보장, 대규모 코드베이스의 자동화된 관리 등 긍정적 효과가 예상됩니다. 다만 복잡한 문제 해결과 설계 의사결정은 여전히 인간 주도 영역입니다.\\n  - 교육 관점: AI 도구의 활용은 학습 속도와 접근성을 높이지만, 기초 프로그래밍 원리의 이해를 저해하지 않도록 교육 설계가 필요합니다.\\n  - 사회/정책 관점: 프라이버시 보호, 데이터 소유권, 알고리즘의 편향성 관리, 국제 표준 정립이 중요합니다.\\n- 전문가 의견 및 현장 견해\\n  - 전문가 다수는 “AI는 개발자의 보조 도구”로 보며, 시스템 설계/아키텍처, 보안, 도메인 지식의 융합 능력이 앞으로의 핵심 역량이라고 봅니다.\\n  - 감정 인식 분야 연구자들은 교육 현장과 산업 적용에서의 잠재력은 크지만, 구현의 복잡성과 윤리적 문제에 대한 신중한 접근이 필요하다고 지적합니다.\\n  - 스타트업 및 대기업 간의 협력/경쟁 구도가 강화되며, 데이터/AI 생태계의 통합에 대한 투자와 전략적 파트너십이 중요하다고 봅니다.\\n- 도전 과제\\n  - 정확도와 신뢰성: AI가 생성한 코드의 품질에 대한 인간의 검증 필요성은 여전합니다.\\n  - 보안/프라이버시: 에이전트 기억(memory) 관리와 데이터 접근 제어, 감사 가능성 확보가 필수입니다.\\n  - 문화적 차이: 글로벌 채택에서 문화적 맥락 차이가 UX 설계에 반영되어야 합니다.\\n  - 교육 체계의 적응: 재교육/직무전환 프로그램의 설계와 실행이 중요합니다.\\n\\nFuture Outlook\\n- Emerging trends\\n  - AI 에이전트 주도 개발의 확대: IDE 내에서 에이전트가 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 방향으로 발전 가능.\\n  - 멀티모달 감정 인식의 확장: 텍스트, 음성, 얼굴 표정, 생리 신호 등을 결합한 감정 인식이 개발 환경의 적응성 및 협업 품질을 향상시킬 전망.\\n  - 문화적 맥락의 반영: 지역/문화별 감정 표현 차이를 반영한 인터페이스 및 모델 표준화 필요성이 커짐.\\n  - 교육-산업 연계 강화: affective computing/감정인식 기반 학습 도구가 현장 직무 교육에 통합될 가능성 증가.\\n- Predictions\\n  - 향후 5–10년간 반복적이고 보일러플레이트 수준의 코딩은 AI 도구가 더욱 많이 대체하거나 보조하는 수준으로 발전할 가능성 큼.\\n  - 개발자의 역할은 고급 설계, 아키텍처 의사결정, 보안/컴플라이언스, 도메인 전문성 중심으로 재편될 가능성.\\n  - 교육 체계는 AI 도구의 사용법뿐 아니라 윤리/보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 재구성될 필요가 증가.\\n- Implications\\n  - 기업: 데이터/AI 워크플로우의 통합 확대, 에이전트 관리 거버넌스 구축, 보안·윤리 정책 강화 필요.\\n  - 교육: 감정 인식 기반 교육 도입 및 재교육 프로그램 설계, 시스템적 사고/아키텍처 역량 강화 필요.\\n  - 정책/표준: 데이터 프라이버시, 책임소재, 국제 표준 정립 등 정책적 규범의 정교화가 중요해질 것.\\n\\nRecommendations (실무-교육-정책 측면 제안)\\n- 기업 전략\\n  - AI 도구 채택 로드맵 수립: 어느 단계에서 어떤 도구를 도입하고, 언제 인간의 검증을 필수로 할지 명확한 프로세스 수립.\\n  - 에이전트 거버넌스 및 데이터 보안: 메모리 관리, 접근 제어, 감사 로그, 보안 표준 준수 체계 확립.\\n  - 문화/윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영한 정책 및 교육.\\n- 교육-인재 전략\\n  - AI 도구 활용 역량 중심 교육: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함한 교육과정 설계.\\n  - 시스템적 사고 강화: 아키텍처 설계, 성능 최적화, 보안 등 고급 역량 강화 교육 강화.\\n  - 감정 인식 연구·교육의 도입: 데이터 관리와 윤리 교육을 함께 다루는 커리큘럼 개발.\\n- 연구 방향\\n  - 감정 상태와 코드 품질/생산성의 관계 실증 연구.\\n  - 다문화/다언어 맥락에서의 감정 모델 비교 연구.\\n  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구.\\n\\nSources\\n- Stack Overflow Developer Survey 2025: AI 도구 사용 현황, 일상 사용 비율, 신뢰도/복잡도 처리 인식 등\\n  - 예시: https://survey.stackoverflow.co/2025/ai\\n- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\\n  - 데이터/동향: Databricks의 바이브 코딩 컨셉, Lakebase 출시 등의 보도\\n  - 스타트업 사례 및 밸류에이션: Cursor, Windsurf, Codeium 등\\n  - 기사 예시: TokenPost, Nate News, Unite.AI, Dev.to 등\\n- AI 코드 생성 도구 비교 및 활용 사례\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter, Cursor, Windsurf 등\\n  - Replit의 에이전트 기능, 협업 기능 등\\n- 감정 인식/Affective Computing 연구 및 데이터셋\\n  - Real-time emotion aware applications(연구 사례), Towards Emotion-awareness in Programming Education(IEEE Xplore)\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends( Intell Comput, 2024)\\n  - DevEmo, AffecTube, DEAP, SEED 등 데이터셋/연구\\n- 표준 및 정책\\n  - ISO/IEC 30150-1:2022, Affective computing user interface\\n  - 감정 인식 기술의 윤리 및 데이터 프라이버시 논의\\n- 미래 직무 및 업계 분석 리뷰\\n  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈에 관한 최신 리뷰 및 설문\\n  - AI가 직무에 미치는 영향에 관한 종합 연구 및 현장 인터뷰\\n\\nREPORT_COMPLETE', type='TextMessage'), TextMessage(id='0285359e-d6cd-4e0c-939c-86db44097832', source='research_analyst', models_usage=RequestUsage(prompt_tokens=64504, completion_tokens=4117), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 35, 14, 398915, tzinfo=datetime.timezone.utc), content='보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\\n\\nExecutive Summary\\n- 정의 확장: 바이브 코딩은 분위기/감정 인식 기반의 코딩 및 인터랙티브한 개발 환경을 포괄하는 개념으로, AI 보조 코딩의 진화와 데이터-AI 워크플로우의 통합 흐름을 함께 다룬다.\\n- 현황 요약: AI 코드 생성 도구의 급속한 확산, IDE/에디터의 AI 내재화, 에이전트 기반 협업 기능의 확대가 두드러진 현상이다. Databricks의 레이크베이스 같은 플랫폼 출시와 스타트업(Cursor, Windsurf, Codeium 등)의 밸류에이션 상승이 시장의 활황을 시사한다.\\n- 소프트웨어 개발자의 미래: 개발자의 역할은 “보조 도구의 활용과 고부가 가치 영역의 설계/아키텍처/보안”으로 재편될 가능성이 크다. 엔트리 레벨 개발자 취업 환경은 AI 도입으로 변화하고 있으며, 재교육/직무 전환이 중요한 과제다.\\n- 시사점: 교육 체계의 현대화, 데이터 프라이버시 및 보안 거버넌스 강화, 문화적 맥락을 반영한 UX/윤리 표준 확립이 필요하다.\\n\\nBackground & Current State\\n- 현장 landscape\\n  - 바이브 코딩은 자연어 기반 코드 생성과 업무 자동화를 넘어, 데이터-플랫폼 서비스와 AI 자동화 도구를 하나의 생태계로 연결하려는 방향으로 발전하고 있다.\\n  - Lakebase(데이터브릭스의 서버리스 OLTP 데이터베이스)와 Agent Bricks, AI 판사(Judges) 등의 기능이 데이터-AI 워크플로우의 혁신적 통합을 촉진한다.\\n- 최근 developments\\n  - 대기업-스타트업 생태계의 경쟁 구도 강화: Cursor, Windsurf, Codeium 등 AI 코드 생성 스타트업의 밸류에이션 상승 및 투자 증가.\\n  - 플랫폼 중심의 협업 도구 강화: Replit, Cursor, Windsurf 등의 협업/에이전트 기능으로 팀 간 개발 흐름이 더욱 연결되고 있다.\\n  - 졸업/학계 트렌드: 감정 인식(Affective Computing) 연구 및 교육 현장의 활용 사례 증가, DevEmo, DEAP 등 데이터셋과 국제 표준(IS/IEC 30150-1:2022) 이슈가 주목받고 있다.\\n- 주요 통계 및 데이터\\n  - Stack Overflow Developer Survey 2025: AI 도구를 사용하거나 사용할 계획인 개발자 비중이 높음(대부분의 직군에서 80%대에 이르는 추세). 일부 응답자는 매일 AI 도구를 사용.\\n  - AI 코드 생성 도구 관련 시장 동향: Copilot, CodeWhisperer, Cursor, Windsurf 등의 도구가 실무에 널리 도입되며, 스타트업 투자 및 밸류에이션이 상승하고 있음.\\n  - 엔트리 레벨 개발자 채용 변화: AI 도입으로 반복적 작업이 축소되면서 신규 채용 흐름에 변화가 나타남.\\n- 데이터의 한계\\n  - AI 도구의 정확도, 보안성, 데이터 프라이버시 이슈는 여전히 중요한 제약 요인이다. 윤리적·문화적 맥락 반영에 관한 연구와 정책 정비가 필요하다.\\n\\nAnalysis & Insights\\n- 주요 트렌드\\n  - AI 보조의 고도화: 코드 작성 보조를 넘어 요구 분석, 테스트 작성, 문서화까지 포괄하는 자동화 수준으로 확장 중이다.\\n  - 에이전트 기반 협업 확산: 에이전트 빅스, AI 판사 등의 도구가 코드 품질 관리 및 멀티에이전트 협업을 촉진한다.\\n  - 데이터-플랫폼의 통합: Lakebase 등의 플랫폼이 데이터 관리와 AI 개발 워크플로우를 하나의 환경에서 처리하도록 진화한다.\\n  - 감정 인식의 적용 확대: 교육 및 협업 도구의 학습효과 및 몰입도 개선 가능성이 있다. 다만 프라이버시와 문화적 차이에 대한 주의가 필요하다.\\n- 다양한 관점\\n  - 기술/기업 관점: 생산성 향상과 대규모 코드베이스의 관리 효율성 증가가 기대되나, 보안·규정 준수의 강화가 필수다.\\n  - 교육 관점: AI 도구의 활용 능력과 함께 기초 원리 이해, 보안/윤리 의식 강화가 필요하다.\\n  - 사회/정책 관점: 개인정보 보호, 알고리즘 편향성 관리, 국제 표준 정립 등 정책적 이슈가 부상한다.\\n- 전문가 의견 요약\\n  - “AI는 개발자의 보조 도구”라는 공통된 시각이 지배적이며, 시스템 설계, 아키텍처, 보안, 도메인 전문성 같은 고부가 가치 영역으로 인간의 역할이 재정의될 가능성이 크다.\\n  - 감정 인식 연구는 교육과 인터랙티브 도구 개선에 기여하나, 구현의 복잡성과 윤리 문제에 대한 신중한 접근이 필요하다는 견해가 많다.\\n  - 스타트업-대기업 간의 협력 및 경쟁은 기술 생태계의 건강성과 혁신 속도를 좌우한다.\\n- 도전 과제\\n  - AI의 불완전성 및 디버깅 부담: “Almost right” 문제가 여전히 흔하며 디버깅 시간이 길어질 수 있다.\\n  - 데이터 관리 및 보안: 에이전트의 기억(memory) 관리, 로그 기록, 접근 제어 등 거버넌스가 중요하다.\\n  - 문화적 맥락의 반영 필요성: 문화적 차이에 따른 감정 표현 모델의 차이가 UX에 큰 영향을 준다.\\n  - 교육 재구성 필요성: AI 도구의 도입과 함께 기초 원리/아키텍처 역량의 체계적 재교육이 필요하다.\\n\\nFuture Outlook\\n- Emerging trends\\n  - AI 에이전트 주도 개발의 확산: IDE 내에서의 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 흐름이 확대될 전망.\\n  - 멀티모달 감정 인식의 강화: 텍스트, 음성, 표정, 생리 신호를 결합한 감정 인식으로 개발 환경의 적응성 및 협업 품질이 향상될 가능성.\\n  - 문화적 맥락 반영과 표준화: 다문화 환경에서의 감정 모델 차이를 고려한 UX 설계 및 국제 표준 정립이 중요한 의제가 됨.\\n  - 교육-산업 연계 강화: affective computing 기반 학습 도구의 현장 직무 학습 도구로의 확산.\\n- Predictions\\n  - 5–10년 내 반복적이고 boilerplate 수준의 코딩은 AI 도구에 의해 상당 부분 대체 혹은 보조될 가능성 큼.\\n  - 개발자의 역할은 고급 설계/아키텍처/보안/도메인 전문성 중심으로 재편될 가능성 높음.\\n  - 교육 체계는 AI 도구 활용법 외에도 윤리, 보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 진화할 가능성.\\n- Implications\\n  - 기업: 데이터/AI 워크플로우의 통합 투자, 에이전트 거버넌스 강화, 프라이버시 및 보안 정책의 정교화 필요.\\n  - 교육: AI 활용 역량과 함께 시스템적 사고, 아키텍처 설계 역량 강화 필요.\\n  - 정책/표준: 데이터 보호, 책임 소재, 공정성/투명성 등의 원칙 정립이 중요해질 것.\\n\\nRecommendations\\n- 실무/기업\\n  - AI 도구 채택 로드맵 수립: 도구 도입 시점별 목표와 인간 검증 포인트를 명확히 정의.\\n  - 거버넌스 체계 강화: 메모리 관리, 접근 제어, 감사 로그, 보안 규정 준수 등을 포함한 에이전트 관리 프레임워크 구축.\\n  - 문화·윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영하는 정책 마련.\\n- 교육\\n  - AI 도구 활용 역량 중심의 커리큘럼 강화: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함.\\n  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계/성능 최적화/보안 등 영역에 집중한 재교육 프로그램 운영.\\n  - 감정 인식 연구의 교육적 활용: 데이터 관리 및 윤리 교육을 포함한 커리큘럼 디자인.\\n- 연구 방향\\n  - 감정 상태와 코드 품질/생산성 간의 관계 실증 연구\\n  - 다문화 맥락에서의 감정 모델 비교 및 적합성 평가\\n  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구\\n\\nSources\\n- Stack Overflow Developer Survey 2025: AI 도구 사용 현황 및 개발자 인식\\n  - 예시 출처: Stack Overflow, 2025 Developer Survey, AI 도구 관련 섹션\\n- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\\n  - Databricks의 바이브 코딩 개념, Lakebase 출시 관련 보도\\n  - Cursor, Windsurf, Codeium 등의 스타트업 가치평가 및 투자 현황\\n- AI 코드 생성 도구 및 비교\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 사례\\n  - Replit의 에이전트 협업 기능, 협업 환경 소개\\n- 감정 인식/Affective Computing 연구\\n  - Real-time emotion aware applications: GPU 병렬처리 가속 사례\\n  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing, 2024)\\n  - DevEmo, AffecTube, DEAP 등 데이터셋 및 연구\\n- 표준 및 정책\\n  - ISO/IEC 30150-1:2022 (Affective computing user interface)\\n  - 프라이버시 및 데이터 보안 관련 논의 및 윤리 가이드라인\\n- 미래 직무 및 업계 분석\\n  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈 관련 최신 리뷰 및 기사\\n- 대표 기사 예시(한국어/영문)\\n  - TokenPost, Nate News 등 바이브 코딩 관련 기사\\n  - Dev.to/엘리트브레인즈(EliteBrains) 등의 AI 코드 생성 도구 비교 및 전망 기사\\n\\nREPORT_COMPLETE', type='TextMessage'), ToolCallRequestEvent(id='7dbedc14-b125-43ce-9090-895f429a7c5d', source='quality_reviewer', models_usage=RequestUsage(prompt_tokens=67128, completion_tokens=3007), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 35, 43, 138308, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_NhPPT9nX4cBHtxpqXt6dlNSP', arguments='{\"content\":\"보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\\\\n\\\\nExecutive Summary\\\\n- 정의 확장: 바이브 코딩은 분위기/감정 인식 기반의 코딩 및 인터랙티브한 개발 환경을 포괄하는 개념으로 이해되며, AI 기반 코드 작성 보조와 데이터-AI 워크플로우의 통합 트렌드를 아우릅니다.\\\\n- 현황 요약: AI 코드 생성 도구의 급속한 확산, IDE/에디터의 AI 내재화, 에이전트 기반 협업 기능의 확대가 두드러진 현상이다. Databricks의 레이크베이스 같은 플랫폼 출시와 스타트업(Cursor, Windsurf, Codeium 등)의 밸류에이션 상승이 시장의 활황을 시사한다.\\\\n- 소프트웨어 개발자의 미래: 개발자의 역할은 “보조 도구의 활용과 고부가 가치 영역의 설계/아키텍처/보안”으로 재편될 가능성이 크다. 엔트리 레벨 개발자 취업 환경은 AI 도입으로 변화하고 있으며, 재교육/직무 전환이 중요한 과제다.\\\\n- 시사점: 교육 체계의 현대화, 데이터 프라이버시 및 보안 거버넌스 강화, 문화적 맥락을 반영한 UX/윤리 표준 확립이 필요합니다.\\\\n\\\\nBackground & Current State\\\\n- 현장 landscape\\\\n  - 바이브 코딩은 자연어 기반 코드 생성과 업무 자동화를 넘어, 데이터-플랫폼 서비스와 AI 자동화 도구를 하나의 생태계로 연결하려는 방향으로 발전하고 있다.\\\\n  - Lakebase(데이터브릭스의 서버리스 OLTP 데이터베이스)와 Agent Bricks, AI 판사(Judges) 같은 기능이 데이터-AI 워크플로우의 혁신적 연결고리로 주목됩니다.\\\\n- 최근 developments 및 데이터 포인트\\\\n  - 대기업-스타트업 생태계의 경쟁 구도 강화: Cursor, Windsurf, Codeium 등 AI 코드 생성 스타트업의 밸류에이션 상승 및 투자 증가.\\\\n  - 플랫폼 중심의 협업 도구 강화: Replit, Cursor, Windsurf 등의 협업/에이전트 기능으로 팀 간 개발 흐름이 더욱 연결되고 있다.\\\\n  - 졸업/학계 트렌드: 감정 인식(Affective Computing) 연구 및 교육 현장의 활용 사례 증가, DevEmo, DEAP 등 데이터셋과 국제 표준(IS/IEC 30150-1:2022) 이슈가 주목받고 있다.\\\\n- 주요 통계 및 데이터\\\\n  - Stack Overflow Developer Survey 2025: AI 도구를 사용하거나 사용할 계획인 개발자 비중이 높음(대부분의 직군에서 80%대에 이르는 추세). 일부 응답자는 매일 AI 도구를 사용.\\\\n  - AI 도구에 대한 인식 및 신뢰도: 전문가층에서 AI 도구의 긍정적 효과에 대한 인식은 높지만, 도구의 정확성·보안·프라이버시 이슈에 대한 우려도 여전히 큽니다.\\\\n  - 엔트리 레벨 개발자 채용 변화: AI 도입으로 반복적 작업이 감소하는 한편, 대형 IT 기업들은 자체 AI 코드 생성 솔루션을 확산시키고 있습니다.\\\\n- 데이터의 한계\\\\n  - AI 도구의 정확도, 보안성, 데이터 프라이버시 이슈는 여전히 중요한 제약 요인이다. 윤리적·문화적 맥락 반영에 관한 연구와 정책 정비가 필요하다.\\\\n\\\\nAnalysis & Insights\\\\n- 주요 트렌드\\\\n  - AI 보조의 고도화: 코드 작성 보조를 넘어 요구 분석, 테스트 작성, 문서화까지 포괄하는 자동화 수준으로 확장 중이다.\\\\n  - 에이전트 기반 협업 확산: 에이전트 빅스, AI 판사 등의 도구가 코드 품질 관리 및 멀티에이전트 협업을 촉진한다.\\\\n  - 데이터-플랫폼의 통합: Lakebase 등의 플랫폼이 데이터 관리와 AI 개발 워크플로우를 하나의 환경에서 처리하도록 진화한다.\\\\n  - 감정 인식의 적용 확대: 교육 및 협업 도구의 학습효과 및 몰입도 개선 가능성이 있다. 다만 프라이버시와 문화적 차이에 대한 주의가 필요하다.\\\\n- 다양한 관점\\\\n  - 기술/기업 관점: 생산성 향상과 대규모 코드베이스의 관리 효율성 증가가 기대되나, 보안·규정 준수의 강화가 필수다.\\\\n  - 교육 관점: AI 도구의 활용 능력과 함께 기초 원리 이해, 보안/윤리 의식 강화가 필요하다.\\\\n  - 사회/정책 관점: 개인정보 보호, 알고리즘 편향성 관리, 국제 표준 정립 등 정책적 이슈가 부상한다.\\\\n- 전문가 의견 요약\\\\n  - “AI는 개발자의 보조 도구”라는 공통된 시각이 지배적이며, 시스템 설계, 아키텍처, 보안, 도메인 전문성 같은 고부가 가치 영역으로 인간의 역할이 재정의될 가능성이 크다.\\\\n  - 감정 인식 연구는 교육과 인터랙티브 도구 개선에 기여하나, 구현의 복잡성과 윤리 문제에 대한 신중한 접근이 필요하다는 견해가 많다.\\\\n  - 스타트업-대기업 간의 협력 및 경쟁은 기술 생태계의 건강성과 혁신 속도를 좌우한다.\\\\n- 도전 과제\\\\n  - AI의 불완전성 및 디버깅 부담: “Almost right” 문제가 여전히 흔하며 디버깅 시간이 길어질 수 있다.\\\\n  - 데이터 관리 및 보안: 에이전트의 기억(memory) 관리, 로그 기록, 접근 제어 등 거버넌스가 중요하다.\\\\n  - 문화적 맥락의 반영 필요성: 문화적 차이에 따른 감정 표현 모델의 차이가 UX에 큰 영향을 준다.\\\\n  - 교육 재구성 필요성: AI 도구의 도입과 함께 기초 원리/아키텍처 역량의 체계적 재교육이 필요하다.\\\\n\\\\nFuture Outlook\\\\n- Emerging trends\\\\n  - AI 에이전트 주도 개발의 확산: IDE 내에서의 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 흐름이 확대될 전망.\\\\n  - 멀티모달 감정 인식의 강화: 텍스트, 음성, 표정, 생리 신호를 결합한 감정 인식으로 개발 환경의 적응성 및 협업 품질이 향상될 가능성.\\\\n  - 문화적 맥락 반영과 표준화: 다문화 환경에서의 감정 모델 차이를 고려한 UX 설계 및 국제 표준 정립이 중요한 의제가 됨.\\\\n  - 교육-산업 연계 강화: affective computing 기반 학습 도구의 현장 직무 학습 도구로의 확산.\\\\n- Predictions\\\\n  - 5–10년 내 반복적이고 boilerplate 수준의 코딩은 AI 도구에 의해 상당 부분 대체 혹은 보조될 가능성 큼.\\\\n  - 개발자의 역할은 고급 설계/아키텍처/보안/도메인 전문성 중심으로 재편될 가능성 높음.\\\\n  - 교육 체계는 AI 도구 활용법 외에도 윤리, 보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 진화할 가능성.\\\\n- Implications\\\\n  - 기업: 데이터/AI 워크플로우의 통합 투자, 에이전트 거버넌스 강화, 프라이버시 및 보안 정책의 정교화 필요.\\\\n  - 교육: 감정 인식 기반 교육 도입 및 재교육 프로그램 설계, 시스템적 사고/아키텍처 역량 강화 필요.\\\\n  - 정책/표준: 데이터 보호, 책임 소재, 공정성/투명성 등의 원칙 정립이 중요해질 것.\\\\n\\\\nRecommendations\\\\n- 실무/기업\\\\n  - AI 도구 채택 로드맵 수립: 어느 단계에서 어떤 도구를 도입하고, 언제 인간의 검증을 필수로 할지 명확한 프로세스 수립.\\\\n  - 거버넌스 체계 강화: 메모리 관리, 접근 제어, 감사 로그, 보안 규정 준수 등을 포함한 에이전트 관리 프레임워크 구축.\\\\n  - 문화·윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영하는 정책 마련.\\\\n- 교육\\\\n  - AI 도구 활용 역량 중심의 커리큘럼 강화: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함.\\\\n  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계/성능 최적화/보안 등 영역에 집중한 재교육 프로그램 운영.\\\\n  - 감정 인식 연구의 교육적 활용: 데이터 관리 및 윤리 교육을 포함한 커리큘럼 디자인.\\\\n- 연구 방향\\\\n  - 감정 상태와 코드 품질/생산성 간의 관계 실증 연구\\\\n  - 다문화 맥락에서의 감정 모델 비교 및 적합성 평가\\\\n  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구\\\\n\\\\nSources\\\\n- Stack Overflow Developer Survey 2025: AI 도구 사용 현황 및 개발자 인식\\\\n  - 예시 출처: Stack Overflow, 2025 Developer Survey, AI 도구 관련 섹션\\\\n- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\\\\n  - Databricks의 바이브 코딩 개념, Lakebase 출시 관련 보도\\\\n  - Cursor, Windsurf, Codeium 등의 스타트업 가치평가 및 투자 현황\\\\n- AI 코드 생성 도구 및 비교\\\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 사례\\\\n  - Replit의 에이전트 협업 기능, 협업 환경 소개\\\\n- 감정 인식/Affective Computing 연구\\\\n  - Real-time emotion aware applications: GPU 병렬처리 가속 사례\\\\n  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\\\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing, 2024)\\\\n  - DevEmo, AffecTube, DEAP 등 데이터셋 및 연구\\\\n- 표준 및 정책\\\\n  - ISO/IEC 30150-1:2022 (Affective computing user interface)\\\\n  - 프라이버시 및 데이터 보안 관련 논의 및 윤리 가이드라인\\\\n- 미래 직무 및 업계 분석\\\\n  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈에 관한 최신 리뷰 및 기사\\\\n- 대표 기사 예시(한국어/영문)\\\\n  - TokenPost, Nate News 등 바이브 코딩 관련 기사\\\\n  - Dev.to/엘리트브레인즈(EliteBrains) 등의 AI 코드 생성 도구 비교 및 전망 기사\\\\n\\\\nREPORT_COMPLETE\"}', name='save_report_to_md')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(id='4307507d-e5be-48ae-9367-b0454375706a', source='quality_reviewer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 35, 43, 141395, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='report.md', name='save_report_to_md', call_id='call_NhPPT9nX4cBHtxpqXt6dlNSP', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(id='bd7c2578-a496-4c46-a13c-71d9c1d2f4e1', source='quality_reviewer', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 35, 43, 141594, tzinfo=datetime.timezone.utc), content='report.md', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_NhPPT9nX4cBHtxpqXt6dlNSP', arguments='{\"content\":\"보고서 제목: 바이브 코딩과 소프트웨어 개발자의 미래에 관한 연구 보고서\\\\n\\\\nExecutive Summary\\\\n- 정의 확장: 바이브 코딩은 분위기/감정 인식 기반의 코딩 및 인터랙티브한 개발 환경을 포괄하는 개념으로 이해되며, AI 기반 코드 작성 보조와 데이터-AI 워크플로우의 통합 트렌드를 아우릅니다.\\\\n- 현황 요약: AI 코드 생성 도구의 급속한 확산, IDE/에디터의 AI 내재화, 에이전트 기반 협업 기능의 확대가 두드러진 현상이다. Databricks의 레이크베이스 같은 플랫폼 출시와 스타트업(Cursor, Windsurf, Codeium 등)의 밸류에이션 상승이 시장의 활황을 시사한다.\\\\n- 소프트웨어 개발자의 미래: 개발자의 역할은 “보조 도구의 활용과 고부가 가치 영역의 설계/아키텍처/보안”으로 재편될 가능성이 크다. 엔트리 레벨 개발자 취업 환경은 AI 도입으로 변화하고 있으며, 재교육/직무 전환이 중요한 과제다.\\\\n- 시사점: 교육 체계의 현대화, 데이터 프라이버시 및 보안 거버넌스 강화, 문화적 맥락을 반영한 UX/윤리 표준 확립이 필요합니다.\\\\n\\\\nBackground & Current State\\\\n- 현장 landscape\\\\n  - 바이브 코딩은 자연어 기반 코드 생성과 업무 자동화를 넘어, 데이터-플랫폼 서비스와 AI 자동화 도구를 하나의 생태계로 연결하려는 방향으로 발전하고 있다.\\\\n  - Lakebase(데이터브릭스의 서버리스 OLTP 데이터베이스)와 Agent Bricks, AI 판사(Judges) 같은 기능이 데이터-AI 워크플로우의 혁신적 연결고리로 주목됩니다.\\\\n- 최근 developments 및 데이터 포인트\\\\n  - 대기업-스타트업 생태계의 경쟁 구도 강화: Cursor, Windsurf, Codeium 등 AI 코드 생성 스타트업의 밸류에이션 상승 및 투자 증가.\\\\n  - 플랫폼 중심의 협업 도구 강화: Replit, Cursor, Windsurf 등의 협업/에이전트 기능으로 팀 간 개발 흐름이 더욱 연결되고 있다.\\\\n  - 졸업/학계 트렌드: 감정 인식(Affective Computing) 연구 및 교육 현장의 활용 사례 증가, DevEmo, DEAP 등 데이터셋과 국제 표준(IS/IEC 30150-1:2022) 이슈가 주목받고 있다.\\\\n- 주요 통계 및 데이터\\\\n  - Stack Overflow Developer Survey 2025: AI 도구를 사용하거나 사용할 계획인 개발자 비중이 높음(대부분의 직군에서 80%대에 이르는 추세). 일부 응답자는 매일 AI 도구를 사용.\\\\n  - AI 도구에 대한 인식 및 신뢰도: 전문가층에서 AI 도구의 긍정적 효과에 대한 인식은 높지만, 도구의 정확성·보안·프라이버시 이슈에 대한 우려도 여전히 큽니다.\\\\n  - 엔트리 레벨 개발자 채용 변화: AI 도입으로 반복적 작업이 감소하는 한편, 대형 IT 기업들은 자체 AI 코드 생성 솔루션을 확산시키고 있습니다.\\\\n- 데이터의 한계\\\\n  - AI 도구의 정확도, 보안성, 데이터 프라이버시 이슈는 여전히 중요한 제약 요인이다. 윤리적·문화적 맥락 반영에 관한 연구와 정책 정비가 필요하다.\\\\n\\\\nAnalysis & Insights\\\\n- 주요 트렌드\\\\n  - AI 보조의 고도화: 코드 작성 보조를 넘어 요구 분석, 테스트 작성, 문서화까지 포괄하는 자동화 수준으로 확장 중이다.\\\\n  - 에이전트 기반 협업 확산: 에이전트 빅스, AI 판사 등의 도구가 코드 품질 관리 및 멀티에이전트 협업을 촉진한다.\\\\n  - 데이터-플랫폼의 통합: Lakebase 등의 플랫폼이 데이터 관리와 AI 개발 워크플로우를 하나의 환경에서 처리하도록 진화한다.\\\\n  - 감정 인식의 적용 확대: 교육 및 협업 도구의 학습효과 및 몰입도 개선 가능성이 있다. 다만 프라이버시와 문화적 차이에 대한 주의가 필요하다.\\\\n- 다양한 관점\\\\n  - 기술/기업 관점: 생산성 향상과 대규모 코드베이스의 관리 효율성 증가가 기대되나, 보안·규정 준수의 강화가 필수다.\\\\n  - 교육 관점: AI 도구의 활용 능력과 함께 기초 원리 이해, 보안/윤리 의식 강화가 필요하다.\\\\n  - 사회/정책 관점: 개인정보 보호, 알고리즘 편향성 관리, 국제 표준 정립 등 정책적 이슈가 부상한다.\\\\n- 전문가 의견 요약\\\\n  - “AI는 개발자의 보조 도구”라는 공통된 시각이 지배적이며, 시스템 설계, 아키텍처, 보안, 도메인 전문성 같은 고부가 가치 영역으로 인간의 역할이 재정의될 가능성이 크다.\\\\n  - 감정 인식 연구는 교육과 인터랙티브 도구 개선에 기여하나, 구현의 복잡성과 윤리 문제에 대한 신중한 접근이 필요하다는 견해가 많다.\\\\n  - 스타트업-대기업 간의 협력 및 경쟁은 기술 생태계의 건강성과 혁신 속도를 좌우한다.\\\\n- 도전 과제\\\\n  - AI의 불완전성 및 디버깅 부담: “Almost right” 문제가 여전히 흔하며 디버깅 시간이 길어질 수 있다.\\\\n  - 데이터 관리 및 보안: 에이전트의 기억(memory) 관리, 로그 기록, 접근 제어 등 거버넌스가 중요하다.\\\\n  - 문화적 맥락의 반영 필요성: 문화적 차이에 따른 감정 표현 모델의 차이가 UX에 큰 영향을 준다.\\\\n  - 교육 재구성 필요성: AI 도구의 도입과 함께 기초 원리/아키텍처 역량의 체계적 재교육이 필요하다.\\\\n\\\\nFuture Outlook\\\\n- Emerging trends\\\\n  - AI 에이전트 주도 개발의 확산: IDE 내에서의 코드 작성, 디버깅, PR 생성 등을 자동으로 수행하는 흐름이 확대될 전망.\\\\n  - 멀티모달 감정 인식의 강화: 텍스트, 음성, 표정, 생리 신호를 결합한 감정 인식으로 개발 환경의 적응성 및 협업 품질이 향상될 가능성.\\\\n  - 문화적 맥락 반영과 표준화: 다문화 환경에서의 감정 모델 차이를 고려한 UX 설계 및 국제 표준 정립이 중요한 의제가 됨.\\\\n  - 교육-산업 연계 강화: affective computing 기반 학습 도구의 현장 직무 학습 도구로의 확산.\\\\n- Predictions\\\\n  - 5–10년 내 반복적이고 boilerplate 수준의 코딩은 AI 도구에 의해 상당 부분 대체 혹은 보조될 가능성 큼.\\\\n  - 개발자의 역할은 고급 설계/아키텍처/보안/도메인 전문성 중심으로 재편될 가능성 높음.\\\\n  - 교육 체계는 AI 도구 활용법 외에도 윤리, 보안, 문화적 맥락 이해를 포함한 포괄적 AI 리터러시 교육으로 진화할 가능성.\\\\n- Implications\\\\n  - 기업: 데이터/AI 워크플로우의 통합 투자, 에이전트 거버넌스 강화, 프라이버시 및 보안 정책의 정교화 필요.\\\\n  - 교육: 감정 인식 기반 교육 도입 및 재교육 프로그램 설계, 시스템적 사고/아키텍처 역량 강화 필요.\\\\n  - 정책/표준: 데이터 보호, 책임 소재, 공정성/투명성 등의 원칙 정립이 중요해질 것.\\\\n\\\\nRecommendations\\\\n- 실무/기업\\\\n  - AI 도구 채택 로드맵 수립: 어느 단계에서 어떤 도구를 도입하고, 언제 인간의 검증을 필수로 할지 명확한 프로세스 수립.\\\\n  - 거버넌스 체계 강화: 메모리 관리, 접근 제어, 감사 로그, 보안 규정 준수 등을 포함한 에이전트 관리 프레임워크 구축.\\\\n  - 문화·윤리 가이드라인: 감정 인식 UX 설계 시 문화 차이를 반영하는 정책 마련.\\\\n- 교육\\\\n  - AI 도구 활용 역량 중심의 커리큘럼 강화: 도구 사용법, 디버깅 전략, 보안/프라이버시 의식 포함.\\\\n  - 시스템적 사고 및 아키텍처 역량 강화: 고급 설계/성능 최적화/보안 등 영역에 집중한 재교육 프로그램 운영.\\\\n  - 감정 인식 연구의 교육적 활용: 데이터 관리 및 윤리 교육을 포함한 커리큘럼 디자인.\\\\n- 연구 방향\\\\n  - 감정 상태와 코드 품질/생산성 간의 관계 실증 연구\\\\n  - 다문화 맥락에서의 감정 모델 비교 및 적합성 평가\\\\n  - 에이전트 협업의 효과성 분석: 팀 성과, 학습 효과, 협업 품질에 대한 실증 연구\\\\n\\\\nSources\\\\n- Stack Overflow Developer Survey 2025: AI 도구 사용 현황 및 개발자 인식\\\\n  - 예시 출처: Stack Overflow, 2025 Developer Survey, AI 도구 관련 섹션\\\\n- 바이브 코딩 및 스타트업 생태계 관련 기사/리포트\\\\n  - Databricks의 바이브 코딩 개념, Lakebase 출시 관련 보도\\\\n  - Cursor, Windsurf, Codeium 등의 스타트업 가치평가 및 투자 현황\\\\n- AI 코드 생성 도구 및 비교\\\\n  - GitHub Copilot, Amazon CodeWhisperer, Tabnine, Replit Ghostwriter 등 도구 비교 및 사례\\\\n  - Replit의 에이전트 협업 기능, 협업 환경 소개\\\\n- 감정 인식/Affective Computing 연구\\\\n  - Real-time emotion aware applications: GPU 병렬처리 가속 사례\\\\n  - Towards Emotion-awareness in Programming Education with Behavior-based Emotion Estimation (IEEE Xplore)\\\\n  - Affective Computing: Recent Advances, Challenges, and Future Trends (Intelligent Computing, 2024)\\\\n  - DevEmo, AffecTube, DEAP 등 데이터셋 및 연구\\\\n- 표준 및 정책\\\\n  - ISO/IEC 30150-1:2022 (Affective computing user interface)\\\\n  - 프라이버시 및 데이터 보안 관련 논의 및 윤리 가이드라인\\\\n- 미래 직무 및 업계 분석\\\\n  - AI 도구의 직무 영향, 에이전트 협업의 생산성 효과, 보안/윤리 이슈에 관한 최신 리뷰 및 기사\\\\n- 대표 기사 예시(한국어/영문)\\\\n  - TokenPost, Nate News 등 바이브 코딩 관련 기사\\\\n  - Dev.to/엘리트브레인즈(EliteBrains) 등의 AI 코드 생성 도구 비교 및 전망 기사\\\\n\\\\nREPORT_COMPLETE\"}', name='save_report_to_md')], results=[FunctionExecutionResult(content='report.md', name='save_report_to_md', call_id='call_NhPPT9nX4cBHtxpqXt6dlNSP', is_error=False)]), TextMessage(id='a1b0b20e-3d88-4874-b586-4020dbb1bdff', source='quality_reviewer', models_usage=RequestUsage(prompt_tokens=69696, completion_tokens=34), metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 35, 58, 433433, tzinfo=datetime.timezone.utc), content='The research is complete. The report has been saved to report.md. Please review the report and let me know if you approve it or need additional research.', type='TextMessage'), UserInputRequestedEvent(id='00bf40dc-5142-4c2b-b96c-ee2d07eb995a', source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 36, 29, 189056, tzinfo=datetime.timezone.utc), request_id='13c63d39-b1c4-4fa4-b5b4-e96b6130cfc9', content='', type='UserInputRequestedEvent'), TextMessage(id='60d74eee-4c83-4fea-8cfd-2e6a4257375c', source='user_proxy', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 9, 8, 12, 37, 29, 106300, tzinfo=datetime.timezone.utc), content='APPROVED', type='TextMessage')], stop_reason=\"Text 'APPROVED' mentioned\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(\n",
    "    team.run_stream(task=\"바이브 코딩과, 소프트웨어 개발자의 미래에 대해서 조사해\"),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
